[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook. You can download the notes, practice problems and application activities which can be edited locally on your computer. Completed assignments will be submitted in Canvas as web-page reports.\n\n\nTo download the folder containing all of the exercises, notes and application activities by following these steps:\n\nCreate a class folder where you will save all your work throughout this semester. This can be on your computer locally or in cloud-based storage such as One-Drive or iCloud.\nClick on the link below to download the .zip file containing all contents of the course website:\n\n\n Download Course Files \n\n\nOpen the .zip file\nSave the Student_Work folder to your class folder created in step 1. This can typically be done by right clicking the folder then copying and pasting it into the new location.\n\nThese files will be the basis of your coursework.\nThere may be additional files to download throughout the semester. You can save those to your\n\n\n\nEach unit contains:\n\nPreparation readings\nPractice problems\nApplication Activities\nAssessments\n\nGetting Started guides you through getting R and R-Studio set up, introduces the tools we will use for the class, and introduces basic statistical concepts to help get you started. We learn how to use R-Studio to make lovely reports that can be viewed in a web-browser. It’s a gentle enough introduction to enable even those who have never coded before to start working with R.\nDescriptive Statistics introduces tools to help understand data. We review summary statistics and visualizations for quantitative and categorical data types and learn the basic skills to turn data into information.\nData Wrangling and Visualization takes a step further into dealing with messy data and creating better visualizations.\nFoundations of Statistical Inference introduces the key concepts for using sample data to make generalized conclusions about a population. We cover probability, hypothesis testing, confidence intervals and other concepts necessary for making the leap from sample to population.\nStatistical Tests - Part 1 and 2 cover specific inferential statistics for different data types.\nThe Semester Project is an opportunity for students to demonstrate the skills acquired during the course. You will be able to find and import data into R, clean the data, create top notch visualizations, perform appropriate statistical analysis and create a beautiful report.\nR Help provides additional resources for learning how to use R.\n\n\nBy the end of this semester, students will confidently be able to:\n\nImport data and apply basic data wrangling to moderately messy data\nDescribe data with numerical and graphical summaries\nMake evidence-based decisions regarding situations with inherent randomness\nInvestigate questions through the application of probability distributions\nPerform the appropriate statistical analysis based on specific data being analyzed including hypothesis tests and confidence intervals\nCreate web-based reports to communicate the results of statistical analyses to relevant audiences"
  },
  {
    "objectID": "index.html#an-interactive-textbook",
    "href": "index.html#an-interactive-textbook",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook. You can download the notes, practice problems and application activities which can be edited locally on your computer. Completed assignments will be submitted in Canvas as web-page reports.\n\n\nTo download the folder containing all of the exercises, notes and application activities by following these steps:\n\nCreate a class folder where you will save all your work throughout this semester. This can be on your computer locally or in cloud-based storage such as One-Drive or iCloud.\nClick on the link below to download the .zip file containing all contents of the course website:\n\n\n Download Course Files \n\n\nOpen the .zip file\nSave the Student_Work folder to your class folder created in step 1. This can typically be done by right clicking the folder then copying and pasting it into the new location.\n\nThese files will be the basis of your coursework.\nThere may be additional files to download throughout the semester. You can save those to your\n\n\n\nEach unit contains:\n\nPreparation readings\nPractice problems\nApplication Activities\nAssessments\n\nGetting Started guides you through getting R and R-Studio set up, introduces the tools we will use for the class, and introduces basic statistical concepts to help get you started. We learn how to use R-Studio to make lovely reports that can be viewed in a web-browser. It’s a gentle enough introduction to enable even those who have never coded before to start working with R.\nDescriptive Statistics introduces tools to help understand data. We review summary statistics and visualizations for quantitative and categorical data types and learn the basic skills to turn data into information.\nData Wrangling and Visualization takes a step further into dealing with messy data and creating better visualizations.\nFoundations of Statistical Inference introduces the key concepts for using sample data to make generalized conclusions about a population. We cover probability, hypothesis testing, confidence intervals and other concepts necessary for making the leap from sample to population.\nStatistical Tests - Part 1 and 2 cover specific inferential statistics for different data types.\nThe Semester Project is an opportunity for students to demonstrate the skills acquired during the course. You will be able to find and import data into R, clean the data, create top notch visualizations, perform appropriate statistical analysis and create a beautiful report.\nR Help provides additional resources for learning how to use R.\n\n\nBy the end of this semester, students will confidently be able to:\n\nImport data and apply basic data wrangling to moderately messy data\nDescribe data with numerical and graphical summaries\nMake evidence-based decisions regarding situations with inherent randomness\nInvestigate questions through the application of probability distributions\nPerform the appropriate statistical analysis based on specific data being analyzed including hypothesis tests and confidence intervals\nCreate web-based reports to communicate the results of statistical analyses to relevant audiences"
  },
  {
    "objectID": "Spaced_Learning_Activities/Prop_Table_Party_Affiliation_and_Gender.html",
    "href": "Spaced_Learning_Activities/Prop_Table_Party_Affiliation_and_Gender.html",
    "title": "Political Party and Gender",
    "section": "",
    "text": "Read in the data and answer the questions below:\n\nlibrary(rio)\n\nWarning: package 'rio' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following object is masked from 'package:rio':\n\n    factorize\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nparty &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/Voting_Gender.csv')\n\nRows: 1000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Gender, Party\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQUESTION: Based on this survey of registered voters, what percent of females are Democrat?\nQUESTION: What percent of Independents are male?"
  },
  {
    "objectID": "Spaced_Learning_Activities/DataCleaning1_Notes.html",
    "href": "Spaced_Learning_Activities/DataCleaning1_Notes.html",
    "title": "Data Cleaning Practice",
    "section": "",
    "text": "Notes:\nBreak this into 3, then do cumulative retrieval (pair-share each day; start with the hardest one so it gets the most retrieval); possibly have some scaffolding slide, prompt words.\nGroups of 3 for a week; what if you used the same topic? Do 3 filter exercises; do 3 gg-plots, etc.\n\n\nIntroduction\nIn this exercise, you’ll practice the basics of data cleaning and visualization.\nLoad and explore the data then respond to the questions below.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nreal_estate &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/MadisonCountyRealEstateA.csv')\n\nglimpse(real_estate)\n\nRows: 149\nColumns: 18\n$ ListPrice    &lt;dbl&gt; 136500, 136500, 136500, 167900, 319000, 699000, 445000, 2…\n$ Bedrooms     &lt;dbl&gt; 21, 2, 2, 3, 6, 6, 6, 6, 2, 5, 3, 5, 5, 2, 5, 6, 3, 3, 3,…\n$ TotalBaths   &lt;dbl&gt; 1.5, 1.5, 1.5, 2.0, 3.0, 4.0, 3.0, 3.0, 1.0, 3.0, 2.0, 3.…\n$ Style        &lt;chr&gt; \"2 Story\", \"2 Story\", \"2 Story\", \"1 Story\", \"1 Story\", \"2…\n$ Age          &lt;dbl&gt; 3, 3, 3, 30, 6, 2, 6, 2, 750, 14, 5, 4, 3, 32, 10, 4, 2, …\n$ LotSize      &lt;dbl&gt; 0.03, 0.03, 0.03, 0.30, 5.98, 1.00, 1.00, 0.24, 0.16, 0.2…\n$ SQFT         &lt;dbl&gt; 1250, 1250, 1250, 1760, 3300, 5226, 4550, 3220, 1310, 280…\n$ Taxes        &lt;dbl&gt; NA, NA, NA, 1032, 1881, NA, 2408, 448, 641, 1970, 2019, 9…\n$ IsFixerUpper &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Elementary   &lt;chr&gt; \"KENNEDY\", \"KENNEDY\", \"KENNEDY\", \"MISSING\", \"ARCHER\", \"HI…\n$ JrHigh       &lt;chr&gt; \"MADISON 321JH\\xa0\", \"MADISON 321JH\\xa0\", \"MADISON 321JH\\…\n$ High         &lt;chr&gt; \"MADISON 321HS\\xa0\", \"MADISON 321HS\\xa0\", \"MADISON 321HS\\…\n$ Subdivision  &lt;chr&gt; \"RIVERWOODS TOWNHOUSES\", \"RIVERWOODS TOWNHOUSES\", \"RIVERW…\n$ City         &lt;chr&gt; \"REXBURG\", \"REXBURG\", \"REXBURG\", \"REXBURG\", \"REXBURG\", \"R…\n$ County       &lt;chr&gt; \"Madison\", \"Madison\", \"Madison\", \"Madison\", \"Madison\", \"M…\n$ Fireplace    &lt;chr&gt; \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\"…\n$ FloodPlain   &lt;chr&gt; \"N\", \"N\", \"N\", \"U\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n$ Source       &lt;chr&gt; \"http://snakerivermls.com/\", \"Accessed January 2010\", NA,…\n\n\nQUESTION: Make a Histogram of ListPrice\n\nggplot(real_estate, aes(x = ListPrice)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nQUESTION: What is the shape of the distribution of list prices?\nANSWER:\nQUESTION: Make a table of summary statistics for ListPrice\n\nfavstats(real_estate$ListPrice)\n\n  min     Q1 median     Q3    max     mean       sd   n missing\n 1.47 139500 189900 259000 924500 218137.6 126135.8 149       0\n\n\nQUESTION: What is the minimum value for list prices?\nANSWER: 1.47\nCreate a clean dataset that:\n\nRemoves houses that are more than $1M\nHas only 1 Story houses\nNo Missing Elementary school information\nIs in Rexburg\nContains only the columns: ListPrice, Style, Elementary, City, and IsFixerUpper\n\n\nunique(real_estate$City)\n\n[1] \"REXBURG\"    \"SUGAR CITY\" \"MENAN\"     \n\nclean &lt;- real_estate %&gt;%\n  filter(ListPrice &gt; 2,\n         ListPrice&lt;1000000,\n         Style == \"1 Story\",\n         Elementary != \"MISSING\",\n         City == \"REXBURG\") %&gt;%\n  select(ListPrice, Style, Elementary, City, IsFixerUpper)\n\nclean\n\n# A tibble: 70 × 5\n   ListPrice Style   Elementary City    IsFixerUpper\n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;       \n 1    319000 1 Story ARCHER     REXBURG No          \n 2    445000 1 Story HIBBARD    REXBURG No          \n 3    274000 1 Story ADAMS      REXBURG No          \n 4    119900 1 Story ADAMS      REXBURG No          \n 5    219900 1 Story LINCOLN    REXBURG No          \n 6    149900 1 Story KENNEDY    REXBURG No          \n 7    279000 1 Story ADAMS      REXBURG No          \n 8    189900 1 Story LYMAN      REXBURG No          \n 9    305000 1 Story ADAMS      REXBURG No          \n10    184500 1 Story SOUTH      REXBURG No          \n# ℹ 60 more rows\n\n\nQUESTION: What percent of homes in the clean dataset are fixer-uppers?\n\nprop.table(table(clean$IsFixerUpper))\n\n\n        No        Yes \n0.97142857 0.02857143 \n\n\nQUESTION: Use GGPLOT to create a boxplot for ListPrice that compares Elementary Schools?\n\nggplot(clean, aes(y = ListPrice, x = fct_reorder(Elementary, ListPrice), fill=Elementary)) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(\n    legend.position = \"null\"\n  )\n\n\n\n\n\n\n\n\nQUESTION: Which Elementary school appears to have the lowest distribution of housing prices?\nANSWER:"
  },
  {
    "objectID": "Spaced_Learning_Activities/ContingencyTableProportions.html",
    "href": "Spaced_Learning_Activities/ContingencyTableProportions.html",
    "title": "Table Proportions: Olympics",
    "section": "",
    "text": "Introduction\nThis dataset has information about every Olympic participant and medal awards for all the Summer and Winter Olympics from 1896-2016.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nolympics &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/Olympics.csv')\n\nglimpse(olympics)\n\nRows: 271,116\nColumns: 15\n$ ID     &lt;dbl&gt; 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, …\n$ Name   &lt;chr&gt; \"A Dijiang\", \"A Lamusi\", \"Gunnar Nielsen Aaby\", \"Edgar Lindenau…\n$ Sex    &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"M\"…\n$ Age    &lt;dbl&gt; 24, 23, 24, 34, 21, 21, 25, 25, 27, 27, 31, 31, 31, 31, 33, 33,…\n$ Height &lt;dbl&gt; 180, 170, NA, NA, 185, 185, 185, 185, 185, 185, 188, 188, 188, …\n$ Weight &lt;dbl&gt; 80, 60, NA, NA, 82, 82, 82, 82, 82, 82, 75, 75, 75, 75, 75, 75,…\n$ Team   &lt;chr&gt; \"China\", \"China\", \"Denmark\", \"Denmark/Sweden\", \"Netherlands\", \"…\n$ NOC    &lt;chr&gt; \"CHN\", \"CHN\", \"DEN\", \"DEN\", \"NED\", \"NED\", \"NED\", \"NED\", \"NED\", …\n$ Games  &lt;chr&gt; \"1992 Summer\", \"2012 Summer\", \"1920 Summer\", \"1900 Summer\", \"19…\n$ Year   &lt;dbl&gt; 1992, 2012, 1920, 1900, 1988, 1988, 1992, 1992, 1994, 1994, 199…\n$ Season &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", \"Winter\", \"Winter\", \"Wi…\n$ City   &lt;chr&gt; \"Barcelona\", \"London\", \"Antwerpen\", \"Paris\", \"Calgary\", \"Calgar…\n$ Sport  &lt;chr&gt; \"Basketball\", \"Judo\", \"Football\", \"Tug-Of-War\", \"Speed Skating\"…\n$ Event  &lt;chr&gt; \"Basketball Men's Basketball\", \"Judo Men's Extra-Lightweight\", …\n$ Medal  &lt;chr&gt; NA, NA, NA, \"Gold\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nCreate a clean dataset for Summer Olympics that includes:\n\nSummer Olympics (Season)\nKeeps only the columns NOC and Medal\nDrops the missing values (drop_na())\n\n\nsummer &lt;- olympics %&gt;%\n\nError in parse(text = input): &lt;text&gt;:5:0: unexpected end of input\n3:   \n4: \n  ^\n\n\nCreate a table that has the proportion of medals won by NOC (National Olympic Committees)"
  },
  {
    "objectID": "Spaced_Learning_Activities/Bar_Chart_Party_Affiliation_and_gender.html",
    "href": "Spaced_Learning_Activities/Bar_Chart_Party_Affiliation_and_gender.html",
    "title": "Political Party and Gender",
    "section": "",
    "text": "Read in the data and answer the questions below:\n\nlibrary(rio)\n\nWarning: package 'rio' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following object is masked from 'package:rio':\n\n    factorize\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nparty &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/Voting_Gender.csv') %&gt;% mutate(Party=factor(Party, levels = c(\"Democrat\", \"Republican\", \"Independent\")))\n\nRows: 1000 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Gender, Party\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreate a side-by-side bar chart of party affiliation that groups the bars based on Gender:"
  },
  {
    "objectID": "8-References/Hypothesis_Test_Process.html",
    "href": "8-References/Hypothesis_Test_Process.html",
    "title": "Hypothesis Test Steps",
    "section": "",
    "text": "Hypothesis Test Steps\nEvery statistical hypothesis test will follow the form:\n\nState the null (\\(H_0\\)) and alternative (\\(H_a\\)) hypotheses and choose the significance level, \\(\\alpha\\)\n\nThese will always reflect statements about population parameters ( \\(\\mu\\), \\(p\\), \\(\\beta\\), etc. )\n\n\\(\\alpha\\) is the probability of reject a true null hypothesis. It is selected based on the severity of that type of error. Common values include 0.1, 0.05, and 0.01.\n\nCalculate the Test Statistic based on your sample data\n\nThe math behind how this is calculated depends on the type of test, but all are some version of \\(\\frac{signal}{noise}\\)\n\nTest statistics have probability distributions with different shapes depending on what scenario is being tested. The shape of these distributions often depends on degrees of freedom which are related to the number of data points and/or the number of groups for categorical data.\n\nCalculate the P-value, the probability of observing the test statistic that we observed (or more extreme) if the null hypothesis were true.\nMake your conclusion by comparing the P-value to \\(\\alpha\\)\n\nWhen \\(\\text{P-value} &lt; \\alpha\\) we REJECT Ho in favor of Ha. Sufficient evidence to suggest that the alternative is true.\n\nWhen \\(\\text{P-value} \\ge \\alpha\\) we FAIL TO REJECT Ho in favor of Ha. Insufficient evidence to suggest that the alternative is true.\n\nCheck your test requirements\n\nAgain, which test requirements will depend on data types."
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html",
    "href": "7-Semester_Project/Importing_Data.html",
    "title": "Downloading Data",
    "section": "",
    "text": "So far in class, we have encountered only fairly clean data examples carefully managed and stored in an easy to access location. We have been able to use the import() function from the rio library with a link to a well-contained data resource.\nWhen we encounter data in the wild, it can be much more complicated to extract and clean up. However, good research should be transparent, with data sources cited and, where possible, published. We can often find links to raw data for graphs.\nIn these notes we will see how we can use tools we’ve already used to import data directly from the internet (or a saved file on your computer) and learn some new skills about how to clean up the data for specific needs."
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html#mac-instructions",
    "href": "7-Semester_Project/Importing_Data.html#mac-instructions",
    "title": "Downloading Data",
    "section": "Mac Instructions:",
    "text": "Mac Instructions:\n\nOpen Finder, which is the smiley face icon located on the bottom of your screen.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it (or hold down the Control key while clicking), and select “Get Info”.\nIn the window that pops up, you’ll see a field called “Where:”, which shows you the file path. It looks something like /Users/YourUsername/Documents/YourFile.csv. You can copy this path by selecting it and pressing Command + C."
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html#pc-instructions",
    "href": "7-Semester_Project/Importing_Data.html#pc-instructions",
    "title": "Downloading Data",
    "section": "PC Instructions:",
    "text": "PC Instructions:\n\nOpen File Explorer, which is usually the folder icon located on your taskbar or in the Start menu.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it and select “Properties”.\nIn the Properties window, you’ll see a field called “Location” or “Location:” which shows you the file path. It looks something like C:\\Users\\YourUsername\\Documents\\YourFile.csv. You can copy this path by selecting it and pressing Ctrl + C.\n\nIn both cases, the file path tells the computer where to find your file, similar to how a street address tells someone where to find a physical location. You can use this file path to access your file from anywhere on your computer.\nUnfortunately, R doesn’t like single backslashes, \\. If you’re on a PC you can either add a second \\ or switch them to /\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nwar &lt;- import('C:\\\\Users\\\\paulccannon\\\\OneDrive - BYU-Idaho\\\\Math221inR\\\\countries-in-conflict-data.csv')\nwar &lt;- import('C:/Users/paulccannon/OneDrive - BYU-Idaho/Math221inR/countries-in-conflict-data.csv')\n\nThe rio::import() function can handle many different types data types (.xlsx, .xls, .csv, .txt, and many others). If you run into a data file type that rio can’t import, there are other libraries that have similar functions that can do the same thing. But we limit the scope for this class to sources that rio can handle."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html",
    "title": "Who Shot First?",
    "section": "",
    "text": "Use the Star Wars dataset to answer the following questions:\n\nDo less than 20% of respondents feel Very Favorably towards Emperor Palpatine? (1-sample Z test for proportion)\nWhat is the difference in proportions of females and males who are Very Favorable towards Jar-Jar Binks? (2-sample Proportion)\nCome up with one other 2-sample proportion test using anything from the Star Wars dataset.\nTest to see if income and response to “Which Character Shot First?” are Independent (Chi-square)\n\nFor the proportion tests:\n\nDefine the null and alternative hypotheses\nInclude an explanation and conclusion for hypothesis tests\nInclude Confidence intervals and a sentence explaining each\nCheck the requirements for the hypothesis test and the confidence intervals\n\nFor the Chi-square test:\n\nDefine the null and alternative hypotheses\nInclude an explanation of the conclusion for\nBe sure to check the hypothesis requirements for a test of independence"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "title": "Who Shot First?",
    "section": "Explore the data",
    "text": "Explore the data\n\nnames(sw)\n\n [1] \"Are You a Fan of SW?\"               \"Favorability_Han Solo\"             \n [3] \"Favorability_Luke Skywalker\"        \"Favorability_Princess Leia Organa\" \n [5] \"Favorability_Anakin Skywalker\"      \"Favorability_Obi Wan Kenobi\"       \n [7] \"Favorability_Emperor Palpatine\"     \"Favorability_Darth Vader\"          \n [9] \"Favorability_Lando Calrissian\"      \"Favorability_Boba Fett\"            \n[11] \"Favorability_C-3P0\"                 \"Favorability_R2 D2\"                \n[13] \"Favorability_Jar Jar Binks\"         \"Favorability_Padme Amidala\"        \n[15] \"Favorability_Yoda\"                  \"who_shot_first\"                    \n[17] \"Familiar_with_expanded_universe\"    \"are_you_a_fan_of_expanded_universe\"\n[19] \"fan_of_star_trek\"                   \"Gender\"                            \n[21] \"Age\"                                \"Household.Income\"                  \n[23] \"Education\"                          \"Location\"                          \n\ntable(sw$`Favorability_Han Solo`)\n\n\n                                            \n                                          5 \nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\naddmargins(table(sw$`Favorability_Han Solo`, sw$Gender))\n\n                                             \n                                                  Female Male Sum\n                                                0      2    3   5\n  Neither favorably nor unfavorably (neutral)   1     22   21  44\n  Somewhat favorably                            4     71   76 151\n  Somewhat unfavorably                          1      3    4   8\n  Unfamiliar (N/A)                              0      9    6  15\n  Very favorably                               10    289  311 610\n  Very unfavorably                              0      0    1   1\n  Sum                                          16    396  422 834\n\naddmargins(table(sw$`Favorability_Emperor Palpatine`))\n\n\n                                            \n                                         20 \nNeither favorably nor unfavorably (neutral) \n                                        213 \n                         Somewhat favorably \n                                        143 \n                       Somewhat unfavorably \n                                         68 \n                           Unfamiliar (N/A) \n                                        156 \n                             Very favorably \n                                        110 \n                           Very unfavorably \n                                        124 \n                                        Sum \n                                        834"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "One-sample Proportion Test",
    "text": "One-sample Proportion Test\nWhat proportion of respondents are very favorable towards Emperor Palpatine?\nIs this significantly less than 20%?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "Two-sample Proportion Test",
    "text": "Two-sample Proportion Test\nWhat percent of female respondents are favorable towards Jar-Jar Binks?\nWhat percent of male respondents are favorable towards Jar-Jar Binks?\nAre they significantly different?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "title": "Who Shot First?",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nCompare 2 proportions of your choosing and perform a prop.test()."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "href": "6-Statistical_Tests_Part2/AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "title": "Who Shot First?",
    "section": "Chi-square Test for Independence",
    "text": "Chi-square Test for Independence\nTest to see if how you responded to the question “Who Shot First” is independent of income category.\nState your conclusion:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "To test the relationship between 2 categorical variables, we perform a \\(\\chi^2\\) test of independence. [NOTE: The Greek letter, \\(\\chi\\), is pronounced like “ki” in “kite”, not like “chi” in “tai chi”].\nWe begin a Chi-square test with a contingency table. Recall that a contingency shows the frequency of each combination of categorical variables in a sample.\nThe Chi-square test statistics is derived by looking at how far away are the observed frequencies away from what we would expect, if there was no relationship between the 2 variables.\n\n\nConsider the relationship between regular exercise and whether they have a healthy weight. A survey of 200 people showed:\n\n\n\n\n\n\n\n\n\n\nHealthy Weight\nNot Healthy Weight\nRow Total\n\n\n\n\nExercises Regularly\n60\n20\n80\n\n\nDoesn’t Exercise\n40\n80\n120\n\n\nColumn Total\n100\n100\n200\n\n\n\nDoes exercising make you more likely to have a healthy weight? Or are the two things completely unrelated?\n\n\nExpected counts represent what we would expect to see in each cell of the table if there was no relationship between exercise and weight.\nIf there’s no relationship, then the proportion of people with a healthy weight should be the same regardless of whether they exercise or not. Similarly, the proportion of people who exercise should be the same regardless of whether they have a healthy weight or not.\nConsider the expected number of people who Exercises Regularly and have a Healthy Weight.\nOut of the 200 people, 100 have a healthy weight. So, the overall proportion of people with a healthy weight is 100/200 = 0.5 (or 50%). If exercise has no effect on weight, we would expect 50% of the people who exercise to have a healthy weight.\nBecause a total of 80 people exercise regularly, we would expect \\(0.5 * 80 = 40\\) people to both exercise regularly and have a healthy weight.\nR will calculate the expected counts for every cell automatically.\nComparing Observed and Expected Counts:\nTo test the hypothesis that there is no relationship between exercise and weight, we compare what we actually observed to what we would expect if there was no relationship. Big differences between the observed and expected counts suggest that there is a relationship between the variables.\n\nFor example, we observed 60 people who exercise and have a healthy weight, but we expected only 40. This is a difference of 20.\nSimilarly, we observed 20 people who exercise and don’t have a healthy weight, but we expected 40. This is a difference of -20.\n\nThe Chi-Square Test\nThe chi-square test uses these differences between observed and expected counts to calculate a test statistic. The larger the differences, the larger the test statistic, and the stronger the evidence against the null hypothesis."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#example",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#example",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "Consider the relationship between regular exercise and whether they have a healthy weight. A survey of 200 people showed:\n\n\n\n\n\n\n\n\n\n\nHealthy Weight\nNot Healthy Weight\nRow Total\n\n\n\n\nExercises Regularly\n60\n20\n80\n\n\nDoesn’t Exercise\n40\n80\n120\n\n\nColumn Total\n100\n100\n200\n\n\n\nDoes exercising make you more likely to have a healthy weight? Or are the two things completely unrelated?\n\n\nExpected counts represent what we would expect to see in each cell of the table if there was no relationship between exercise and weight.\nIf there’s no relationship, then the proportion of people with a healthy weight should be the same regardless of whether they exercise or not. Similarly, the proportion of people who exercise should be the same regardless of whether they have a healthy weight or not.\nConsider the expected number of people who Exercises Regularly and have a Healthy Weight.\nOut of the 200 people, 100 have a healthy weight. So, the overall proportion of people with a healthy weight is 100/200 = 0.5 (or 50%). If exercise has no effect on weight, we would expect 50% of the people who exercise to have a healthy weight.\nBecause a total of 80 people exercise regularly, we would expect \\(0.5 * 80 = 40\\) people to both exercise regularly and have a healthy weight.\nR will calculate the expected counts for every cell automatically.\nComparing Observed and Expected Counts:\nTo test the hypothesis that there is no relationship between exercise and weight, we compare what we actually observed to what we would expect if there was no relationship. Big differences between the observed and expected counts suggest that there is a relationship between the variables.\n\nFor example, we observed 60 people who exercise and have a healthy weight, but we expected only 40. This is a difference of 20.\nSimilarly, we observed 20 people who exercise and don’t have a healthy weight, but we expected 40. This is a difference of -20.\n\nThe Chi-Square Test\nThe chi-square test uses these differences between observed and expected counts to calculate a test statistic. The larger the differences, the larger the test statistic, and the stronger the evidence against the null hypothesis."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#test-statistic",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#test-statistic",
    "title": "Chi-square Test of Independence",
    "section": "Test Statistic",
    "text": "Test Statistic\nThe \\(\\chi^2\\) test statistic measures how far away the observed counts are away from what is expected if the null hypothesis is true.\nAs with other test statistics, \\(\\chi^2\\) has a probability distribution. We use the probability distribution to calculate a P-value, which measures how likely it is to observe what we observed if the null hypothesis is true.\nThe \\(\\chi^2\\) test statistic is based on squared differences between observed and expected counts, so it is always positive and right skewed like the F-statistic.\n\nDegrees of Freedom\nThe shape of the \\(\\chi^2\\) distribution depends on the number of row and column levels. The degrees of freedom determine the shape of the Chi-square probability distribution.\nThe \\(\\chi^2\\) degrees of freedom are:\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#test-requirements",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#test-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Test Requirements",
    "text": "Test Requirements\nFor a Chi-square test to be valid, we need to have enough data so that each of the table cell combinations have adequate representation. This is similar to the 2-sample tests for proportions where we needed at least 10 expected successes and at least 10 expected failures (\\(np \\ge 10\\) and \\(n(1-p)\\ge10\\)), only now we need to check all the combinations.\nIf all the expected counts are greater than or equal to 5, we can trust the P-value."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#check-the-requirements",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#check-the-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Check the Requirements",
    "text": "Check the Requirements\nCheck that all the expected counts are greater than 5. We can extract the expected counts from the chisq.test() using the $expected and see if they are all greater than 5.\n\nchisq.test(chiro_table)$expected\n\n               \n                 At Risk Prevention Self Care Sick Role Wellness\n  Australia     73.77128   58.09043  190.6649  80.89894 65.57447\n  Europe        44.35714   34.92857  114.6429  48.64286 39.42857\n  United States 88.87158   69.98100  229.6922  97.45821 78.99696\n\n\nA useful trick is to use a logical operator to test if each value is greater than 5:\n\nchisq.test(chiro_table)$expected &gt;= 5\n\n               \n                At Risk Prevention Self Care Sick Role Wellness\n  Australia        TRUE       TRUE      TRUE      TRUE     TRUE\n  Europe           TRUE       TRUE      TRUE      TRUE     TRUE\n  United States    TRUE       TRUE      TRUE      TRUE     TRUE\n\n\nIf they are all TRUE, then your requirements are satisfied."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#visualization",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence.html#visualization",
    "title": "Chi-square Test of Independence",
    "section": "Visualization",
    "text": "Visualization\nWe can use ggplot() to create nice bar charts to help interpret the results.\n\nggplot(chiropractic, aes(x = location, fill = motivation)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression.html",
    "href": "6-Statistical_Tests_Part2/2-Regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Consider the relationship between Score on a math exam and a student’s self-reported Confidence Rating.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\n\nQuestion: What is the explanatory (aka independent) variable, \\(x\\)?\nAnswer:\nQuestion: What is the response (aka the dependent) variable, \\(y\\)?\nAnswer:\nPlot the relationship:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Does the relationship appear linear?\nAnswer:\nQuestion: What is the direction of the relationship? Answer:\nQuestion: What do you think is the strength of the relationship? (Strong/Moderate/Weak) Answer:\nQuestion: What is the correlation coefficient, r? Answer:\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression.html#plotting-the-regression-line",
    "href": "6-Statistical_Tests_Part2/2-Regression.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\n\nBase R\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function in base R, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression.html#ggplot",
    "href": "6-Statistical_Tests_Part2/2-Regression.html#ggplot",
    "title": "Simple Linear Regression",
    "section": "GGPlot",
    "text": "GGPlot\nUsing ggplot(), we can simply add a geom_smooth() geometry and specify the method of “smoothing” as a linear model:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\nBy default, this gives us a confidence interval for the slope of the regression line. We can turn that off by forcing the “standard error” to be FALSE:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression.html#hypothesis-testing-for-regression",
    "href": "6-Statistical_Tests_Part2/2-Regression.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most often only interested in the slope:\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese hypotheses are the same for all simple linear regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\nTechnically correct: 95% Confident that the true population slope is within (10.674297, 14.71535)\nContextual Explanation: For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression.html#regression-requirements-is-our-p-value-sus",
    "href": "6-Statistical_Tests_Part2/2-Regression.html#regression-requirements-is-our-p-value-sus",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements (Is our P-value sus?)",
    "text": "Regression Requirements (Is our P-value sus?)\nThere are certain requirements for all statistical tests to be valid. For regression analysis, we have to have 5 requirements for our statistics to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot(lm_output).\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is done by: plot(lm_output, which=1). This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The proper analysis for situations with a quantitative response variable and a categorical explanatory variable with 3 or more levels/categories is called Analysis of Variance. This has the unfortunate acronym, ANOVA."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#test-statistic",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#test-statistic",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Statistic",
    "text": "Test Statistic\nThe Test Statistic for testing the relationship between a quantitative response variable and a categorical explanatory variable with &gt;2 categories is called \\(F\\).\nThe formula for \\(F\\) is complicated. But we can consider the signal-to-noise ratio as follows:\nSIGNAL: The variation between the means, ie. how far apart the group means are spread out.\nNOISE: The variation within groups. This variation represents the natural variation inherent in the data independent of any differences between the means.\n\\[ F=\\frac{\\text{Between-group Variation}}{\\text{Within-group Variation}}\\]\nLet’s look at an example to illustrate.\n\nSeed Treatment Experiment\nImagine you’re comparing 4 pesticide seed treatments for corn seeds. These treatments are all supposed to improve emergence uniformity and crop yield.\nThe null hypothesis is that seed treatment has no effect on crop yield. This is typically expressed as a statistical hypothesis by stating that the average emergence is the same regardless of which treatment you use.\nWe would express this as:\n\\[H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\] PONDER: Can you see the connection between the treatment means being the same and “no relation” between treatment and emergence uniformity?\nThe alternative hypothesis for an \\(F\\)-statistics is:\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\]\nNote: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that at least one mean is different.\nA good experiment would run multiple experimental trials on different farms. Ideally, we’d want a good random sample of farms in the area we expect to be planting.\nIf the null hypothesis were true, any differences observed between the treatment means should be due to random variation or “noise” inherent in growing corn. The within-treatment variation is a good estimate of the “noise” because it reflects the natural variability in seed yields when the null hypothesis is true.\nIf the null hypothesis is false, we would expect the treatment means to be much more spread out relative to the within-treatment variation.\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more evidence we have against \\(H_0\\).\nA large F-statistics means the group averages are much more spread out than the natural variability in crop emergence.\n\n\nVisualizing Between-group and Within-group Variation\nWe can get a sense the signal and noise visually."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#the-f-distribution",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "The F-Distribution",
    "text": "The F-Distribution\nThe shape of the probability distribution of the F-statistic changes shape depending on how much data we have AND how many groups we have. These parameters in the \\(F\\)-distribution are called Degrees of Freedom, and the F-distribution has 2 sets.\nThe numerator (signal), or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe will get the values of both degrees of freedom directly from the R output.\nUnlike the standard normal distribution, the \\(F\\)-distribution is not centered around zero and can never be negative because \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests.\n\nThe P-value\nThe P-value is the probability of observing an \\(F\\)-statistic higher than the one we observed if the null hypothesis were true.\nFor a cutoff a given \\(\\alpha\\):\n\nWhen \\(P\\)-value is less than \\(\\alpha\\), we REJECT the null hypothesis and say we have SUFFICIENT evidence to suggest that at least one of the group means is different\nWhen \\(P\\)-value is greater than \\(\\alpha\\), we FAIL TO REJECT the null hypothesis and say we have INSUFFICIENT evidence to suggest that any of the means are different"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#test-requirements",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Requirements",
    "text": "Test Requirements\nJust as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\nTest Equal Variances\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\nNormality of Residuals\nResiduals are defined as the difference between an observation and it’s “expected” value:\n\\[residual = \\text{observed}-\\text{expected}\\]\nIn the case of ANOVA, the expected value is simply the group average. We get the residual for each observation by taking the observed value and subtracting its group mean. Once again, R will provide this for us.\nFor our analysis to be valid, we need to see if the residuals are normally distributed. We can do a qqPlot() of the residuals to assess this requirement.\nIf both requirements are met, the P-value is sound."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#analysis-in-r",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Analysis in R",
    "text": "Analysis in R\nTo perform an ANOVA, we use the aov() function with the familiar formula data$response ~ data$explanatory.\nThe aov() function by itself isn’t very useful on its own. We can use the summary() function to give us all the output we need to perform a hypothesis test.\nWe typically name our output using the assignment operator &lt;- to make it easier to get the information we would like.\nThe inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\naov_output &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(aov_output)\n\n\nCheck ANOVA Requirments:\nJust as with the other statistical tests, ANOVA has certain requirements for us to be able to trust the P-value.\n\nChecking Equal Standard Deviations:\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n# extract only the standard deviations from favstats output using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\n# if max/min &lt; 2, then we're ok\n\nThis gives us a valuable rule by which to decide if there is a violation of the constant variance requirement for the F-test.\nWe can visualize this just as we did with the regression analysis by plotting the residuals against their predicted values.\nplot(aov_output, which = 1)\nIn an ANOVA, the predicted value is the group mean. Because every observation in the group has the same predicted value, the residuals line up on vertical lines according to their group average. We can visually assess if one of the groups looks like there is much smaller variability than the others.\n\n\nAssessing Normality of the Residuals\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n# Name the output of the aov() function `output`:\n\naov_output &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# do a qqPlot of the residuals (observation values - group mean) that R calculates.  We can use th `$` to tell R what part of the output we want to make into a qqPlot():  \n\nqqPlot(aov_output$residuals)\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed.\n\n\nWhich Means are Different?\nIf the overall \\(F\\)-test is statistically significant (\\(P\\)-value &lt; \\(\\alpha\\)) then we can use what is called Tukey’s Honest Significant Difference to look at pairwise comparisons between the groups. We use the TukeyHSD() function by including the aov() model output:\nTukeyHSD(aov_output)\n\nCAUTION: Only use TukeyHSD() when the overall \\(F\\)-test is statistically significant.\nWe can easily plot the confidence"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#step-1-read-in-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#step-2-review-the-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#step-2-explore-the-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4), ylab=\"Sepal Length\", main = \"Sepal Length by Species\")\n\n\n\n\n\n\n\n\nUsing GGPlot:\n\nggplot(iris, aes(x=Species, y = Sepal.Length)) +\n  geom_boxplot(fill=c(2,3,4)) +\n  theme_bw() +\n  labs(\n    y = \"Sepal Length\",\n    title = \"Sepal Length by Species\"\n  )"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA.html#step-4-perform-the-appropriate-analysis",
    "href": "6-Statistical_Tests_Part2/1-ANOVA.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\nplot(aov_sepal, which=1)\n\n\n\n\n\n\n\n\nQuestion: What is the F-statistics?\nAnswer: 119.3\nQuestion: What are the between-groups degrees of freedom?\nAnswer: 2\nQuestion: What are the within-groups degrees of freedom?\nAnswer: 147\nQuestion: What is the P-value?\nAnswer: &lt;2e-16\nQuestion: What is your conclusion?\nAnswer: Because p is less than alpha, we reject the null hypothesis. We conclude that at least one of the means of Sepal Length is different from the other group means.\n\nWhich Means are Different\nIn the event that the overall test statistics is significant, we can look deeper into the pair-wise differences using the TukeyHSD() function as follows:\n\nTukeyHSD(aov_sepal)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nThe output gives us confidence intervals and P-values for each of the pairwise differences between the means."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html",
    "title": "2-Sample Proportion Practice",
    "section": "",
    "text": "Complete the following questions about testing differences between 2 proportions. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#confidence-interval",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of females to males who prefer to keep the penny:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#hypothesis-test",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nYou also want to see if there is a difference between the proportion of women who want to keep the penny and the proportion of men who want to keep the penny. Use a level of significance of \\(\\alpha = 0.05\\).\nState your null and alternative hypotheses:\n\\[H_0: p_{female}???p_{male}\\]\n\\[H_a: p_{female}???p_{male}\\]\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#hypothesis-test-1",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nConstruct a null and alternative hypothesis for the study:\n\\[H_0: \\]\n\\[H_a: \\]\nPerform the appropriate analysis:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions_Practice.html#confidence-interval-1",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of divorces between communicative and non-communicative couples:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html",
    "title": "Matched Pairs T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing dependent samples using R. For each question, you will:\n\nRead in data\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-1-load-the-data",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-1-load-the-data",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nhelmet_fit &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/helmet_fit.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms and summary statistic tables for the measurements for each type of tool:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nGive the summary statistics (favstats()) for the differences in the measured head diameters.\nQuestion: What does a negative number mean given the definition of the difference?\nAnswer:\nCreate a qqPlot() of the differences:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 4: Perform the Appropriate Analyses",
    "text": "Step 4: Perform the Appropriate Analyses\n\nHypothesis Test\nMake the following null and alternative hypothesis correct by deleting what doesn’t belong:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\n\n# Perform a t-test for the mean of the differences between cardboard and caliper data\n\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Can we trust this P-value? (eg. How many differences in our sample? Check the qqPlot() of the differences for normality)\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Intervals\nCreate a 95% confidence interval for the true average difference between the cardboard and the metal measurement tools:\nQuestion: Give a one-sentence explanation of your confidence interval.\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-1-load-the-data-1",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-1-load-the-data-1",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 2: Review the Data Generate Hypotheses",
    "text": "Step 2: Review the Data Generate Hypotheses\nCreate histograms and summary statistics tables for the cholesterol measurements at 2 days and at 4 days."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you are going to define the difference (chol_day2 - chol_day4 or chol_day4 - chol_day2).\nWhat does a negative number mean:\nWhat are your null and alternative hypotheses:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\nCreate a qqPlot() of the differences and determine if you can trust the statistical tests."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "title": "Matched Pairs T-Test Practice",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nPerform a t-test for the differences.\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Are the requirements satisfied for a matched pairs t-test? (Can we trust the P-value?)\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference in cholesterol scores:\nQuestion: Explain your confidence interval in context of the research question\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html",
    "title": "2-Sample Independent T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing 2-sample independent t-tests using R. For each question, you will:\n\nRead in data\n\nIdentify the Response and Independent variable\n\nCreate data summaries (numerical and graphical)\n\nStatistically analyze the data\n\nCheck for the suitability of the statistical test (CLT, Normality)\n\nState your hypothesis test conclusions and interpret your confidence intervals\n\nWhen you finish, render this document and submit the .html in Canvas.\n\n# Load the libraries\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#load-the-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#load-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\ndating &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/dating_attractive_longformat.csv')"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#explore-the-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#explore-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Explore the Data",
    "text": "Explore the Data\nQUESTION: What is the response variable?\nANSWER:\nQUESTION: What is the explanatory variable?\nANSWER:\nCreate a side-by-side boxplot for the amount of reported importance of attractiveness for each biosex.\nAdd a title and change the colors of the boxes.\nCreate a table of summary statistics for each group (favstats()):"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the ??? with the appropriate symbol):\n\\[H_0:  \\mu_{F}???\\mu_{M}\\]\n\\[H_a:\\mu_{F}???\\mu_{M}\\]\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = Female.\nQUESTION: Check that the samples for both groups are normally distributed:\n\nqqPlot()\n\nError in qqPlot.default(): argument \"x\" is missing, with no default\n\n\nQUESTION: Do the data for each group appear normally distributed?\nANSWER:\nQUESTION: Why is it OK to continue with the analysis? ANSWER:\nPerform a t-test.\nQUESTION: What is the value of the test statistic?\nANSWER:\nQUESTION: How many degrees of freedom for this test?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: What do you conclude?\nANSWER:\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#review-the-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#review-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQUESTION: What is the response variable?\nANSWER:\nQUESTION: What is the explanatory variable?\nANSWER:\nCreate summary statistics tables of dental costs for each office:\nCreate a side-by-side boxplot for dental costs for each office.\nCheck the normality of each group.\nQUESTION: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{IF}???\\mu_{R}\\]\n\\[H_a:\\mu_{IF}???\\mu_{R}\\]\nPerform the appropriate analysis:\nQUESTION: What is the test statistic?\nANSWER:\nQUESTION: What is the P-value?\nANSWER:\nState your conclusion:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#confidence-interval-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the difference in costs between the IF and Rexburg offices:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#review-the-data-1",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#review-the-data-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQUESTION: What is the response variable?\nANSWER:\nQUESTION: What is the explanatory variable?\nANSWER:\nCreate summary statistics tables of birth weights for each country:\nCreate a side-by-side boxplot for birth weights for each country:\nCheck the normality of each group.\nQUESTION: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?\nASNWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test-2",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#hypothesis-test-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{A}???\\mu_{IL}\\]\n\\[H_a:\\mu_{A}???\\mu_{IL}\\]\nPerform the appropriate analysis:\nQUESTION: What is the test statistic?\nANSWER:\nQUESTION: What is the P-value?\nANSWER:\nState your conclusion:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#confidence-interval-2",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest_practice.html#confidence-interval-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the average difference in weights between babies born to mothers in Africa and Illinois:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html",
    "title": "Confidence Intervals and Hypothesis Tests Practice",
    "section": "",
    "text": "A study was conducted to determine the proportion of American teenagers between 13 and 17 who smoke. Previous surveys showed that 15% percent of all teenagers smoke. A Gallup survey interviewed a nationally representative sample of 785 teenagers aged 13 to 17. Seventy-one (71) teenagers in the survey acknowledged having smoked at least once in the past week.\nWe want to explore if the study provide adequate evidence to conclude that the percentage of teenagers who smoke has changed since the original study (that is, is \\(p\\) now different than 15%).\nQUESTION: What are the requirements for a valid hypothesis test for a proportion, and are they met?\nANSWER:\nWhether or not the test requirements are met, perform the appropriate hypothesis test and create a confidence interval for the true proportion of teenagers who smoke.\n\n\nState the null and alternative hypotheses (replace the ??? with the correct information):\n\\[H_0: p=???\\]\n\\[H_a: p???0.15\\] Choose your significance level:\n\\[\\alpha = 0.\\]\n\n# perform the hypothesis test:  \n\nQUESTION: What is the value of the test statistic?\nANSWER:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: Based on your selected \\(\\alpha\\) and \\(P\\)-value, what is your conclusion?\nANSWER:\n\n\n\nCreate a \\((1-\\alpha)\\)% confidence interval for the true population proportion of teenagers who smoke?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#hypothesis-test",
    "title": "Confidence Intervals and Hypothesis Tests Practice",
    "section": "",
    "text": "State the null and alternative hypotheses (replace the ??? with the correct information):\n\\[H_0: p=???\\]\n\\[H_a: p???0.15\\] Choose your significance level:\n\\[\\alpha = 0.\\]\n\n# perform the hypothesis test:  \n\nQUESTION: What is the value of the test statistic?\nANSWER:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: Based on your selected \\(\\alpha\\) and \\(P\\)-value, what is your conclusion?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#confidence-interval",
    "title": "Confidence Intervals and Hypothesis Tests Practice",
    "section": "",
    "text": "Create a \\((1-\\alpha)\\)% confidence interval for the true population proportion of teenagers who smoke?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#confidence-interval-1",
    "title": "Confidence Intervals and Hypothesis Tests Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 90% confidence interval for the true population proportion of left-handed visual artists:\nQUESTION: What are the requirements for a confidence interval for a proportion, and are they met?\nANSWER:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions_Practice.html#hypothesis-test-1",
    "title": "Confidence Intervals and Hypothesis Tests Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform a hypothesis test to determine if the proportion of left-handed Visual Art majors is higher than the general population of 11%.\nQUESTION: What is the value of the test statistic?\nANSWER:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: Based on the P-value and a confidence level of \\(\\alpha=.05\\), what is your conclusion?\nANSWER:\nQUESTION: What are the requirements for a hypothesis test for a proportion, and are they met?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest_Practice.html",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest_Practice.html",
    "title": "Don’t Take it Personally",
    "section": "",
    "text": "Introduction\nIn this activity, you will execute statistical hypothesis tests and generate confidence intervals for each of the Big 5 personality traits using data collected from a random sample of Brother Cannon’s Math 221 students.\nQuestion: What is the population of this analysis?\nAnswer:\nFor each personality trait, include:\n\nA statement of the null and alternative hypotheses and why you chose the alternative you did.\nChoose alpha, $= $\nCheck that you can trust the normality of the mean (n &gt; 30 or qqPlot(respons_variable))\nRun the one-sample t-test and state your conclusion (technical and contextual explanation)\nCalculate a \\(1-\\alpha\\) level confidence interval and describe in words what it means in context of the research question\n\nRecall that we can use favstats() to get summary statistics, boxplot() and histogram() to get visualizations, and the t.test() function to get hypothesis tests and confidence intervals. Be sure to label your plots’ axes and include a title.\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\n\n\nExtroversion\nState your null and alternative hypotheses:\n\\[H_o:  \\mu = 50\\]\n\\[H_a:  \\mu &gt; 50\\]\n\\[\\alpha = 0.025 \\]\n1. Create a table of summary statistics:\n\n# Extroversion\n\nfavstats(big5$Extroversion) %&gt;% knitr::kable()\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n1\n42\n58\n73\n100\n56.98267\n21.09599\n404\n1\n\n\n\n\n\n\nCreate a histogram of Extroversion:\n\n\n### I'm making a single variable called \"extrov\" that drops the missing values.  You can do the same thing with the other traits to make analysis a little easier\nextrov &lt;- na.omit(big5$Extroversion) \n\nhistogram(extrov, xlab = \"Extroversion\", main = \"Histogram of Extroversion Percentiles\")\n\n\n\n\n\n\n\n\n\nPerform the one-sample t.test:\n\n\n# For Hypothesis Test:\nt.test(extrov, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  extrov\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\n\nExplain your conclusion:\n\nTechnical: Because the p-value is less than \\(\\alpha\\), I reject the null hypothesis.\nContextual: I have sufficient evidence to suggest that Brother Cannon’s students are, on average, more Extroverted than the general population.\n\nCreate a Confidence Interval for the average extroversion of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test(extrov, conf.level = 1-.025)$conf.int\n\n[1] 54.62135 59.34399\nattr(,\"conf.level\")\n[1] 0.975\n\n\n\nExplain your confidence interval:\n\nI am 97.5% confident that the true average extroversion of Brother Cannon’s students is somewhere between the 54.62 and 59.34 percentiles.\n\n\nAgreeableness\nState your null and alternative hypotheses. For example, do you think Brother Cannon’s students are more, less, or just different than the general population?\n\\[H_o:  \\mu  \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Agreeableness:\n\nCreate a histogram of Agreeableness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average agreeableness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nOpenness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n\nCreate a table of summary statistics for Openness:\n\n\nCreate a histogram of Openness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average openness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nNeuroticism\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Neuroticism:\n\nCreate a histogram of Neuroticism:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average neuroticism of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nConscientiousness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n1. Create a table of summary statistics:\n\nCreate a histogram of Conscientiousness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average conscientiousness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means_Practice.html",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means_Practice.html",
    "title": "Confidence Intervals For Means - Practice",
    "section": "",
    "text": "Introduction\nIn this exercise, you will create several confidence intervals for an unknown population mean, \\(\\mu\\), based on your sample mean, \\(\\bar x\\).\nStart by loading the libraries:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\n\n\nCreativity\nCreate a 95% confidence interval for the true population average creativity percentile (Creativity_Percentile) of Brother Cannon’s students (\\(\\mu_{\\text{creativity}}\\)):\n\ncreativity &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/creativity_personality_Clean.csv')\n\nt.test(, conf.level = )$conf.int\n\nError in t_test.default(, conf.level = ): argument \"x\" is missing, with no default\n\n\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:\nCreate a 93% confidence interval for the true population average number of siblings (How_Many_Siblings) of Brother Cannon’s students(\\(\\mu_{\\text{number of siblings}}\\)):\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:\n\n\nMedical Malpractice Lawsuits\nCreate a 99% Confidence interval for the true population average payout for operating on the wrong patient (Wrong_Patient) in malpractice lawsuits (\\(\\mu_{\\text{payout}}\\)):\n\nmalpractice &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")\n\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:\n\n\nOld Faithful\nCreate a 90% Confidence Interval for the true population average wait time for an Old Faithful eruption (\\(\\mu_{\\text{wait time}}\\)):\n\nold_faithful &lt;- rio::import(\"https://byuistats.github.io/M221R/Data/old_faithful.xlsx\")\n\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html",
    "href": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html",
    "title": "Statistical Significance",
    "section": "",
    "text": "Statistical significance is a crucial concept for determining whether the results observed in a study are likely due to a real effect or simply due to chance. It’s a core component of hypothesis testing.\n\n\nThe \\(P\\)-value is the probability of observing results as extreme as or more extreme than the results obtained in your sample assuming the null hypothesis is true. In other words, it indicates how likely it is to see the data you did if there was actually no effect present.\nA small p-value suggests strong evidence against the null hypothesis, while a large p-value suggests the observed data are consistent with sample-to-sample variability.\n\n\n\nBefore starting a study, we set a threshold that can be used to determine if the \\(P\\)-value is small enough to reject the null hypothesis. This number is called the level of significance and is denoted by the symbol \\(\\alpha\\) (pronounced “alpha”.)\nCommon values for \\(\\alpha\\) are 0.05, 0.01 and 0.1.\nWe will use the same decision rule for all hypothesis tests:\n\nIf the \\(P\\)-value is less than \\(\\alpha\\), we reject the null hypothesis.\n\nIf the \\(P\\)-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\nMemory Aid: If \\(P\\) is low, reject \\(H_O\\)\nWhere “low” means less than \\(\\alpha\\).\n\n\n\nWe can never be 100% certain that the signal observed in our sample was not just noise. Randomization is used to design statistical experiments to avoid bias in our samples, but sometimes that randomness can lead to results that look significant just by chance.\nConsider a clinical trial looking at 5-year survival rates for a new cancer therapy with 3 different levels of the treatment. We randomly assign patients to each treatment group.\nThere is a chance that the hardiest patients all end up in the same treatment group making it look like that treatment is better, but it wasn’t because of the treatment.\nWhen we conclude that there is a relationship between X and Y, when what we observed was actually “noise”, we have committed a Type I error.\nDEFINITION: Type I Error: Rejecting a TRUE null hypothesis. In some contexts this is called a FALSE POSITIVE\nBecause \\(\\alpha\\) is our decision point for rejecting the null hypothesis, \\(\\alpha\\) is the probability of rejecting a TRUE null hypothesis, meaning \\(\\alpha\\) is the probability of a Type I error.\nLet’s see what an \\(\\alpha = 0.01\\) looks like in the case of against the foot-long sandwiches:\n\n\n\n\n\n\n\n\n\nThe red area, \\(\\alpha\\), shows the probability of getting a sample mean in that tail when the null hypothesis is true. There’s always a chance we get such an extreme value and end up rejecting the null when it is true.\nIf we make \\(\\alpha\\) too small, we will reject \\(H_0\\) less often, thereby avoiding Type I errors. But that means that we are MORE likely to fail to reject the null hypothesis when we SHOULD have rejected it. This is a Type II error.\nDEFINITION: Type II Error: Failing to Reject a FALSE null hypothesis. In some contexts this is called a FALSE NEGATIVE\nFor example, if we failed to find evidence that the new cancer therapy worked but it really WAS effective, then we missed out on an a new, life-saving breakthrough.\n\n\n\nChoosing an appropriate alpha level involves balancing the risk of making Type I and Type II errors\nThe “best” choice for alpha depends on the context of the research. In exploratory research, a higher alpha (e.g., 0.10) might be acceptable to avoid missing potentially important effects. In situations where making a false positive conclusion could have serious consequences, a lower alpha (e.g., 0.01 or even lower) is preferred.\nIf the \\(\\alpha\\) value (the probability of committing a Type I error) is very small, the probability of committing a Type II error will be large. Conversely, if \\(\\alpha\\) is allowed to be very large, then the probability of committing a Type II error will be very small.\nA level of significance of \\(\\alpha=0.05\\) seems to strike a good balance between the probabilities of committing a Type I versus a Type II error. However, there may be instances where it will be important to choose a different value for \\(\\alpha\\). The important thing is to choose \\(\\alpha\\) before you collect your data. Typical choices of \\(\\alpha\\) are \\(0.05\\) (most common), \\(0.1\\), and \\(0.01\\).\n\n\nThe graph below illustrates the relationship between Type I and Type II errors. The red distribution represents the Null Hypothesis, the sampling distribution of mean lengths of foot-long sandwiches, assuming \\(\\mu_0=12\\).\nThe blue distribution represents the “TRUE” distribution with a mean, \\(\\mu_{truth}=11.2\\). We never know this in the real world, but to illustrate, here is a situation where the true population mean is, in fact, less than 12. The right decision would be to REJECT \\(H_0\\).\nIf we set \\(\\alpha = 0.01\\) (red shaded area), we can see that any sample mean to the right of the cutoff would fail to reject the null hypothesis. This mistake would be a Type II error.\nWhile we don’t know what the “TRUE” distribution is, we can see the relationship between moving the cutoff based on \\(\\alpha\\) would do to the probability of failing to reject the null hypothesis even when it is FALSE."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#reviw-p-value",
    "href": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#reviw-p-value",
    "title": "Statistical Significance",
    "section": "",
    "text": "The \\(P\\)-value is the probability of observing results as extreme as or more extreme than the results obtained in your sample assuming the null hypothesis is true. In other words, it indicates how likely it is to see the data you did if there was actually no effect present.\nA small p-value suggests strong evidence against the null hypothesis, while a large p-value suggests the observed data are consistent with sample-to-sample variability."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#significance-level-alpha-alpha",
    "href": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#significance-level-alpha-alpha",
    "title": "Statistical Significance",
    "section": "",
    "text": "Before starting a study, we set a threshold that can be used to determine if the \\(P\\)-value is small enough to reject the null hypothesis. This number is called the level of significance and is denoted by the symbol \\(\\alpha\\) (pronounced “alpha”.)\nCommon values for \\(\\alpha\\) are 0.05, 0.01 and 0.1.\nWe will use the same decision rule for all hypothesis tests:\n\nIf the \\(P\\)-value is less than \\(\\alpha\\), we reject the null hypothesis.\n\nIf the \\(P\\)-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\nMemory Aid: If \\(P\\) is low, reject \\(H_O\\)\nWhere “low” means less than \\(\\alpha\\)."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#mistaking-randomness-for-signal",
    "href": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#mistaking-randomness-for-signal",
    "title": "Statistical Significance",
    "section": "",
    "text": "We can never be 100% certain that the signal observed in our sample was not just noise. Randomization is used to design statistical experiments to avoid bias in our samples, but sometimes that randomness can lead to results that look significant just by chance.\nConsider a clinical trial looking at 5-year survival rates for a new cancer therapy with 3 different levels of the treatment. We randomly assign patients to each treatment group.\nThere is a chance that the hardiest patients all end up in the same treatment group making it look like that treatment is better, but it wasn’t because of the treatment.\nWhen we conclude that there is a relationship between X and Y, when what we observed was actually “noise”, we have committed a Type I error.\nDEFINITION: Type I Error: Rejecting a TRUE null hypothesis. In some contexts this is called a FALSE POSITIVE\nBecause \\(\\alpha\\) is our decision point for rejecting the null hypothesis, \\(\\alpha\\) is the probability of rejecting a TRUE null hypothesis, meaning \\(\\alpha\\) is the probability of a Type I error.\nLet’s see what an \\(\\alpha = 0.01\\) looks like in the case of against the foot-long sandwiches:\n\n\n\n\n\n\n\n\n\nThe red area, \\(\\alpha\\), shows the probability of getting a sample mean in that tail when the null hypothesis is true. There’s always a chance we get such an extreme value and end up rejecting the null when it is true.\nIf we make \\(\\alpha\\) too small, we will reject \\(H_0\\) less often, thereby avoiding Type I errors. But that means that we are MORE likely to fail to reject the null hypothesis when we SHOULD have rejected it. This is a Type II error.\nDEFINITION: Type II Error: Failing to Reject a FALSE null hypothesis. In some contexts this is called a FALSE NEGATIVE\nFor example, if we failed to find evidence that the new cancer therapy worked but it really WAS effective, then we missed out on an a new, life-saving breakthrough."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#choosing-alpha-and-understanding-error-types",
    "href": "4-Foundations_Statistical_Inference/8-Statistical_Significance.html#choosing-alpha-and-understanding-error-types",
    "title": "Statistical Significance",
    "section": "",
    "text": "Choosing an appropriate alpha level involves balancing the risk of making Type I and Type II errors\nThe “best” choice for alpha depends on the context of the research. In exploratory research, a higher alpha (e.g., 0.10) might be acceptable to avoid missing potentially important effects. In situations where making a false positive conclusion could have serious consequences, a lower alpha (e.g., 0.01 or even lower) is preferred.\nIf the \\(\\alpha\\) value (the probability of committing a Type I error) is very small, the probability of committing a Type II error will be large. Conversely, if \\(\\alpha\\) is allowed to be very large, then the probability of committing a Type II error will be very small.\nA level of significance of \\(\\alpha=0.05\\) seems to strike a good balance between the probabilities of committing a Type I versus a Type II error. However, there may be instances where it will be important to choose a different value for \\(\\alpha\\). The important thing is to choose \\(\\alpha\\) before you collect your data. Typical choices of \\(\\alpha\\) are \\(0.05\\) (most common), \\(0.1\\), and \\(0.01\\).\n\n\nThe graph below illustrates the relationship between Type I and Type II errors. The red distribution represents the Null Hypothesis, the sampling distribution of mean lengths of foot-long sandwiches, assuming \\(\\mu_0=12\\).\nThe blue distribution represents the “TRUE” distribution with a mean, \\(\\mu_{truth}=11.2\\). We never know this in the real world, but to illustrate, here is a situation where the true population mean is, in fact, less than 12. The right decision would be to REJECT \\(H_0\\).\nIf we set \\(\\alpha = 0.01\\) (red shaded area), we can see that any sample mean to the right of the cutoff would fail to reject the null hypothesis. This mistake would be a Type II error.\nWhile we don’t know what the “TRUE” distribution is, we can see the relationship between moving the cutoff based on \\(\\alpha\\) would do to the probability of failing to reject the null hypothesis even when it is FALSE."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/6-Central_Limit_Theorem_Practice.html",
    "href": "4-Foundations_Statistical_Inference/6-Central_Limit_Theorem_Practice.html",
    "title": "CLT Practice",
    "section": "",
    "text": "Instructions\nAnswer the following questions, render the document and submit the .html report.\n\n\nQuestions\nSuppose you take a sample of size \\(n=5\\) from a right skewed distribution with a population mean, \\(\\mu=125\\) and a standard deviation, \\(\\sigma=12\\)\nQuestion: What is the mean of the distribution of sample means (\\(\\mu_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the standard deviation of sample means (\\(\\sigma_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the shape of the distribution of sample means and why?\nAnswer:\nNow suppose you increase the sample size to \\(n=100\\). What is:\nMean of sample means:\nStandard deviation of sample means:\nShape of the distribution of sample means:\nDescribe the difference between the Law of Large Numbers and the Central Limit Theorem.\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/4-Assessing_Normality.html",
    "href": "4-Foundations_Statistical_Inference/4-Assessing_Normality.html",
    "title": "Assessing Normality",
    "section": "",
    "text": "Assessing Normality\nIn practice, we must often confirm that the distribution of sample means is normally distributed. This is true when:\n\nThe population is normally distributed\n\\(n &gt; 30\\) because of the Central Limit Theorem\n\nBut how do you know if a population is normally distributed? In the real world, there is no teacher to tell you when to assume a population is normal.\nIf our sample size is large enough, we don’t have to worry. We can trust the Central Limit Theorem.\nIf \\(n &lt; 30\\), we can assess the normality of our sample to decide if we can still trust output of our statistical conclusions.\nPreviously, we’ve used histograms to help visualize the distribution of a sample. However, when sample sizes are small, even samples from a standard normal distribution can look skewed.\nAll of the examples below are histograms of random samples from actual standard normal distributions:\n\n\nA New Way to Assess Normality\nStatisticians use QQPlots which are better at assessing normality. QQPlots plot the ordered, observed percentiles for each data point in a dataset on the x-axis with the theoretical percentile from a normal distribution on the y-axis. If the observed percentiles and theoretical percentiles line up, then we can be reasonably sure the population is normally distributed.\nThese are easier to use than to explain. We use the car library and the function qqPlot() to create a chart. (Note the Capital P in the middle.)\nKey Point: If most of the data points line up in the shaded region, we can consider the population as normally distributed.\nBelow are examples of QQPlots for a normal distribution and a right skewed distribution.\n\n\nThese work much better for small sample sizes. Below are several examples of QQPlots for small sample sizes. They are the same samples as shown above in the histograms.\n\nWhile not perfect, these are a vastly better tool to assess normality than a histogram.\n\n\n\nPractice\nLet’s try assessing the normality of some data. Below are 3 datasets. Find the response variable(s) from each and determine if the data are sufficiently normally distributed:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(car)\n\nold_faithful &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/OldFaithful.xlsx')\n\nqqPlot(old_faithful$Duration)\n\n\n\n\n\n\n\n\n[1] 19 58\n\nqqPlot(old_faithful$Wait)\n\n\n\n\n\n\n\n\n[1] 265 127\n\nrent &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/Rent.csv')\n\n\nmcat_gpa &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html",
    "title": "Normal Probability Practice",
    "section": "",
    "text": "Recall that the normal distribution is a probability distribution defined by its center (\\(\\mu\\)) and its spread (\\(\\sigma\\))."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html#brisket-competition",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html#brisket-competition",
    "title": "Normal Probability Practice",
    "section": "Brisket Competition",
    "text": "Brisket Competition\nCompetition was tight at a Saint Louis BBQ competition. Brisket scores were normally distributed with an average score of 5.941 with a standard deviation of 0.04.\nQuestion: What’s the probability of getting a score GREATER than 6?\nAnswer:\nQuestion: What’s the probability of getting a score LESS than 4?\nAnswer:\nQuestion: What is the 99th percentile of this Brisket Competition?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html#prbability-of-a-false-start",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution_Practice.html#prbability-of-a-false-start",
    "title": "Normal Probability Practice",
    "section": "Prbability of a “False Start”",
    "text": "Prbability of a “False Start”\n“At high level meets, the time between the gun and first kick against the starting block is measured electronically, via sensors built in the gun and the blocks. A reaction time less than 0.1 s is considered a false start. The 0.2-second interval accounts for the sum of the time it takes for the sound of the starter’s pistol to reach the runners’ ears, and the time they take to react to it.” (https://en.wikipedia.org/wiki/100_metres) Let’s suppose that reaction times are normally distributed with a mean of 0.2 seconds and a standard deviation of 0.03.\nWhat’s the probability of a false start? (meaning a reaction time LESS than 0.1)"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/1-Probability.html",
    "href": "4-Foundations_Statistical_Inference/1-Probability.html",
    "title": "Probability",
    "section": "",
    "text": "Probability plays a fundamental role in Statistics. In this section, we introduce the basic rules of probability for discrete events and introduce the idea of a probability distribution for continuous events. We provide a few examples of continuous probability distributions that will be used for making inference about a population."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/1-Probability.html#practice",
    "href": "4-Foundations_Statistical_Inference/1-Probability.html#practice",
    "title": "Probability",
    "section": "Practice",
    "text": "Practice\nUse the probabilty rules above to answer the following questions.\nQUESTION: In a political poll, 40% of respondents favor candidate A, 35% favor candidate B, and the rest are undecided. If one respondent is selected at random, what is the probability that they either favor candidate A or are undecided?\nANSWER:\nQUESTION: In a survey of 288 BYU-I students, 144 said they prefer Redneck Mixers from Great Scott’s for their soda, 58 said they prefer Swig and 68 preferred Fixxology. The rest are heathens with questionable orthodoxy who have a distaste for non-alcoholic sparkling drinks. What is the probability of randomly selecting a student that hates delicious beverages?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/1-Probability.html#the-normal-distribution",
    "href": "4-Foundations_Statistical_Inference/1-Probability.html#the-normal-distribution",
    "title": "Probability",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nWe will use the Normal Distribution to illustrate the probability rules for continuous probability distributions. The rules apply to any continuous probability distribution, but the normal distribution is one of the fundamental probability distributions in statistics. We will learn more about it in future lessons. For now, we use it only to illustrate the probability rules for continuous distributions.\nThe normal distribution is like a smooth histogram that is perfectly symmetric around the mean and has a standard deviation that determines how spread out the distribution is. Consider below a histogram of Major League Baseball batting averages:\n The smooth line is a normal distribution superimposed onto the histogram of the data. Using our data, we can calculate percentiles by adding up\nSuppose we want to estimate the probability that a randomly selected player will have a batting average that is greater than 0.280. One way to do this would be to find the proportion of players in the data set who have a batting average above 0.280. We can do this by finding the number of players who fall into each of the red-colored bins below and dividing this number by the total number of players.\n In other words, we could find the proportion of the total area of the bars that is shaded red out of the combined area of all the bars. This gives us the proportion of players whose batting averages are greater than 0.280.\nOut of the 446 players listed, there are a total of 133 players with batting averages over 0.280. This suggests that the proportion of players whose batting average exceeds 0.280 is:\n\\[\\displaystyle{\\frac{133}{446}} = 0.298\\]\nAlternatively, we can use the fact that the data follow a bell-shaped distribution to find the probability that a player has a batting average above 0.280."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/1-Probability.html#probability-rules-for-continuous-distributions",
    "href": "4-Foundations_Statistical_Inference/1-Probability.html#probability-rules-for-continuous-distributions",
    "title": "Probability",
    "section": "Probability Rules for Continuous Distributions",
    "text": "Probability Rules for Continuous Distributions\nThe principles are roughly the same as for discrete events.\n\n1. Total Probability:\n\n\\[\\text{The Total} \\textbf{ area under the curve} \\text{ is equal to 1.}\\]\n\n2. P(E &lt; a)\n\nThe probability that an event is less than some value, \\(a\\), is the area under the curve to the left of \\(a\\).\n\n\n3. P(E &gt; b)\n\nThe probability that an event is greater than some value, \\(b\\), is the area under the curve to the right of \\(b\\).\n\n\n4. P(a &lt; E &lt; b)\n\nThe probability of being between 2 numbers is the area under the curve between the 2 numbers:\n\n\n4. The Complement Rule\nThe probability that a continuous random variable does not fall within an interval ([a, b]) is:\n\\[\nP(\\text{not in } [a, b]) = 1 - P(a \\leq X \\leq b)\n\\]"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_More_Tidy_Practice.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_More_Tidy_Practice.html",
    "title": "Level up your Tidy-ness",
    "section": "",
    "text": "Putting it All Together\nHere are two more data wrangling questions to test your skills. Try as best you can to work each step on your own before checking solutions.\nThe questions relate to the High School survey used in other examples.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nTelepaths, Gender and Sleep\nSuppose we want to see who gets more sleep on non-school nights, males or females whose chosen superpower would be telepathy. Also, create a column that is the ratio of sleep hours on non-school nights to 8. This calculates the percent of recommended sleep on non-school nights.\n\nCreate a dataset that includes columns Gender, Superpower, Sleep_Hours_Non-Schoolnight, and the ratio of non-schoolnight sleep hours divided by 8, for Males and Females who choose Telepathy as their superpower.\n\n\n# Your Code:\n\n\n\nSolution\n\n\nunique(survey$Superpower)\n\n[1] \"Telepathy\"      \"Invisibility\"   \"Fly\"            \"Freeze time\"   \n[5] \"Super strength\"\n\ntelepaths &lt;- survey %&gt;%\n  select(Gender, Superpower, Sleep_Hours_Non_Schoolnight) %&gt;%\n  filter(Superpower==\"Telepathy\") %&gt;%\n  mutate(\n    percent_of_recommended = Sleep_Hours_Non_Schoolnight / 8\n  )\n\ntelepaths\n\n# A tibble: 81 × 4\n   Gender Superpower Sleep_Hours_Non_Schoolnight percent_of_recommended\n   &lt;chr&gt;  &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Male   Telepathy                            7                  0.875\n 2 Female Telepathy                            9                  1.12 \n 3 Male   Telepathy                            9                  1.12 \n 4 Female Telepathy                            8                  1    \n 5 Female Telepathy                            9                  1.12 \n 6 Male   Telepathy                           11                  1.38 \n 7 Female Telepathy                           11                  1.38 \n 8 Female Telepathy                            9                  1.12 \n 9 Female Telepathy                            9                  1.12 \n10 Female Telepathy                           10                  1.25 \n# ℹ 71 more rows\n\n\n\n\nCreate a summary table comparing males and females whose preferred super power is telepathy that includes:\n\na. Mean, standard deviation, and sample size of Sleep Hours on non-school nights \nb. Mean, standard deviation, and sample size of the percent of recommended sleep\n\n# Your Code\n\n\n\nSolution\n\n\ntelepaths %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mn_hrs = mean(Sleep_Hours_Non_Schoolnight),\n    mn_percent_recommended = mean(percent_of_recommended),\n    count = n()\n  )\n\n# A tibble: 2 × 4\n  Gender mn_hrs mn_percent_recommended count\n  &lt;chr&gt;   &lt;dbl&gt;                  &lt;dbl&gt; &lt;int&gt;\n1 Female   8.73                   1.09    60\n2 Male     8.19                   1.02    21\n\n\n\n\n\nVegetarians and Height\n\nHow many vegetarians say meat is their favorite food?\n\nHINT: This can be done with a single filter statement\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  filter(Favorite_Food == \"Meat\",\n         Vegetarian == \"Yes\")\n\n# A tibble: 1 × 60\n  Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n  &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 USA     NC         2022         11 Male         16 Right-Handed       178\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;,\n#   Imprtance_wning_cmputer &lt;int&gt;, Imprtance_Internet_access &lt;int&gt;, …\n\n\n\n\nCompare mean, and standard deviation of heights between those who are vegetarian and those who aren’t. Include the number of respondents in your analysis.\n\nBe sure to filter out any major outliers in heights first.\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100) %&gt;%\n  group_by(Vegetarian) %&gt;%\n  summarise(\n    med_ht = median(Height_cm),\n    mean_ht = mean(Height_cm),\n    sd_ht = sd(Height_cm),\n    count = n()\n  )\n\n# A tibble: 2 × 5\n  Vegetarian med_ht mean_ht sd_ht count\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 No           170.    171.  10.8   285\n2 Yes          163     162.  17.1    15\n\n\nAfter removing outliers, it looks like vegetarians are shorter, on average.\n\n# Bonus Boxplot:\nveg &lt;- survey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100)\n\n\nboxplot(veg$Height_cm ~ veg$Vegetarian, col = c(5,6), main = \"Heights (cm) of Vegetarians and Non-Vegetarians\", xlab=\"vegetarian\", ylab = \"Height (cm)\")\n\n\n\n\n\n\n\n\n\n\nCreate a dataset that:\n\nIncludes a column that is percent of recommended sleep (Sleep_Hours_Schoolnight divided by 8 using a mutate statement)\nIncludes only columns for Favourite_physical_activity, Reaction_time, percent_recommended_sleep (part a)\nIncludes only students whose favorite physical activity is Walking/Hiking, Basketball, Swimming, Soccer\nFilters Reaction Times to be less than 1 second\n\n\n\n# Your Code:\n\n\n\nSolution\n\n\nphys_act &lt;- survey %&gt;%\n  mutate(\n    pct_recommended_sleep = Sleep_Hours_Schoolnight / 8\n  ) %&gt;%\n  filter(Favourite_physical_activity %in% c('Walking/Hiking', \"Basketball\", \"Swimming\", \"Soccer\"),\n         Reaction_time &lt; 1) %&gt;%\n  select(Favourite_physical_activity, Reaction_time, pct_recommended_sleep)\n\n\nUse the clean dataset to:\n\nCreate a side-by-side boxplot for the percent of recommended sleep comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$pct_recommended_sleep ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"% Recommended Sleep on School Nights\", main = \"School Night Sleep by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\n\nCreate a side-by-side boxplot for the reaction times comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$Reaction_time ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"Reaction Time\", main = \"Reaction Time Results by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\nWhich physical activity group has the quickest reaction time?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html",
    "href": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html",
    "title": "Data Wrangling - Application Activity",
    "section": "",
    "text": "The Big 5 personality test is the most widely accepted tool for modelling personality in academic psychology. It is based on decades of statistical analysis of personality descriptions across languages and cultures. The big 5 traits are:\n\nOpenness\nConscientiousness\nExtroversion\nAgreeableness\nNeuroticism\n\nBrother Cannon collected personality data on students for the past several semesters, including a few metrics that may be associated with personality traits.\nNOTE: Scores for personality traits are given in percentiles relative to the general population.\nIn this activity, you will practice the process for approaching a dataset outlined in class:\n\nLoad the data and libraries\nExplore the data and generate hypotheses\nPrepare the data for analysis\nPerform the appropriate analysis\n\nData preparation will include using the filter() function. For now, analysis means creating good visualizations that tells a story using ggplot() and base R."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html#extroversion",
    "href": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html#extroversion",
    "title": "Data Wrangling - Application Activity",
    "section": "Extroversion",
    "text": "Extroversion\nQUESTION: Create a new dataset called extro that includes only columns for birth month and extroversion scores. Make sure it only has values that are real.\nHINT: Extroversion is measured in percentiles, and you should already know what the months of the year are.\n\nextro &lt;- \n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: extro &lt;- \n3:   \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot of Extroversion scores for all birth months.\n\nggplot()\n\n\n\n\n\n\n\n\nQUESTION: Based on the boxplot, which month appears to be the least extroverted? Explain your reasoning.\nANSWER:\nQUESTION: Based on the boxplot, which month appears to be the most extroverted? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html#neuroticism",
    "href": "3-Data_Wrangling_Visualization/6-AA_Data_Wrang_and_Viz.html#neuroticism",
    "title": "Data Wrangling - Application Activity",
    "section": "Neuroticism",
    "text": "Neuroticism\nQUESTION: Create a dataset called neuro that includes only the Section and Neuroticism columns:\n\nneuro &lt;- \n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: neuro &lt;- \n3:   \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot comparing Neuroticism for all the different sections:\nQUESTION: Based on the boxplot, which section appears to be the lowest in trait neuroticism? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/4-Select.html",
    "href": "3-Data_Wrangling_Visualization/4-Select.html",
    "title": "select()",
    "section": "",
    "text": "Selecting Columns\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nIt is likely that we are not interested in analyzing every column in this dataset. Many may even be useless. We can use the tidyverse function, select() to create a subset of the columns that we are primarily interested in.\nRecall that we can “pipe” the raw data into tidy functions using %&gt;%. Suppose we want to see if there are differences in reaction times (Reaction_time) for left-handed and right-handed students. We could create a more manageable dataset with only the columns of interest:\n\nsurvey %&gt;%\n  select(Handed, Reaction_time)\n\n# A tibble: 312 × 2\n   Handed       Reaction_time\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Left-Handed          0.349\n 2 Right-Handed         0.358\n 3 Right-Handed         0.447\n 4 Right-Handed         0.438\n 5 Left-Handed          0.542\n 6 Right-Handed         0.428\n 7 Ambidextrous         0.258\n 8 Right-Handed         0.427\n 9 Right-Handed         0.412\n10 Right-Handed         0.346\n# ℹ 302 more rows\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter out Ambidextrous participants and reaction time outliers (reaction times less than 1 seconds):\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  select(Handed, Reaction_time)\n\nboxplot(clean$Reaction_time ~ clean$Handed, col = c(2,3), ylab = \"Reaction Times\", xlab=\"\", main = \"Distribution of Reaction times for \\n Left and Right-hand Dominance\")\n\n\n\n\n\n\n\n\nWhich hand dominance appears to have quicker reaction times?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/2-Wrangling_Basics.html",
    "href": "3-Data_Wrangling_Visualization/2-Wrangling_Basics.html",
    "title": "Intro to Data Wrangling",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/2-Wrangling_Basics.html#the-pipe-in-action",
    "href": "3-Data_Wrangling_Visualization/2-Wrangling_Basics.html#the-pipe-in-action",
    "title": "Intro to Data Wrangling",
    "section": "The Pipe in Action",
    "text": "The Pipe in Action\nSuppose we have a dataset, marital_status_data, with 2 columns: age and marital_status. We expect the marital_status to be one of 4 options: “Single”, “Married”, “Divorced”, “Widowed”, but some joker input, “It’s complicated”. Because we are mainly interested in making inference about the primary statuses, we may want to filter out the rows with “It’s complicated” as the status.\nWe would begin with the raw data, “pipe” it into the filter() function and tell R what I want it to do. I can either tell R what I want to keep or what I want to exclude (more here):\nmarital_Status_data %&gt;% filter(marital_status != \"It's complicated\")\nNOTE: The != is read “not equal to”, and is a common logical operator used in computer programming. So the above code returns a subset of the original data omitting the rows.\nI can use pipes sequentially to do complicated data wrangling in very few lines of code."
  },
  {
    "objectID": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html",
    "href": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html",
    "title": "Application Activity: Descriptive Statistics",
    "section": "",
    "text": "In this activity, you will practice everything we’ve covered up to this point including:\n\nReviewing a dataset\n\nDetermining response and explanatory variables\n\nDetermining data types (quantitative, categorical)\n\nCreating descriptive statistics:\n\nQuantitative data (one and two variables)\n\nSummary statistics for a single variable (favstats(data$y))\nHistogram for one variable (histogram(data$y))\n\nSummary statistics for multiple groups (favstats(data$y ~ data$x))\n\nSide-by-side boxplots (boxplot(data$y ~ data$x))\n\nScatter plots, continuous \\(x\\) (plot(data$y ~ data$x))\n\nCorrelation between 2 quantitative variables (correlation coefficient, \\(r\\))\n\n\nCategorical data (one and two variables)\n\nIdentifying levels of a categorical variable (unique())\n\nFrequency and proportion tables (table(data$variable), prop.table(table(data$variable)))\n\nUnivariate, SORTED bar charts (barplot(SORTED_table_name))\n\nContingency tables (table(row, column))\n\nRow/Column Percents (prop.table(tbl_name, margin = ))\n\nBivariate bar charts (barplot(table_name, beside=TRUE, col=c(2,3,4,...)))\n\n\n\nWe will be using data collected about students in 2 Portuguese schools including their final grade. The goal of the research was to see what variables impact final grades."
  },
  {
    "objectID": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#response-variable-univariate-summary-statistics",
    "href": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#response-variable-univariate-summary-statistics",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Response Variable: Univariate Summary Statistics",
    "text": "Response Variable: Univariate Summary Statistics\nQUESTION: What is the primary response variable? (What are we trying to predict?)\nANSWER:\nQUESTION: Create a table of summary statistics for the primary response variable:\n\nfavstats()\n\nError in favstats(): argument \"x\" is missing, with no default\n\n\nQUESTION: What is the mean grade?\nANSWER:\nQUESTION: What is the median grade?\nANSWER:\nQUESTION: What is the Standard Deviation of grades?\nANSWER:\nQUESTION: Interpret in words what the standard deviation means?\nANSWER:\nQUESTION: What is the 75th percentile of grades?\nANSWER:\nQUESTION: Interpret in words what the 75th percentile means?\nANSWER:\nCreate a histogram of the primary response variable:\nQUESTION: What is the basic shape of the distribution?\nANSWER:\nQUESTION: What anomalies, if any, do you notice?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#explanatory-variables",
    "href": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#explanatory-variables",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Explanatory Variables",
    "text": "Explanatory Variables\nList 5 potential explanatory variables and whether or not they are quantitative or categorical:\n\n\n\n\n\n\n\n\nRelationship Between 2 Variables\n\nCategorical X\nCreate a table of summary statistics for final grade for each level of sex:\n\nfavstats(~)\n\nError in parse(text = input): &lt;text&gt;:2:11: unexpected ')'\n1: \n2: favstats(~)\n             ^\n\n\nQUESTION: What is the mean grade for females?\nANSWER:\nCreate a side-by-side boxplot of final grades for each level of father’s job (Fjob):\nQUESTION: Which level of Fjob tends to have students with the highest final grades?\nANSWER:\nCreate a side-by-side boxplot of final grades for each level of mother’s job (Mjob):\nQUESTION: Which level of Mjob appears to have students with the highest final grades?\nANSWER:\n\n\nQuantitative X\nCreate a scatter plot looking at the relationship between mid-term grade (G2) and final grade (G3):\nQUESTION: Does the relationship appear roughly linear?\nANSWER:\nQUESTION: What anomalies, if any, do you notice?\nANSWER:\nQUESTION: Based on the scatter plot, guess the correlation coefficient:\nANSWER:\nCalculate the correlation coefficient, r:"
  },
  {
    "objectID": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#categorical-descriptive-statistics",
    "href": "2-Descriptive_Statistics/AA_Descriptive_Statistics.html#categorical-descriptive-statistics",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Categorical Descriptive Statistics",
    "text": "Categorical Descriptive Statistics\nQUESTION: What are the levels of mother’s profession?\n\nunique()\n\nError in unique.default(): argument \"x\" is missing, with no default\n\n\nCreate a proportion table table for sex\nQUESTION: What percent of respondents are female?\nANSWER:\nCreate a SORTED bar chart of the counts of reasons students chose their school (reason):\nQUESTION: Which reason appears to be the most popular?\nANSWER:\nCreate and name a contingency table looking at the relationship between gender and whether or not they are in a romantic relationship.\nThen calculate the proportion of each gender in a romantic relationship (should sum to 1 for each gender):\nQUESTION: What percent of female respondents are in a romantic relationship?\nANSWER:\nQUESTION: What percent of male respondents are in a romantic relationship?\nANSWER:\nCreate a bar chart that groups bars based on sex on the x-axis and adds a legend for the colors of the bars by romantic.\n\n# Start by naming the table\ntbl_relationship &lt;- table()\n\nError in table(): nothing to tabulate\n\n# Then create the chart:"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "",
    "text": "There are situations where you would like to study the relationship between 2 categorical variables.\nIn this section, we introduce numerical and graphical summaries of comparing categorical variables and discuss some quirks about dealing with categorical data.\n\n\n\nSummarize bivariate data in contingency tables\nCalculate row and column percents\nCreate side-by-side, grouped bar plots\n\n\n\n\nIn this section, we will be using survey responses about Star Wars. The survey was carried out by FiveThirtyEight about the first 6 Star Wars films. The survey contains demographic information as well as movie rankings and character favorability rankings.\n\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#lesson-objectives",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#lesson-objectives",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "",
    "text": "Summarize bivariate data in contingency tables\nCalculate row and column percents\nCreate side-by-side, grouped bar plots"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#a-more-civilized-age",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#a-more-civilized-age",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "",
    "text": "In this section, we will be using survey responses about Star Wars. The survey was carried out by FiveThirtyEight about the first 6 Star Wars films. The survey contains demographic information as well as movie rankings and character favorability rankings."
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#load-the-data-and-libraries",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#load-the-data-and-libraries",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#calculating-proportions",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#calculating-proportions",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "Calculating Proportions",
    "text": "Calculating Proportions\nWhen looking at a single categorical variable, we input a table into the prop.table() function to get proportions instead of counts. It’s slightly more complicated with 2 variables because there are several proportions that can be calculated. The denominator depends on what we’re interested in studying as illustrated above (percent of females? or percent of Fans?).\nWe can use the prop.table() function to get:\n\nTable Percents: Sum to 1 across the entire table\n\nRow Percents: Sum to 1 across rows\n\nColumn Percents: Sum to 1 across columns\n\n\nTable Proportions\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nFor example, about 37% of respondents are male fans of Star Wars. About 19% of respondents are females who are not fans.\nThis is not typically the most interesting way to look at the data. We are more often interested in row or column proportions.\n\n\nRow Proportions\nWe can specify row proportions by including another input into the prop.table() function. We specify which margin to use as the denominator.\nRecall that the table() function will put the first input as the row and the second input as the column. To get row proportions, we tell R to divide by the row totals:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nQUESTION: What percent of males are fans of Star Wars?\nANSWER:\nQUESTION: What percent of females are fans of Star Wars?\nANSWER:\n\n\nColumn Proportions\nTo get column proportions, specify margin = 2 (the second input in the table() function)\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQUESTION: What percent of Star Wars fans are females?\nANSWER:\nQUESTION: What percent of Star Wars fans are males ?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#visual-summaries",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#visual-summaries",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "Visual Summaries",
    "text": "Visual Summaries\nThe best way to visualize 2 categorical variables is with a grouped, side-by-side bar plot. Soon we will learn a better way to make visualizations in R. For now, it’s a bit clunky to get the right type of graph that makes sense. But here’s the process:\n\nCreate and name a contingency table. Your column variable will be how the bars are grouped, and the row variable will determine the colors of the grouped bars\nInput the table name into the barplot() including the additional inputs: beside=TRUE which puts the bars next to each other, and legend=TRUE which will add a legend showing which bars correspond to which colors\nThe default colors are atrocious. You can specify the colors by adding the additional input col=c(2,3,4) or col=c(\"lightblue\", \"lightgreen\", \"darkred\",...) including as many colors as there are levels in the row variable.\n\nVisually, bar plots are the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\n\ntbl1 &lt;- table(sw$Gender, sw$`Are You a Fan of SW?`)\n\nbarplot(tbl1, beside=TRUE, legend=TRUE)\n\n\n\n\n\n\n\n# Adding Color\nbarplot(tbl1, beside=TRUE, legend=TRUE, col=c(2, 4))"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#proportion-table",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#proportion-table",
    "title": "Categorical Data Summaries - 2 Variables",
    "section": "Proportion Table",
    "text": "Proportion Table\nWe would like to compare the relationship between Gender and Household Income.\nCreate and name a table that shows the percent of Genders in each of the income levels:\n\ntbl3 &lt;- table()\n\nError in table(): nothing to tabulate\n\n\nQuestion: What percent of female respondents are in the 150,000+ category?\nAnswer:\nQuestion: What percent of male respondents are in the 150,000+ category?\nAnswer:\nCreate a bar plot that compares the income distribution for each gender in the study:\n\nbarplot(tbl3, beside = TRUE, legend=TRUE)\n\nError: object 'tbl3' not found\n\n\nSwap the row and column inputs and create the bar plot with the opposite grouping:\n\ntbl4 &lt;- table()\n\nError in table(): nothing to tabulate\n\nbarplot(tbl4, beside = TRUE, legend=TRUE)\n\nError: object 'tbl4' not found\n\n\nWhich chart is more interesting?"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html",
    "title": "Practice: Bivariate Data",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data\nCalculating and interpreting the correlation coefficient, r\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r",
    "title": "Practice: Bivariate Data",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\n\nplot()\n\nError in plot.default(): argument \"x\" is missing, with no default\n\ncor()\n\nError in cor(): argument \"x\" is missing, with no default\n\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What does this r measure?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#load-the-data",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#load-the-data",
    "title": "Practice: Bivariate Data",
    "section": "Load the data:",
    "text": "Load the data:\n\nmanatees &lt;- read_csv('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/manatees.csv')\n\nglimpse(manatees)\n\nRows: 35\nColumns: 4\n$ `Fiscal Year`             &lt;dbl&gt; 1977, 1978, 1979, 1980, 1981, 1982, 1983, 19…\n$ `Power Boats (in 1000's)` &lt;dbl&gt; 436, 449, 470, 487, 502, 501, 515, 548, 575,…\n$ Manatees                  &lt;dbl&gt; 14, 21, 18, 19, 24, 18, 20, 27, 30, 34, 31, …\n$ Comments                  &lt;chr&gt; \"Source for the Powerboat Data:\", \"Data for …\n\n\nQUESTION: What is the response/dependent variable?\nANSWER:\nQUESTION: What is the explanatory variable?\nANSWER:\nQUESTION: What do you think is the nature of the relationship between the two?\nANSWER:\nQUESTION: What is your best guess at the correlation coefficient, \\(r\\)?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Practice: Bivariate Data",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nQUESTION: Does the relationship look linear?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#load-the-data-1",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#load-the-data-1",
    "title": "Practice: Bivariate Data",
    "section": "Load the data:",
    "text": "Load the data:\n\nmcat &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')\n\nglimpse(mcat)\n\nRows: 55\nColumns: 2\n$ GPA  &lt;dbl&gt; 3.62, 3.84, 3.23, 3.69, 3.38, 3.72, 3.89, 3.34, 3.71, 3.89, 3.97,…\n$ MCAT &lt;int&gt; 38, 45, 33, 40, 35, 36, 40, 39, 35, 34, 39, 31, 35, 32, 32, 38, 3…\n\n\nQUESTION: What is the response/dependent variable?\nANSWER:\nQUESTION: What is the explanatory variable?\nANSWER:\nQUESTION: What do you think is the nature of the relationship between the two?\nANSWER:\nQUESTION: What is your best guess at the correlation coefficient, \\(r\\)?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Practice: Bivariate Data",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nQUESTION: Does the relationship look linear?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html",
    "title": "Practice: Summarizing Multiple Groups",
    "section": "",
    "text": "This example is taken from an experiment listed in the R help files under ?CO2.\n“An experiment on the cold tolerance of the grass species Echinochloa crus-galli was conducted. The CO2 uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient CO2 concentration. Half the plants of each type were chilled overnight before the experiment was conducted.” Plants were considered tolerant to the cold if they were still able to achieve high CO2 uptake values after being chilled.\nNOTE: Recall that the formula for separating a response variable by a category is to use the formula (data$y ~ data$x) which works for favstats() and boxplot()\nQUESTION: What is the response variable?\nANSWER:\nQUESTION: What are the levels of Type?\n\nunique()\n\nError in unique.default(): argument \"x\" is missing, with no default\n\n\n\n\nCreate a table of summary statistics of uptake for each Type:\n\nfavstats(~)\n\nError in parse(text = input): &lt;text&gt;:2:11: unexpected ')'\n1: \n2: favstats(~)\n             ^\n\n\nQUESTION: What is the mean uptake for the Quebec type?\nANSWER:\nQUESTION: What is the median of the Mississippi type?\nANSWER:\nQUESTION: Which type has the highest standard deviation?\nANsWER:\nCreate a side-by-side boxplot of uptake comparingType:\n\nboxplot(~)\n\nError in parse(text = input): &lt;text&gt;:2:10: unexpected ')'\n1: \n2: boxplot(~)\n            ^\n\n\n\n\n\nCreate a table of summary statistics of uptake for each Treatment used in the experiment.\nQUESTION: What is the mean uptake for the non-chilled treatment?\nANSWER:\nQUESTION: What is the mean of the chilled treatment?\nANSWER:\nCreate a side-by-side boxplot of uptake for each Treatment used in the experiment.\nQUESTION: Based on the above summaries, which combination of Type and Treatment do you think would result in the largest CO2 uptake?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html#impact-of-plant-type-on-uptake",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html#impact-of-plant-type-on-uptake",
    "title": "Practice: Summarizing Multiple Groups",
    "section": "",
    "text": "Create a table of summary statistics of uptake for each Type:\n\nfavstats(~)\n\nError in parse(text = input): &lt;text&gt;:2:11: unexpected ')'\n1: \n2: favstats(~)\n             ^\n\n\nQUESTION: What is the mean uptake for the Quebec type?\nANSWER:\nQUESTION: What is the median of the Mississippi type?\nANSWER:\nQUESTION: Which type has the highest standard deviation?\nANsWER:\nCreate a side-by-side boxplot of uptake comparingType:\n\nboxplot(~)\n\nError in parse(text = input): &lt;text&gt;:2:10: unexpected ')'\n1: \n2: boxplot(~)\n            ^"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html#impact-of-treatment-on-uptake",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups_Practice.html#impact-of-treatment-on-uptake",
    "title": "Practice: Summarizing Multiple Groups",
    "section": "",
    "text": "Create a table of summary statistics of uptake for each Treatment used in the experiment.\nQUESTION: What is the mean uptake for the non-chilled treatment?\nANSWER:\nQUESTION: What is the mean of the chilled treatment?\nANSWER:\nCreate a side-by-side boxplot of uptake for each Treatment used in the experiment.\nQUESTION: Based on the above summaries, which combination of Type and Treatment do you think would result in the largest CO2 uptake?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data_Practice.html",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data_Practice.html",
    "title": "Practice: Graphical Summaries (Quant.)",
    "section": "",
    "text": "This is an opportunity for you to practice creating graphical summaries of quantitative data.\nWe will explore 2 datasets. The first contains information about the duration and the time between geyser eruptions at Old Faithful in Yellowstone National park.\nThe second is data collected about expressions of gratitude and their impact on subjective well-being.\nFor each dataset, you will create numerical summaries of the data."
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data_Practice.html#histogram",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data_Practice.html#histogram",
    "title": "Practice: Graphical Summaries (Quant.)",
    "section": "Histogram",
    "text": "Histogram\nCreate a histogram and boxplot for happiness scores:\nQuestion: What is the general shape of the distribution?\nAnswer:"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html",
    "title": "Practice: Numerical Summaries (Quant.)",
    "section": "",
    "text": "This is an opportunity for you to practice creating numerical summaries of quantitative data.\nWe will explore 2 datasets. The first contains information about the duration and the time between geyser eruptions at Old Faithful in Yellowstone National park.\nThe second is data collected about expressions of gratitude and their impact on subjective well-being.\nFor each dataset, you will create numerical summaries of the data."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html#calculate-the-summary-statistics-for-duration",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html#calculate-the-summary-statistics-for-duration",
    "title": "Practice: Numerical Summaries (Quant.)",
    "section": "Calculate the Summary Statistics for Duration",
    "text": "Calculate the Summary Statistics for Duration\n\nfavstats()\n\nError in favstats(): argument \"x\" is missing, with no default\n\n\nQUESTION: What is the mean duration time of Old Faithful eruptions?\nANSWER:\nQUESTION: What is the standard deviation of duration?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html#calculate-summary-statistics-for-wait-time",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data_Practice.html#calculate-summary-statistics-for-wait-time",
    "title": "Practice: Numerical Summaries (Quant.)",
    "section": "Calculate Summary Statistics for Wait time",
    "text": "Calculate Summary Statistics for Wait time\nQuestion: What is the mean wait time between eruptions?\nAnswer:\nQuestion: What is the maximum wait time between eruptions?\nAnswer:\nQuestion: The middle 50% of wait times will be between what 2 numbers?\nAnswer:"
  },
  {
    "objectID": "1-Getting_Started/Test.html",
    "href": "1-Getting_Started/Test.html",
    "title": "Testing…Testing…1…2…3",
    "section": "",
    "text": "Introduction\nThis type of file is called a “markdown” file. Markdown is like Microsoft Word but much more powerful. This file is made specifically for creating fancy reports and has a file type .qmd meaning “Quarto Markdown”.\nYou will become very familiar with these files throughout the semester. For now, it’s only necessary to download this file, save it in a sensible folder on your computer or OneDrive, and “run” it.\nClicking on the “Render” button above will create an .html document that should open up in your default browser. This .html document will be created and saved in the same location as this TestingTesting.qmd document.\nNOTE: If this document is in your Downloads folder, that is also where the html file will appear.\nAs you see, we can make section headers and type regular text. But the power of .qmd files is that we can code inside these documents and present our output directly within the document.\nClick “Render” to test to see if your software is set up.\nWhen coding, we have to tell the computer when we’re writing text and when we expect it to compile code. Below is an example of a “code chunk” that creates a made up graph.\nYou do not have to understand this right now. We’re only testing that R and RStudio are set up correctly.\n\n\nInstalling Libraries\nEven though we have R and RStudio installed, we can load additional toolboxes that have more tools than R provides by itself.\n\nx &lt;- seq(0,10, length = 100)\ny &lt;- 2+exp(x)\n\nplot(x,y, type = \"l\", lwd=2, col=\"darkblue\", main = \"Exponential Function\")"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html",
    "href": "1-Getting_Started/5-Stat_Process.html",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#lesson-outcomes",
    "href": "1-Getting_Started/5-Stat_Process.html#lesson-outcomes",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#introduction",
    "href": "1-Getting_Started/5-Stat_Process.html#introduction",
    "title": "The Statistical Process",
    "section": "Introduction",
    "text": "Introduction\nStatistics are used in every aspect of society. Every statistical analysis follows a pattern we will call the Statistical Process. This process will be introduced in this lesson and will be used throughout the course.\n\n\nThe Statistical Process and Daniel’s Experiment\n\n\n\nStained-glass depiction of Daniel’s deliverance from the lions’ den. Found in the old Dominican priory church at Hawkesyard in Staffordshire, England. (Photo credit: Fr Lawrence Lew, O.P. Used by permission.)\n\n\n\nThe Old Testament prophet Daniel planned one of the earliest recorded scientific research studies. We will use his example to illustrate the following five steps of The Statistical Process.\nThe following icons can help you remember these steps. Notice that each icon has a letter and an image to help you remember the five steps of the Statistical Process.\n\n\n\n\n\n\n\n\n\n\n\nThe Statistical Process\n\n\n\n\n\n\nDesign the Study\n\n\n\nCollect the Data\n\n\n\nDescribe the Data\n\n\n\nMake Inference\n\n\n\nTake Action\n\n\n\n\n\n\n\nStep 1: Design the Study\n\nAn important step in scientific inquiry or problem solving can be to state a research question such as:\n\nWill internet advertising increase a company’s revenue?\nDoes expressing gratitude increase a person’s satisfaction with life in general?\nDoes a newly developed vaccine prevent the spread of disease?\n\nResearchers also investigate the background of the situation. What have other people discovered about this situation? How can we find the answer to the research question? What do we need to do? What is the population (or total collection of all individuals) under consideration? What kind of data need to be collected?\nBefore collecting data, researchers make a hypothesis, or an educated guess about the outcome of their research. A hypothesis is a statement such as the following:\n\nUsing internet advertising will increase the company’s sales revenue.\nPeople who express gratitude will be more satisfied with life than those who do not.\nA newly-developed vaccine is effective at preventing tuberculosis.\n\n\n\nDaniel’s Experiment\nAfter taking Israel captive, Babylon’s King Nebuchadnezzar asked his chief officer to bring Israelite children who were well favoured, and skillful in all wisdom, and cunning in knowledge, and understanding science to stand in the king’s palaces (Daniel 1:4). To aid their preparation, Nebuchadnezzar planned to feed them his meat and wine for three years (Daniel 1:5).\nDaniel did not want to defile himself by partaking of the king’s meat and wine. He asked permission to eat pulse[^1] and drink water instead. His supervisor, Melzar, was afraid to displease the king. He thought that after eating pulse and water, the selected Israelites would look worse than their peers, and he would be punished (Daniel 1:8-10).\nWith an understanding of the background of the situation, Daniel proposed an experiment. He said, Prove thy servants, I beseech thee, ten days; and let them give us pulse to eat, and water to drink. Then let our countenances be looked upon before thee, and the countenance of the children that eat of the portion of the king’s meat: and as thou seest, deal with thy servants (Daniel 1:12-13.). In short, Daniel’s implied research question can be stated as: Will those who eat pulse and drink water appear healthier than those who eat the king’s meat and drink his wine? Melzar agreed to the experiment.\n\nAnswer the following question:\n\n\n\nWhat is Daniel’s hypothesis?\n\n\n\nSolution\n\nDaniel’s hypothesis is that the Israelite children who eat pulse and drink water will appear healthier in just ten days, compared to those who eat the king’s meat and drink his wine.\n\n\n\n\n\n\nStep 2: Collect Data\n\nWhen designing a study, much attention is given to the process by which data are observed. When examining data, it is also important to understand the data collection procedures. A sample is a subset (a portion) of a population. How is this sample obtained? How are the observations made?\nDaniel’s study design required that data be collected at the end of 10 days. Melzar would compare the appearances of two groups of people: (1) Israelites who ate pulse and drank water versus (2) Israelites who ate the king’s meat and drank his wine.\n\n\n\nStep 3: Describe the Data\n\nWhen we describe data, we use any tools appropriate to the situation. This can include creating graphs or calculating summary statistics to help understand or visualize the data.\nFor Daniel’s experiment, the data are described in Daniel 1:15: And at the end of ten days [the] countenances [of those who ate pulse] appeared fairer and fatter in flesh than all the children which did eat the portion of the king’s meat.\n\n\n\nStep 4: Make Inferences\n\nInference is the process of using the information contained in a sample from a population to make a general statement (i.e. to infer something) about the entire population. Later in the course we will learn techniques that make this type of analysis possible.\nMelzar made an inference. Based on the results of the sample, he determined that (in general) those who eat pulse and drink water will be healthier than those who eat the king’s meat and drink his wine Daniel 1:15-16.\n\n\n\nStep 5: Take Action\n\nThe goal of a statistical analysis is to determine which action to take in a particular situation. Actions can include many things: launching an internet ad campaign (or not), expressing gratitude (or not), getting vaccinated (or not), etc.\nMelzar took action as described in Daniel 1:16: Thus Melzar took away the portion of their meat, and the wine that they should drink; and gave [all the Israelite children] pulse.\nWas the experiment a success?\n“Now at the end of the days that the king had said he should bring them in… the king communed with them; and among them all was found none like Daniel, Hananiah, Mishael, and Azariah And in all matters of wisdom and understanding, that the king enquired of them, he found them ten times better than all the magicians and astrologers that were in all his realm” Daniel 1:18-20.\n\n\n\nSummary of the Statistical Process\n\nDaniel’s experience can also help you learn the Statistical Process. Look at the first letter of each of the steps in the Statistical Process. You can use the phrase “Daniel Can Discern More Truth” to help you to help you remember the five steps in the Statistical Process.\n\nThe Statistical Process\n\n\n\n\n \nPneumonic\nActual Process Step\n\n\n\n\nStep 1:\nDaniel\nDesign the study\n\n\nStep 2:\nCan\nCollect data\n\n\nStep 3:\nDiscern\nDescribe the data\n\n\nStep 4:\nMore\nMake inferences\n\n\nStep 5:\nTruth\nTake action\n\n\n\n\nThe Statistical Process will be used throughout the course. Take time to memorize the five steps.\n\n\nThe study designed by the Old Testament prophet Daniel provides an ancient example of a designed experiment. Daniel’s experiment included two groups of people: those who had the experimental treatment eating pulse and drinking water (called the treatment group) and those who ate the standard food the king’s meat (called the control group.) The treatment group receives the experimental procedure. The control group is used for comparison.\n\nAnswer the following question:\n\n\n\nWhy was it important that Daniel’s experiment included a control group?\n\n\n\nSolution\n\nIf there was no control group, then there would be no way to compare the effect of the diets (the treatments). Having a control group allows a researcher to see the effect of not taking any action. For Daniel, the control group (who ate the king’s meat and drank his wine) provided a basis for comparing the effect of the new treatment (i.e. eating pulse and drinking water.)"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#design-of-studies",
    "href": "1-Getting_Started/5-Stat_Process.html#design-of-studies",
    "title": "The Statistical Process",
    "section": "Design of Studies",
    "text": "Design of Studies\nMost research projects can be classified into one of two basic categories: observational studies or designed experiments. In an experiment, researchers control (to some extent) the conditions under which measurements are made. In an observational study, researchers simply observe what happens, without controlling the conditions under which measurements are made. Both types of study follow the five steps of the Statistical Process.\n\n\nDesigned Experiments\nIn a designed experiment, researchers manipulate the conditions that the participants experience. They often do this by randomly assigning subjects to one of two groups, a “treatment” group (sometimes called the experimental group) and a “control” group (though this could be second treatment group instead of a control group). The experiment is typically conducted by applying some kind of treatment to the subjects in the treatment group and observing the effect of the treatment. Those in the control group do not receive the treatment and are also observed. In this way researchers can determine the effects of the treatment by comparing the treatment group results to the control group results. The following example illustrates the use of these two groups.\nJonas Salk’s First Polio Vaccine Trial\nBeginning around 1916 and through the 1950s, a mysterious plague attacked infants and children. Symptoms included excruciating muscle pain and a stiff neck. This illness, which became known as poliomyelitis or simply “polio,” left children disfigured, paralyzed, and sometimes even dead.\nWhile working as a researcher at the University of Pittsburgh School of Medicine, Dr. Jonas E. Salk developed a vaccine that might help prevent the spread of this disease. He conducted what has become one of the most famous designed experiments in history.\nThis short video below provides a compelling summary of the famous Jonas Salk vaccine experiment. As you watch, notice each of the 5 steps of a statistical study in this study.  \nAs explained in the video, in the first Salk trial almost 1.1 million children participated in the study. Even though the sample size was large, flaws in the study design rendered the results useless.\nUndaunted, Dr. Salk fixed the problems with the design and enrolled hundreds of thousands of additional children for the second phase of his study. In all, over 1.8 million infants and children participated in this experiment, making it the largest drug trial to date.\n\nStep 1: Design the study.\nThe participants in a study are commonly called subjects. Sometimes subjects are called experimental units or simply units. In the Salk trials, the children who participated were the subjects.\nSubjects (the children) were randomly assigned to one of two groups. The first group was given the experimental vaccine, the treatment. The treatment is the new or experimental condition that is imposed on the subjects. The subjects who receive the treatment make up the treatment group.\nThe second group was given a control or placebo. In this study, the control was an injection that looked just like the vaccine, but contained a harmless saline solution. The control group or placebo group is made up of the subjects assigned to receive the control.\nThis study was double blind. Neither the children’s parents nor their doctors knew whether a particular child received the treatment or the control. Both parties were blinded to this information.\nBecause the children were assigned to the groups randomly, the two groups should be similar. If the vaccine is not effective, the number of future cases of polio should be about the same in each group. However, if Salk’s vaccine helped to prevent the spread of polio, then fewer cases should occur in the vaccinated group.\n\nAnswer the following questions:\n\n\n\nSome children can be identified as having a higher risk of developing polio. Would it have been better if they were assigned to the treatment group so they could get the vaccine?\n\n\n\nSolution\n\n\nNo. The two groups need to be as similar as possible. Specifically, the people in the treatment group need to have the same potential (on average) of contracting polio as the people in the control group. If we put the people who are at a higher risk of developing polio in the treatment group, we run the risk of having more people in the treatment group getting polio simply because they are more likely to get it, whether they are vaccinated or not. Likewise, we might have fewer people in the control group getting polio just because they are less likely to get it, whether they are vaccinated or not.\nThese two effects would create a bias against the vaccine, by making the vaccine look like it doesn’t work, or doesn’t work as well as it does. It might also make it appear that people who aren’t vaccinated stay healthy and the vaccine is not needed. There is even a chance that people will conclude that the vaccine actually gives people polio.\nRandomly assigning subjects to the two groups tends to yield groups with similar characteristics—in this example, similar potential for contracting polio. Randomly assigning subjects to groups therefore defends us against problems like those mentioned in the previous paragraph.\n\n\n\nWhy is it important for the subject and those who assess the health of the subject to be unaware of whether or not that child received the vaccine?\n\n\n\nSolution\n\n\nSubjects: Suppose a subject in the study thinks they’re being treated. It has been documented that subjects with such knowledge tend to show improvement whether they are receiving the treatment or not. To see why, consider how you might feel and act if you were told you had been vaccinated. You might have a more hopeful outlook, leading to healthier living habits such as better hygiene and nutrition. Such changes would tend to reduce your chance of contracting polio whether you’ve received the vaccine or not. This might make the vaccine look like it works better than it does. It also might make the vaccine look like it works, even if it doesn’t.\nNow suppose subjects in the control group know they are not being treated. This can also change the way they feel and act, in ways that can make them more likely to contract polio than they would be if they weren’t in the study. This could make it look like the incidence of polio among unvaccinated persons is higher than it is, again making the vaccine look like it works better than it does.\nTo reduce bias caused by such errors, subjects should not know to which group they are assigned.\nResearchers: Suppose a researcher assessing the health of a subject is told that the subject is in the control group. It has been documented that in such a case, the researcher is more likely to record that the subject has symptoms even if the subject is not actually in the control group. This makes it look like unvaccinated persons are more likely to get polio than they really are, which makes it look like the vaccine works better than it does.\nThere are other effects of knowing to which group the subject belongs, such as doctors treating or advising the patient differently than they would without such knowledge. Such differences can make it harder to tell whether the vaccine works, and how well.\nTo reduce bias caused by such effects, those assessing the health of the subjects should not be told to which group the subject belongs.\n\n\n\n\nStep 2: Collect data.\nThe researchers followed up with each child to determine if they contracted polio. They recorded the number of children in each group that developed polio during the study period. Not all of Salk’s experiments were double-blind. Here is a summary of the results from the regions where a double-blind study was conducted (Francis et al., 1955; Brownlee, 1955):\n\n\nChildren Who Developed Polio\n\n\n\n\n\n\n\nYes\n\n\n\n\nNo\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\nTreatment Group\n\n\n\n\n57\n\n\n\n\n200,688\n\n\n\n\n200,745\n\n\n\n\n\n\nPlacebo Group\n\n\n\n\n142\n\n\n\n\n201,087\n\n\n\n\n201,229\n\n\n\n\n\n\nStep 3: Describe the data.\nOne way to summarize the data is to compute the proportion of children in each group that developed polio. The proportion of children in the treatment group that developed polio during the study period is:\n\\[ \\frac{57}{200745} = 0.000~283~9 \\]\n\nAnswer the following questions:\n\n\n\nCalculate the proportion of children in the placebo group that developed polio during the study period.\n\n\n\nSolution\n\n\n\\[ \\displaystyle{\\frac{142}{201229} = 0.000~705~7} \\]\n\n\n\nCompare the two proportions. What do you observe?\n\n\n\nSolution\n\n\nThe proportion of children in the placebo group that develop polio during the study period was more than double the proportion of children in the treatment group that developed polio during the study period. That suggests that the treatment is effective in reducing the proportion of children that will develop polio.\n\n\n\n\nStep 4: Make inferences\nCareful statistical analysis of the records suggested that this difference was so great that it was attributable to the vaccine and not to chance. Assuming that the vaccine had no effect, the probability that the difference in the proportions between the two groups would be at least as extreme as the difference Dr. Salk observed was very low: 0.00000000093. Because this probability is so small, it is highly unlikely that these results are due to chance.\n\n\nStep 5: Take action\nOnce it was clear that the vaccine was effective, children who were unvaccinated or had received the placebo were given Salk’s vaccine. Since 1954, there has been a marked decrease in the number of polio cases worldwide (Offit, 2005). Public health researchers are striving to eradicate this disease entirely.\n\n\n\nObservational Studies\nIn an observational study researchers observe the responses of the individuals, without controlling the conditions experienced by the individuals. Therefore, they do not assign the participants to treatment or control groups.\nObservational studies commonly occur in business settings. One example is a financial audit. The purpose of a financial audit is to assess the accuracy of a company’s financial business practices. ImmunAvance Ltd., a non-government health care organization, hired the Accounting Office at Global Optimization Unlimited to perform an independent audit of their financial practices. ImmunAvance provides inoculation and other preventative health care services in rural African communities.\n\n\n\n\n\nSampling Methods\nStep 2: Collect data\nThere are several procedures that can be used to select a random sample from a population, including: simple random sampling (SRS), stratified sampling, systematic sampling, cluster sampling, , and convenience sampling (or, haphazard sampling). These are examples of sampling methods.\n\nRandom Sampling Methods\nA simple random sample (SRS) is the best method for obtaining a sample from a population. This method allows each possible sample of a certain size an equal chance at being selected as the chosen sample. A difficulty of this method is that a list of all of the items in the population must be accessible before the sample is taken. Often, we obtain a SRS by allowing a computer to randomly select a certain number of items from the full list of the population. It is akin to the idea of putting all of the names into a hat, shaking them up, and randomly drawing out a few.\n\nFor example, suppose there are 18,000 students in the population of a certain university. School officials can use a computer to randomly choose values between 1 and 18,000 to identify which students are to be selected to complete a survey. In Excel, the command to obtain a random number between 1 and 18,000 is sample(1:18000, 1). A simple random sample can be obtained any time there is a complete list of the items to be sampled and they are all accessible. All the statistical procedures in this course assume that simple random sampling has been used. But in practice, the SRS is often difficult (or impossible) to implement.\n\nA stratified sample is when the items to be sampled are organized in groups of homogeneous (similar) items called strata, then a simple random sample is drawn from each of these strata. Stratified sampling works well when the items are similar within each stratum and tend to differ from one stratum to another. We often use stratified sampling in order to obtain a sample in such a way that we can make comparisons between each of the groups (or strata).\n\nFor example, in obtaining a sample of students from a university, school officials could define the strata as: (1) freshman, (2) sophomores, (3) juniors, and (4) seniors. A simple random sample could then be obtained from each of these strata. This would ensure that each class rank of students was represented in the sample. It would also allow the school officials to see how freshman, sophomore, junior, and senior level students compared in their answers to a survey.\n\nA systematic sample is where every \\(k^{\\text{th}}\\) item in the population is selected to be part of the sample, beginning at a random starting point. Systematic sampling works well when the items are in a random, but sequential ordering. If the items are not arranged randomly, a systematic sample can miss important parts of the population.\n\nFor example, consider a fast food company where every 10th customer is given the opportunity to compete a satisfaction survey in exchange for a small discount coupon towards their next purchase. An airport security line also often implements a procedure where every 100th (or so) person is selected for a more “in depth” security examination. Similarly, factories that use assembly lines will pull say every 500th item from the assembly line to perform a quality control check on the item.\n\nA cluster sample (sometimes called a block sample) consists of taking all items in one or more randomly selected clusters, or blocks. When the variation from one block to another is relatively low, compared to the variation within the block, cluster sampling is a reasonable way to get a sample.\n\nFor example, ecologists could draw grids on a map of a forest to create small sampling regions, or sampling clusters. Then, by randomly selecting one or two of these clusters from the map, the ecologists could go to the areas marked on the map and document information on the health of every tree they find in those clusters. This is a practical way to get a sample in this case because the ecologists only have to go to a few areas of the forest, but are still able to obtain a random sample of all of the trees in the forest. It is also worth noting that the ecologists would not be interesting in comparing the health of the trees from the selected clusters to each other like they would in a stratified sample. Instead, they are just looking for a feasible way to obtain a single random sample of all of the trees in the forest, but want to keep their traveling time to a minimum while collecting their sample. In contrast, to obtain a simple random sample of trees from the same forest, the ecologists would first have to go out and number every tree in the entire forest. Then they would need to use a computer to randomly pick which trees to collect data on. Finally, they would then have to go back to the forest and collect data on the selected trees from across the entire forest. Such an approach just isn’t feasible in practice, so we are willing to settle instead for the cluster sample.\n\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\nAs an example of a convenience sample, an auditor could haphazardly select items from a filing cabinet. This is frequently done when a quick and simple sample is needed, but may not yield a sample that represents the population well. When possible, convenience samples should be avoided.\n\n\n\n\n\nTypes of Data\nWhenever we collect data, we record information about the things we are studying. There are two basic types of data that can be recorded: quantitative measurements and categorical labels. We will call these types of data simply “quantitative” or “categorical” variables. We use the word “variable” to denote the idea that the quantitative measurements or categorical labels can vary from person to person, or item to item, in our study.\nQuantitative variables provide measurement information on each individual (or item) in our study. They represent things that are numeric in nature; things that are measured. They often include units of measurement along with the quantitative value of the measurement. For example, the heights of children measured in inches (or centimeters), or their weight measured in pounds (or kilograms). For a quantitative variable, it makes sense to apply arithmetic operations to the data (such as adding values together, computing the average of the values, or comparing two values). If one child weighs 30 pounds (13.61 kg) and a second child weights 60 pounds (27.22) then the second child is twice as heavy as the first.\nCategorical variables allow us to place each individual (or item) into to a specific category. Categorical variables are labels, and it does not make sense to do arithmetic with them. For example the gender of a newborn child, the ethnicity of an individual, a person’s job title, the brand of phone they own, or the area code of a telephone number, etc are all categorical variables. Notice that although a telephone number consists of numbers, it is not a quantitative measurement. It does not make sense to double someone’s phone number, to average phone numbers together, or to say one phone number is half the size of another. But the area code of the phone number gives information about the region where the phone number was first initiated, which is categorical information.\nNOTE: We often refer to the list of distinct category names in a categorical variable as “levels”.\nReturning to the sample accounts receivable record, we find this data to have information on both types of variables.\n\nAnswer the following question:\n\n\n\nFor each of the following variables taken from this accounts receivable record, indicate whether the variable is quantitative or categorical.\n\n\nTerms\n\n\n\nSolution\n\n\nThe variable “Terms” is categorical. It classifies the invoice by the terms of payment for that invoice.\n\n\n\nAccount number\n\n\n\nSolution\n\n\nThe variable “Account number” is categorical. Even though the account number is given a number, it is actually functioning as a label. It is not something that is counted or measured. It does not make sense to do arithmetic operations (like adding 1 or multiplying by 2) to the account number.\n\n\n\nInvoice amount\n\n\n\nSolution\n\n\nThe variable “Invoice amount” is quantitative. It makes sense to do arithmetic operations to this value. For example, the amount of Invoice 5745 (which is $990.00) is somewhat more than twice as much as that of Invoice 2378 (which is $478.00).\n\n\n \n\n\nStep 3: Describe the data\nAfter auditors collect a sample and compile the data, they review the evidence. Auditors may use graphs or compute numbers (such as the average) to summarize the evidence they found."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#making-inferences-hypothesis-testing",
    "href": "1-Getting_Started/5-Stat_Process.html#making-inferences-hypothesis-testing",
    "title": "The Statistical Process",
    "section": "Making Inferences: Hypothesis Testing",
    "text": "Making Inferences: Hypothesis Testing\nStep 4: Make inferences\nAuditors use the information drawn from the sample to form an opinion about the population. Whenever sample data is used to infer a characteristic of a population, it is called making an inference. Inferential statistics represents a collection of methods that can be used to make inference about a population. Based on the documents reviewed, the auditors assess if the company is conducting its business in a proper manner.\nWhen conducting an audit, the implicit assumption is that transactions have been posted properly. As auditors sample the company’s records, they are looking to see if everything is consistent with the original assumption that all transactions have been posted properly. It would only be in the case of discovering suspicious activity or evidence of fraudulent reporting that the auditors would change their belief about the company and accuse the company ImmunAvance of falsely reporting on their financial statements.\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nThere is a formal procedure for determining when enough evidence has been found to make accusations of fraud. Later this semester, after we establish some foundational principles of statistics, we will study these statistical methods in depth. Of course, these methods can be used for much more than just determining if a company has reported their financial statements fraudulently. So we will look at many different ways these statistical procedures can be applied to research and industry.\nFor ImmunAvance’s audit, based on the samples of financial statements that had been selected, while there were a few errors in the documents, there was not evidence dramatic enough to claim that the company had been fraudulent. So the company passed their audit.\n\nStep 5: Take Action\nThe auditors prepare a report in which they give their opinion on the status of the company’s current operations.\nSince there was not enough evidence to suggest that ImmunAvance’s financial statements were fraudulent, the auditor’s conclusion is that no adjustment is necessary. The few observed discrepancies were apparently just the result of random chance errors, not the deliberate falsefying of information."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#summary",
    "href": "1-Getting_Started/5-Stat_Process.html#summary",
    "title": "The Statistical Process",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe Statistical Process has five steps: Design the study, Collect the data, Describe the data, Make inference, Take action. These can be remembered by the pneumonic “Daniel Can Discern More Truth.”\nIn a designed experiment, researchers control the conditions of the study, typically with a treatment group and a control group, and then observe how the treatments impact the subjects. In a purely observational study, researchers don’t control the conditions but only observe what happens.\nThe population is the entire group of all possible subjects that could be included in the study. The sample is the subset of the population that is actually selected to participate in the study. Statistics use information from the sample to make claims about what is true about the entire population.\nThere are many sampling methods used to obtain a sample from a population. The best methods use some sort of randomness (like pulling names out of a hat, rolling dice, flipping coins, or using a computer generated list of random numbers) to avoid bias.\n\n\nA simple random sample (SRS) is a random sample taken from the full list of the population. This is the least biased (best) sampling method, but can only be implemented when a full list of the population is accessible.\nA stratified sample divides the population into similar groups and then takes an SRS from each group. The main reason to use this sampling method is when a study wants to compare and contrast certain groups within the population, say to compare freshman, sophomores, juniors, and seniors at a university.\nA systematic sample samples every kth item in the population, beginning at a random starting point. This is best applied when subjects are lined up in some way, like at a fast food restaurant, an airport security line, or an assembly line in a factory.\nA cluster sample consists of taking all items in one or more randomly selected clusters, or blocks. For example, ecologists could draw grids on a map of a forest to create small sampling regions and then sample all trees they find in a few randomly selected regions. Note that this differs from a stratified sample in that only a few sub-groups (clusters) are selected and that all subjects within the selected clusters are included in the study.\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\n\nThe best way to avoid bias when trying to make conclusions about a population from a single sample of that population is to use a random sampling method to obtain the sample.\nQuantitative variables represent things that are numeric in nature, such as the value of a car or the number of students in a classroom. Categorical variables represent non-numerical data that can only be considered as labels, such as colors or brands of shoes."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#references",
    "href": "1-Getting_Started/5-Stat_Process.html#references",
    "title": "The Statistical Process",
    "section": "References",
    "text": "References\nBible Dictionary, “Pulse” at http://churchofjesuschrist.org/scriptures/bd/pulse.\nBrownlee, K. A. (1955). Statistics of the 1954 polio vaccine trials. Journal of the American Statistical Association, 50(272), pp. 1005-1013.\nFrancis, T., et. al. (1955). An evaluation of the 1954 poliomyletis vaccine trials. American Journal of Public Health and the Nation’s Health, 45(5)\nOffit, P. A. (2005). Why are pharmaceutical companies gradually abandoning vaccines? Health Affairs, 24(3), 622-630. doi:10.1377/hlthaff.24.3.622"
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html",
    "href": "1-Getting_Started/3-Introducing_R.html",
    "title": "Introducing R",
    "section": "",
    "text": "The file you are now reading is created using the Markdown language. Think of Markdown like Microsoft Word on steroids. The difference is, instead of clicking through drop-down menus to change font, line spacing, etc. you need to type in the instructions inside the document.\nThis file has the extension .qmd which stands for Quarto Markdown. All assignments and activities will be given as .qmd files.\nThe upside is that you can easily write nice looking reports and directly incorporate code output including summary tables and graphs.\n\n\nAt the very top of the document, you will see some blue and green text between two sets of ---. This gives instructions to the computer about what kind of document to make. It is called the YAML. What YAML stands for is up for debate. But the instructions are necessary for building the document and making it look nice.\nThe YAML above is very basic. We may expand on it a little as the semester progresses. But for now, this document can be made into an HTML file by “Rendering” it.\n\n\n\nAbove, you should see a “Render” button. Click on it and see what happens.\nWe can make section headers, type text, include code chunks and stitch it all together into a decent-looking report with only a few lines of text."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-yaml",
    "href": "1-Getting_Started/3-Introducing_R.html#the-yaml",
    "title": "Introducing R",
    "section": "",
    "text": "At the very top of the document, you will see some blue and green text between two sets of ---. This gives instructions to the computer about what kind of document to make. It is called the YAML. What YAML stands for is up for debate. But the instructions are necessary for building the document and making it look nice.\nThe YAML above is very basic. We may expand on it a little as the semester progresses. But for now, this document can be made into an HTML file by “Rendering” it."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#rendering-the-document",
    "href": "1-Getting_Started/3-Introducing_R.html#rendering-the-document",
    "title": "Introducing R",
    "section": "",
    "text": "Above, you should see a “Render” button. Click on it and see what happens.\nWe can make section headers, type text, include code chunks and stitch it all together into a decent-looking report with only a few lines of text."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#this-is-a-section-subheader",
    "href": "1-Getting_Started/3-Introducing_R.html#this-is-a-section-subheader",
    "title": "Introducing R",
    "section": "This is a section subheader",
    "text": "This is a section subheader\n\nsub-subheader\nYou get the point."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-combine-function-c",
    "href": "1-Getting_Started/3-Introducing_R.html#the-combine-function-c",
    "title": "Introducing R",
    "section": "The Combine Function, c()",
    "text": "The Combine Function, c()\nSometimes I want to type in data manually. The simplest way to accomplish this is to use the combine function, c(). Think of c() as a way to combine a list of things that are alike. For example, I can create a new dataset using the combine function:\n\nc(21, 22, 18, 19, 19, 20, 22)\n\n[1] 21 22 18 19 19 20 22\n\n\nI can create a list of names as well, but I have to put characters in quotes:\n\nc(\"Billy\", \"Joel\", \"Bobby\", \"Dylan\", \"Carly\", \"Rae\", \"Jeppson\")\n\n[1] \"Billy\"   \"Joel\"    \"Bobby\"   \"Dylan\"   \"Carly\"   \"Rae\"     \"Jeppson\"\n\n\nRunning the above code isn’t very helpful because I won’t be able to do anything with those lists. Computers are so literal. All the code did was print out a list that I made. If I want to be able to use those lists, I need to give them a name."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-assignment-operator",
    "href": "1-Getting_Started/3-Introducing_R.html#the-assignment-operator",
    "title": "Introducing R",
    "section": "The Assignment Operator",
    "text": "The Assignment Operator\nIn R, we like to name things. You can name pretty much anything including datasets, graphs, output, etc. When we want to assign a name to something we use &lt;-, which is made up of a “less than” sign and a dash. We put the name on the left side.\nLet’s demonstrate by naming our lists for ages and names above.\n\nages &lt;- c(21, 22, 18, 19, 19, 20, 22)\n\nstudents &lt;- c(\"Billy\", \"Joel\", \"Bobby\", \"Dylan\", \"Carly\", \"Rae\", \"Jeppson\")\n\nNow if I want to refer to those data, I can just refer to the name:\n\nages\n\n[1] 21 22 18 19 19 20 22\n\nstudents\n\n[1] \"Billy\"   \"Joel\"    \"Bobby\"   \"Dylan\"   \"Carly\"   \"Rae\"     \"Jeppson\"\n\n\nSHORT KEY: You can quickly create the assignment operator by hitting “Alt” plus “-” on a PC, or “Option” plus “-” on a Mac."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#importing-raw-data",
    "href": "1-Getting_Started/3-Introducing_R.html#importing-raw-data",
    "title": "Introducing R",
    "section": "Importing Raw Data",
    "text": "Importing Raw Data\nIn this course, we will read raw data into R and use it for analysis and visualizations. Say there is a dataset stored online that we would like to bring into R to anlayze. We can use the import() function from the rio library to load the data.\nRunning the following code instructs R to import that data…and nothing else.\n\nread.csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\nAgain, I can’t do anything with it because it hasn’t been stored anywhere. If I want to be able to do anything with the data, I must somehow store it in R. I do this by assigning a name to the data. I can make up any name I like as long as it begins with a letter and has no spaces. Let’s call it “steve”.\n\nsteve &lt;- read.csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\nNotice how nothing printed out. This is because computers, being literal, assigned a dataset to something I named “steve”. I haven’t told the computer to do anything with it yet. But now I have an R “object” that I can use."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#viewing-raw-data",
    "href": "1-Getting_Started/3-Introducing_R.html#viewing-raw-data",
    "title": "Introducing R",
    "section": "Viewing Raw Data",
    "text": "Viewing Raw Data\nThere are several ways to view raw data in R:\n\nRun “steve” by itself\nuse the View() function\n\nRun each line individually by putting the curso on the line you want to run and hitting:\n\nPC: ctrl+Enter\nMac: CMD+Enter\n\n\nsteve\n\nView(steve)\n\nBEST PRACTICE: It is best to name objects in R with descriptive names relating to what the object is. big5_data would be a better name than steve, for example, because this is data about the big 5 personality traits."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#selecting-a-column-from-a-dataset",
    "href": "1-Getting_Started/3-Introducing_R.html#selecting-a-column-from-a-dataset",
    "title": "Introducing R",
    "section": "Selecting a Column From a Dataset",
    "text": "Selecting a Column From a Dataset\nDatasets usually have many columns. When we want to select one column from a dataset to summarize, we can use the $ operator. For example, if we want to look at only the scores for Extroversion, we can run the following code:\n\nsteve$Extroversion \n\nThis prints out only the values in the Extroversion column.\nWATCHOUT: The column name has to be typed EXACTLY as it is in the dataset.\nR makes it a little easier because after you type the $ it will bring up a dropdown menu and you can select the column you’re interested in.\nSPACES: If there are spaces in a column name you have to wrap the whole column name using backticks `. On most keyboards, this is on the same key as the ~ next to the 1 key. It’s always easier to use the dropdown menu for complicated column names.\nNOTE: The drop down only works when you’re typing forward. If you delete text back to the dollar sign it won’t show up. To get it back, delete the dollar sign and retype it.\nWe will use the selection $ all very frequently to specify which columns we want to use for data summaries, visualizations, and analyses.\nFor example, I can easily get the mean Openness score by putting steve$Openness into the mean() function:\n\nmean(steve$Openness)\n\n[1] 71.81111"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html",
    "href": "1-Getting_Started/1-Course_Intro.html",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#lesson-outcomes",
    "href": "1-Getting_Started/1-Course_Intro.html#lesson-outcomes",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#welcome-to-the-course",
    "href": "1-Getting_Started/1-Course_Intro.html#welcome-to-the-course",
    "title": "Course Introduction",
    "section": "Welcome to the Course!",
    "text": "Welcome to the Course!\nIn this course, you will explore important connections between the academic discipline of Statistics and the world around us. By pondering these ideas, your understanding of statistics will increase, as will your knowledge and testimony of the restored Gospel of Jesus Christ.\nThis course has been designed to help you slowly build up a knowledge base of ideas and skills. Not all of these ideas and skills will come easily. It takes a lot of work and practice before some things will even start to make sense, so you should not be surprised to find that it may take you a little time to comprehend these ideas. Just be patient. Once you’re far enough into the course, the ideas will start to come together, and you will see how much progress you have really made. You will understand what this course is all about, and you will be glad you persisted in your efforts to learn."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-description",
    "href": "1-Getting_Started/1-Course_Intro.html#course-description",
    "title": "Course Introduction",
    "section": "Course Description",
    "text": "Course Description\nThis course covers the following topics as they are applied to Statistics: graphical representations of data, measures of center and spread; elementary probability; sampling distributions; correlation and regression; statistical inference involving means, proportions, and contingency tables.\n\n\nCourse Learning Outcomes\nIn this course, we will:\n\nSummarize data numerically and graphically using R programming language\nMake decisions regarding situations with inherent randomness\nApply probability distributions to investigate questions\nEmploy confidence intervals in various situations\nImplement tests of diverse hypotheses\nCommunicate the results of statistical analyses to relevant audiences\n\n\n\n\nHow the Outcomes will Be Assessed\nWhile you may not be tested on everything you learn in this course, the instructor will be assessing your mastery of the Course Learning Outcomes. You may also be asked to create reports with quality visualizations and analyze new datasets. At times, the instructor may assess your performance of a skill, or the instructor may assess products you create using particular skills. In addition, the instructor may engage in personal communication with you to determine how well you understand the course content.\n\n\n\nKeys to Success\n\nFive Principles of the Learning Model\nYou will experience much deeper learning if you follow the Five Principles of the BYU-Idaho Learning Model\n\nExercise Faith: Exercise faith in the Lord Jesus Christ as a principle of action and power.\nLearn by the Holy Ghost: Understand that true teaching is done by and with the Holy Ghost.\nLay Hold on the Word of God: Lay hold of the word of God.\nAct for Themselves: Act for yourself and accept responsibility for learning and teaching.\nLove, Serve, and Teach One Another: Love, serve, and teach other students in your classes.\n\n\n\nThree Process Steps of the Learning Model\nYou will learn more in less time if you follow the Three Process Steps of the BYU-Idaho Learning Model\n\nPrepare: This involves (a) spiritual preparation, (b) individual preparation, and (c) group preparation.\nTeach One Another: You should (a) be on time, (b) pray together, and (c) actively engage with other students.\nPonder/Prove: You should (a) ponder what you have learned, (b) record your learning, and (c) pursue unanswered questions and discuss what you learn with others.\n\nIf you feel confused or have questions about anything in the lesson, take immediate action (Exercise Faith; Act for Themselves) and talk with your classmates, the teaching assistant, or the instructor (Love, Serve, and Teach One Another).\nTeach One Another\nAt BYU-Idaho, an “A” student will demonstrate “diligent application of Learning Model principles, including initiative in serving other students” (BYU-Idaho Catalog). In this class, you will have the opportunity to work with other students.\nDoctrine and Covenants 84:106 states, “And if any man among you be strong in the Spirit, let him take with him him that is weak, that he may be edified in all meekness, that he may become strong also.” In the spirit of this revelation, you will have the opportunity to help others in the class when you have developed an understanding of a principle. Likewise, you will be able to receive help from others (peers, tutors, TA, and your instructor) when you are still working to understand concepts.\nIn a spirit of love and service, please reach out to others. You are not graded on a curve. If someone else does well, it does not adversely affect you. Research has shown that students who help other students to understand the material gain a much deeper grasp on the concepts of the course. Please take opportunities to help your peers succeed."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-structure",
    "href": "1-Getting_Started/1-Course_Intro.html#course-structure",
    "title": "Course Introduction",
    "section": "Course Structure",
    "text": "Course Structure\nThis course consists of lessons presented in a topical order in which concepts and skills learned in the earlier lessons provide the requisite knowledge to succeed in later lessons. If the general order of the lessons doesn’t make sense at first, don’t worry. It will all come together in the end, and you’ll see the reasoning behind why the lessons have been presented in this particular order.\nYour main goal as a student will be to complete all of the learning activities within each lesson by their due dates every week. These activities follow a consistent weekly schedule, and it will be up to you to make sure that you keep on pace with all your assignments. These weekly activities may include the following:\n\nReading assigned texts or viewing presentations before class\nCompleting the practice problems\nParticipating in group discussions and assignments with other class members\nWriting papers and/or developing presentations\nParticipating in meetings with the instructor, teaching assistants, and other students\n\nA typical lesson will include a preparation reading, in-class and practice application activites. Because not all material is equally challenging, some lessons will span multiple days. But expect to cover 2-3 lessons a week.\nWhile a general flow of reading, classwork and practice will be the typical lesson flow, there may be adjustments to the schedule from time to time. Changes are typically due to adjusting the pace of the material to support student learning. Please be flexible as we adapt the pace to suit the needs of the class.\nYou should create a study schedule that will keep you on pace throughout the semester. This is a rigorous course with a lot of subject matter to cover, and it can be extremely difficult to recover if you fall too far behind in your work. So, please make every effort to study on a regular basis and get your work turned in on time."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-materials",
    "href": "1-Getting_Started/1-Course_Intro.html#course-materials",
    "title": "Course Introduction",
    "section": "Course Materials",
    "text": "Course Materials\nThis course has been designed with the student in mind. Every effort has been made to provide a high quality experience at the lowest possible cost.\nTextbook\nTo keep costs as low as possible for students and their families, no physical textbook is required for this class. The readings for this course are provided on this website and will continue to be available to you after the course is completed. Please report any problems with the textbook (links not working, loading slowly, inability to view images, etc.) to your instructor.\nYou may also download all coursework at the main page. This includes all class notes, practice problems and application activities.\nComputer Equipment\nYou will need a laptop with the ability to install software locally.\nNOTE: It is possible to complete this course with a Chromebook or other cloud-based device, but it adds significant complications and stress. For those who are new to programming and statistics, this can be the difference between success and failure in the class."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-resources",
    "href": "1-Getting_Started/1-Course_Intro.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nPeer Support\nYour experience in this course will be enhanced as you work with other students to learn and grow together.\nHelp Desk\nThe BYU-Idaho Help Desk has been established to help students with technological problems related to approved course software. You can access the Help Desk at any time in three ways: - Walk-in: The Help Desk is located in room 322 of the McKay Library - Call in: 208-496-1411 (toll free) - Email: helpdesk@byui.edu Additional information is available at the Help Desk web page: http://www.byui.edu/helpdesk/\nWhen you have technical problems with I-Learn, you should first try contacting the Help Desk before you contact your instructor. They are connected with the IT support staff who can resolve problems with I-Learn. Please take a moment now to look at the Help Desk web page. That way, if a problem does arise later on in the course, you will know where to go for help.\nTutoring Center\nThe BYU-Idaho Study Skills/Tutoring Center is a powerful resource for students who would like a little extra help with a course. The Tutoring Center is located in the McKay Library in room 272. This is in the east wing of the second floor.\nThe Tutoring Center provides many services to help students succeed: - Individual tutors - Walk-in tutoring in the Math Study Center (McKay 266 & 270) - Virtual tutoring\nPlease take 5 minutes to explore the Study Skills/Tutoring Center web site.\nData Science Lab\nThere is a Data Science Lab open for walk-ins. They are able to help with any R-related questions and programming, but you are also likely to find someone who has taken stats classes and can help you with the statistics as well.\nLocation and hours can be found at the link. It is usually in the STC 353, on most days but Wednesday.\nFaculty Support\nYour instructor is committed to your success. If you have any needs or concerns, please contact your instructor for help. If you feel yourself getting behind or struggling, talk to your teacher right away. If caught in time, a small problem can be addressed quickly before it grows."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html",
    "href": "1-Getting_Started/2-Installing_R.html",
    "title": "Installing R",
    "section": "",
    "text": "In this course, we will use RStudio to perform necessary visualizations and analyses and to create web-based reports.\nR and RStudio are not the same thing. R is like the engine in a car, and RStudio is the driver’s controls. We don’t need to know the guts of a combustion engine to drive, and we don’t need to know what’s going on in R, but we do need it on our computer.\nTo install R and RStudio on your computer, please follow these instructions:\n\nGo to the R-Studio page and follow the instructions:\na. Install R. The “Download and Install R” button will take you to the R website. You will have to click through a few links to get to the download.\n\nFor PC: “Download R for Windows” &gt; “Install R for the first time” &gt; Whatever the latest version is will be at the top.\n\nFor Mac: “Download R for Mac” &gt; There will be 2 options on the left with the latest version of R (R-4.4.3, for example). Which version you download depends on what kind of processor chip you have, but most will work with the version ending in “arm64.pkg”\n\nReturn to the R-studio download page and click the “Download RStudio Desktop” button. Run the downloaded program and accept the defaults.\n\nOnce the RStudio installer downloads, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps.\n\n\nNOTE: If you are using a Chromebook or other “web browsing only” computer that will not allow you to install software locally, then set up an account at RStudio Cloud instead of installing R and RStudio as shown above. Use your BYU-I email and user ID."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#chromebooks-and-cloud-based-notebooks",
    "href": "1-Getting_Started/2-Installing_R.html#chromebooks-and-cloud-based-notebooks",
    "title": "Installing R",
    "section": "",
    "text": "NOTE: If you are using a Chromebook or other “web browsing only” computer that will not allow you to install software locally, then set up an account at RStudio Cloud instead of installing R and RStudio as shown above. Use your BYU-I email and user ID."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#keep-calm",
    "href": "1-Getting_Started/2-Installing_R.html#keep-calm",
    "title": "Installing R",
    "section": "Keep Calm",
    "text": "Keep Calm\nIf you have tried going through these instructions and it isn’t working, come to class and we will help troubleshoot."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#mac-processing-chip",
    "href": "1-Getting_Started/2-Installing_R.html#mac-processing-chip",
    "title": "Installing R",
    "section": "Mac Processing Chip",
    "text": "Mac Processing Chip\nFor Macs, Which version of R-Studio you download depends on which processing chip you have. If you followed the instructions above and R-Studio opens but gives you a big error, you need to download the other version of R linked above."
  },
  {
    "objectID": "1-Getting_Started/4-Installing_Packages.html",
    "href": "1-Getting_Started/4-Installing_Packages.html",
    "title": "Installing R Libraries",
    "section": "",
    "text": "Introduction\nR has many built-in toolboxes. R also has access to a vast array of toolboxes that we must install if we want to use them. This is like going to the Home Depot to buy a specialized toolbox and then storing it in your garage. We only have to “buy” it once.\nWe will be using 4 main libraries throughout the semester. This code will walk you through how to install, load and use each of the libraries.\n\n\nInstalling Libraries\nTo install a library, we use the install.packages(\"\") command, where we specify the library we want in the quotes inside the parentheses.\nThis is something we only have to do once for each library. The 4 main libraries we will use in the class are the tidyverse, mosaic, rio, and car libraries.\nRun the code below by clicking the green arrow at the top right of the code chunk.\nYou can also run individual lines inside of a code chunk by pressing:\n\nPC: ctrl + Enter\nMac: CMD + Enter\n\n\ninstall.packages(\"rio\")\ninstall.packages(\"mosaic\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"car\")\n\nIt may take a few minutes to install everything once you run the above code. Be patient.\nAgain, the above code only needs to be executed once. It is like going to the Home Depot to purchase a tool box for specific purposes and storing them in a tool shed.\n\n\nLoading Libraries\nOnce the toolboxes are “purchased” we still need to get them out of the tool shed when we need to use them. This is something we have to do each time we open and use R Studio for a project.\nWe use the library() function in a code chunk and insert which library we want to use inside the parentheses:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nThe above code chunk is something that will become very familiar as we will load these libraries in every activity.\nEach of the above libraries has special tools for doing managing data, creating graphs, data summarization, etc. that we will learn as we go. While we won’t use all of them in each assignment, it is easier to get in the habit of loading them all for each activity.\n\nrio is useful for loading in data of different types using a single function, import()\nmosaic has many data summarization tools\ntidyverse has a vast range of tools that make it easy to wrangle and visualize data\ncar is a little more specific and is used in this class to do one specific type of chart later in the semester.\n\nNOTE: While you only have to install libraries once, you have to load them every time you want to use one. It’s like going to the garage to get the toolbox you need for the job.\n\n\nTesting, Testing\nAfter running the code chunks above, run the code below to verify that you can read in a dataset and make data summaries:\nUse the import() function to load the dataset by running the following code chunk:\n\nwrong_site &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\") %&gt;% tibble()\n\nwrong_site\n\n# A tibble: 411 × 3\n   Wrong_Site Wrong_Patient Comments                                            \n        &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;                                               \n 1      97465        250000 On rare occasions, a medical procedure is performed…\n 2      36141        106900 or on the wrong patient.  These are called wrong-si…\n 3     102362         62307 &lt;NA&gt;                                                \n 4      69951        192800 The Washington Post highlighted a few cases in a Ju…\n 5      83242         20769 * An ophthalmologist operated on the wrong eye of a…\n 6      12824          2680 * Some men have undergone prostate cancer surgery a…\n 7     129900          4300 * One third of the cases resulted in death or serio…\n 8      51764         30819 &lt;NA&gt;                                                \n 9     145976         23214 The medical field is trying to reduce the number of…\n10     152844         26099 &lt;NA&gt;                                                \n# ℹ 401 more rows\n\n\nYou should see a printout of the first few rows of a dataset. The output will either show up below the code chunk or in the “Console” window. Either way works. I can show you how to adjust the settings in class depending on your preference."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html",
    "title": "Response and Explanatory Variables",
    "section": "",
    "text": "In this section, we will:\n\nReview the definitions of Response/Dependent variables and Explanatory/Independent variables\n\nLearn some R tools to help us look at the data (View(), glimpse(), names())\n\nLook at several datasets and identify the response variables and explanatory variables\n\nIdentify data types (categorical or quantitative)\n\nIdentify each level of a categorical variable\n\n\n\nDon’t forget, before we can use the tools in the different toolboxes, we need to retrieve them from the shed. In R, that means, loading the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\nlibrary(rio)"
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#load-the-libraries",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#load-the-libraries",
    "title": "Response and Explanatory Variables",
    "section": "",
    "text": "Don’t forget, before we can use the tools in the different toolboxes, we need to retrieve them from the shed. In R, that means, loading the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\nlibrary(rio)"
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#data",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#data",
    "title": "Response and Explanatory Variables",
    "section": "Data",
    "text": "Data\nDatasets contain rows and columns. Rows in a dataset represent individual observations or records. Each row contains all the relevant data for a single case, instance, or subject being studied. For example, in a dataset about students, each row might represent a different student, with all their associated data (such as name, age, and grade) contained within that row.\nColumns in a dataset represent the different variables or attributes being measured or recorded. Each column contains one specific type of data across all the observations. For example, in the same student dataset, there could be a column for “Name,” a column for “Age,” and another for “Grade.”\nNOTE: Column names in a dataset are considered variables.\nTogether, rows and columns provide a structured format that makes it easy to organize, analyze, and visualize the data.\n\nData Types (Review)\nIn this class, we will use the broad categories of “quantitative” and “categorical” variables to distinguish between numerical values and data that represent group information respectively. As we will see below, it’s not always as easy as it seems to tell the difference.\n\n\nQuantitative Variables\nVariables that are quantitative consists of values that represent measurable quantities.\n\n\nCategroical Variables\nCategorical data consists of values that represent categories or groups. We refer to each of the categories as “levels” of the variable.\nTo see all of the levels of a categorical variable, we can use the unique() function. The code chunk below has a list of favorite pets. The unique() function takes as an input a categorical variable and returns a list of the distinct values.\n\nfavorite_pet &lt;- c(\"cat\", \"cat\", \"dog\", \"ferret\", \"cat\", \"cat\", \"dog\", \"lizard\", \"dog\")\n\nunique(favorite_pet)\n\n[1] \"cat\"    \"dog\"    \"ferret\" \"lizard\"\n\n\nThis lists all the “levels” of a categorical variable."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#responsedependent-variable",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#responsedependent-variable",
    "title": "Response and Explanatory Variables",
    "section": "Response/Dependent Variable",
    "text": "Response/Dependent Variable\nThe response variable, also known as the dependent variable, is the outcome or the variable that researchers are interested in explaining or predicting. Its value depends on the influence of other variables. For example, in a study examining the effect of study time on exam scores, the exam score is the response variable because it changes in response to different amounts of study time."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#explanatoryindependent-variable",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#explanatoryindependent-variable",
    "title": "Response and Explanatory Variables",
    "section": "Explanatory/Independent Variable",
    "text": "Explanatory/Independent Variable\nThe explanatory variable, also known as the independent variable, is the variable that is manipulated or controlled in a statistical experiment to observe its effect on the response variable. It is considered the cause or predictor in the relationship being studied. In the example above, the amount of study time is the explanatory variable, as it is the factor that explains exam scores."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html",
    "title": "Numerical Quantitative Data Summaries",
    "section": "",
    "text": "In this section we discuss data summaries for quantitative data. After this lesson, you should be able to:\n\nUnderstand common measures of center (mean, median, mode)\n\nUnderstand common measures of spread (standard deviation, variance, percentiles), why they are important, and how to interpret them\n\nUse R to calculate measures of center and spread\n\nThroughout this section, we will use the “wrong patient” dataset which contains information about malpractice lawsuits of operations performed on the wrong patient or the wrong site on the patient.\n\n\nWe will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#reading-data-into-r",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#reading-data-into-r",
    "title": "Numerical Quantitative Data Summaries",
    "section": "",
    "text": "We will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#mean",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#mean",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Mean",
    "text": "Mean\nThe sample mean or sample arithmetic mean is the most common tool to estimate the center of a distribution. It is referred to simply as the mean. It is computed by adding up the observed data and dividing by the number of observations in the data set.\nIn Statistics, important ideas are given a name. Very important ideas are given a symbol. The sample mean has both a name (mean) and a symbol (\\(\\bar x\\), called “x-bar”).\n\\[\n  \\bar{x} \\text{ is used to denote the sample mean}\n\\]\nYou may have heard people refer to the sample mean as the average. Technically, the word average refers to any number that is used to estimate the center of a distribution. The mean, median and mode are all examples of “averages.” To avoid confusion, it is best to use the words mean, median, and mode instead of the word average, so that it is clear which “average” your are referencing.\nCalculate the mean payout for operations done on the wrong patient:\n\nmean(wrong_patient$Wrong_Patient)\n\n[1] NA\n\n\nWhen there are missing values in the dataset, the mean() function will return NA. We can make R give us a mean by telling the function to remove the NA values:\n\nmean(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 46172.05\n\n\nNOTE: The mean is a good measure of center when there are few outliers and the data are fairly symmetric."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#median",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#median",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Median",
    "text": "Median\nThe median is the middle value in a sorted data set. Half of the observations in the data set are below the median and half are above the median. To find the median, you:\n\nSort the values from smallest to largest\n\nDo one of the following:\n\nIf there are an odd number of values, the median is the middle value in the sorted list.\nIf there are an even number of values, the median is the mean of the two middle values in the sorted list.\n\n\n\nCalculate the Median payout for operations done on the wrong patient using the median() function:\n\nmedian(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 18882\n\n\nQUESTION: What does this mean?\nANSWER:\nQUESTION: Notice how much bigger the Mean is from the Median. Why is that the case?\nANSWER:\nQUESTION: Which do you think is most appropriate to use, the Mean or the Median? Why?\nANSWER:\nNOTE: The Median is a good measure of center when the data are skewed or there are large outliers."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#mode",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#mode",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Mode",
    "text": "Mode\nThe most frequently occurring value is called the mode. This only works well when you have a few possible outcomes or are counting the frequency of categories. If you have truly quantitative data, such as dollar amounts of payouts, it is unlikely to have the exact same value paid out many times.\nIf no number occurs more than once in the data set, we say that there is no mode.\nBecause there is no meaningful mode in the wrong patient data, let’s look at another example to calculate the mode:\nCalculate a Mode\nWe can tabulate the frequency of specific values using the table() function:\n\n# Create a new dataset called data2:\ndata2 &lt;- c(3,4,9,5,2,3,5,4,2,3,1,5,3,1,2,6,2,4,6,2,2,2,9,1,2,7,8,100)\n\n# The `table()` function counts up all the times specific values show up.  This works for numbers or categories:\ntable(data2)\n\ndata2\n  1   2   3   4   5   6   7   8   9 100 \n  3   8   4   3   3   2   1   1   2   1 \n\n\nThe first row of the table() output is the value being counted. The second row is the frequency of occurrence.\nQUESTION: Which value is most frequently occurring?\nANSWER:\nQUESTION: How many times did that value occur?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#why-do-we-care",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#why-do-we-care",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Why do we care?",
    "text": "Why do we care?\nImagine you are growing feed corn on 100 acres. There are 2 products you could purchase, both claim an average yield of 205 bushels/acre. Product A costs less than Product B. Which one would you plant?\nIf you were basing your decision exclusively on the average yield, you would obviously go with the cheaper option. But what if I told you that with Product A you could get anything from 85 bushels/acre to 250 bushels/acre. Product B ranges from 198 to 212 bushels/acre. Would that change your decision?\nProduct A looks much more risky now. Considering both the average AND the variability helps us make much more informed decisions.\nThe average is only half the story!"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#standard-deviation",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#standard-deviation",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nThe standard deviation is a measure of the spread in the distribution. The standard deviation for a sample is denoted by the latin letter, \\(s\\). If the data tend to be close together, then the standard deviation is relatively small. If the data tend to be more spread out, then the standard deviation is relatively large.\nKEY POINT: Think of the standard deviation as the “average distance” of each data point away from the mean.\nWe can easily calculate the standard deviation in R using the sd() function:\n\nsd(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 105986.7\n\n\nThe standard deviation of the payouts is $105,986.70. This number contains information from all the lawsuits. If the payouts had been more diverse, the standard deviation would be larger. If the payouts were more uniform (i.e. closer together), then the standard deviation would have been smaller. If all the payouts somehow had the same amount, then the standard deviation would be zero.\n\nSummary\n\n  Standard Deviation\n  The standard deviation is one number that describes the spread in a set of data. If the data points are close together the standard deviation will be smaller than if they are spread out.\n  At this point, it may be difficult to understand the meaning and usefulness of the standard deviation. For now, it is enough for you to recognize the following points:\n\nThe standard deviation is a measure of how spread out the data are.\nIf the standard deviation is large, then the data are very spread out.\nIf the standard deviation is zero, then all the values are the identical–there is no spread in the data.\nThe standard deviation cannot be negative."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#variance",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#variance",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Variance",
    "text": "Variance\nThe variance is the square of the standard deviation. The sample variance is denoted by the symbol \\(s^2\\). You found the sample standard deviation for payouts for operating on the wrong patient above is $105,986.70. So, the sample variance for this data set is \\(s^2 = 105,986.70^2 = 11,233,176,611\\).\nYou can also calculate the variance directly from data using the var() function.\n\nvar(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 11233176611\n\n\nThe standard deviation and the variance each have their own pros and cons.\nStandard Deviation is in the original units as the data which makes it easy to interpret.\nThe variance is more difficult to interpret but has many important mathematical attributes that make it more appropriate in certain situations not covered in this introductory course."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#percentiles-and-quartiles",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#percentiles-and-quartiles",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Percentiles and Quartiles",
    "text": "Percentiles and Quartiles\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, the 99th percentile is a number such that 99% of the data are at or below this value. As another example, half (50%) of the data lie at or below the 50th percentile. The word percent means \\(\\div 100\\). This can help you remember that the percentiles divide the data into 100 equal groups.\nQuartiles are special percentiles. The word quartile is from the Latin quartus, which means “fourth.” The quartiles divide the data into four equal groups. The quartiles correspond to specific percentiles. The first quartile, Q1, is the 25th percentile. The second quartile, Q2, is the same as the 50th percentile or the median. The third quartile, Q3, is equivalent to the 75th percentile.\nNOTE: Percentiles can be used to describe the center and spread of any distribution and are particularly useful when the distribution is skewed or has outliers.\nUse R’s quantile() function to calculate percentiles. This functions requires two inputs separated by a comma: the data and the desired percentile input as a decimal.\n\nTo calculate the 25th percentile for the costs of surgery done on the Wrong Site:\n\n\nquantile(wrong_patient$Wrong_Site, .25, na.rm=TRUE)\n\n  25% \n29496 \n\n\nQUESTION: What does this number mean?\nANSWER:\nThe first quartile (\\(Q_1\\)) or 25th percentile (calculated in R) of the wrong-site data is: $29,496. (This result is illustrated in the figure below.) This means that 25 percent of the time hospitals lost a wrong-site lawsuit, they had to pay $29,496 or less. The 25th percentile can be written symbolically as: P25 = $29,496. Other percentiles can be written the same way. The 99th percentile can be written as P99.\nQUESTION: What is the 13th percentile of the wrong patient data?\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n\nQUESTION: Interpret this value:\nANSWER:\nQUESTION: Find P90.\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n\nQUESTION: Half of the wrong-site lawsuits judgments were less than or equal to what value?\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n## OR....\n\n\nThe Five-Number Summary\nAnother way to summarize data is with the five-number summary. The five-number summary is comprised of the minimum, the first quartile, the second quartile (or median), the third quartile, and the maximum.\nThere is a very easy way to get the Five-Number Summary along with the mean and standard deviation. The favstats() function in the mosaic library gives us all of our favorite statistics.\nAs before, we will have to install the mosaic library once, then load it when we want to use it.\nTo find the values for a five-number summary in R, do the following\n\nInput the data into the favstats() function:\n\n\nfavstats(wrong_patient$Wrong_Patient)\n\n min   Q1 median      Q3     max     mean       sd   n missing\n   0 3900  18882 50145.5 1250000 46172.05 105986.7 176     235"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#your-turn",
    "href": "2-Descriptive_Statistics/1-Numerical_Summaries_Quantitative_Data.html#your-turn",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a summary statistics table for the cost of operating on the wrong site:"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html",
    "title": "Graphical Quantitative Data Summaries",
    "section": "",
    "text": "In this section, you will learn the basic visualizations for quantitative variables. After completion, you should be able to:\n\nInterpret data presented in a histogram\n\nIdentify left-skewed, right-skewed and symmetric distributions from histograms\n\nInterpret a boxplot\n\nCreate histograms and boxplots in R\n\n\n\nWe will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#reading-data-into-r",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#reading-data-into-r",
    "title": "Graphical Quantitative Data Summaries",
    "section": "",
    "text": "We will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#histograms",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#histograms",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Histograms",
    "text": "Histograms\nHistograms are commonly used to visualize a single, quantitative variable. It provides a way to visualize how data points are spread out.\nThe key components of a histogram are:\n\nBars: Histograms consist of adjacent bars, where each bar represents a range of values, known as a “bin.” The height of each bar indicates the frequency (or count) of data points that fall within that bin.\nBins: The x-axis is divided into equal intervals (bins). Each bin captures a specific range of values. For example, if you’re plotting test scores, a bin might cover scores at 5-point intervals.\nFrequency: The y-axis shows the frequency or number of observations that fall within each bin. This allows you to see how many data points lie in each bin.\n\n\nHow to Interpret a Histogram:\n\nShape of Distribution: The overall shape of the histogram can indicate the distribution of the data.\nCentral Tendency: You can often observe where most of the data points cluster, giving an idea of the central tendency (mean, median).\nSpread: You can see how spread out the data is, which provides insights into variability.\nOutliers: Histograms can help identify any potential outliers by showing bars with significantly lower frequencies compared to others.\n\n\n\nDescribing the “Shape” of a Distribution\nWe will describe the shape of the distribution of a data set using the following basic categories:\n\nSymmetric (bell-shaped)\n\nRight skewed\n\nLeft skewed\n\nA distribution is symmetric if both the left and right side of the distribution appear to be roughly a mirror image of each other. A special symmetric distribution is a bell-shaped distribution. When data follow a bell-shaped distribution, the histogram looks like a bell. Bell-shaped distributions play an important role in Statistics and will play a role in most of the future lessons.\nA distribution is right-skewed if a histogram of the distribution shows a long right tail. This can occur if there are some very large outliers on the right-hand side of the distribution. A distribution is left-skewed if a histogram shows that it has a long tail to the left.\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\nIf a distribution has only one peak, it is said to be unimodal. The three distributions illustrated above are all unimodal distributions. Some people might argue that there are several peaks in the GPA data, so it should not be considered unimodal. Even though there are jagged bumps in the histogram, it is important to visualize the overall shape in the data. When interpreting a histogram, it can be helpful to blur your eyes and imagine the overall shape after smoothing out the bumps. If the overall trend indicates that there is more than one bump, then we do not consider the distribution to be unimodal. We will usually only work with unimodal data sets in this course.\nSome distributions have no distinct peak, others have more than one peak. When there is no distinct peak, and the histogram shows a relatively flat shape, we might say the data follow a uniform distribution. If there are two distinct peaks, a distribution is called bimodal. If there are more than two peaks, we refer to the distribution as multimodal."
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#histograms-in-r",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#histograms-in-r",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Histograms in R",
    "text": "Histograms in R\nTo make a histogram in R, you can use the histogram() function. This takes as an input a single quantitative variable:\n\nhistogram(wrong_patient$Wrong_Patient)\n\n\n\n\n\n\n\n\nQUESTION: What is the basic shape of the wrong patient procedure lawsuits?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#summary",
    "href": "2-Descriptive_Statistics/2-Graphical_Summaries_Quantitative_Data.html#summary",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA percentile is calculated in R using quantile(data, 0.#) where the 0.# is the percentile written as a decimal number. So the 20th percentile would be written as 0.2.\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, if say 80% of college students were shorter than (or equal to) 70 inches tall in height, then the 80th percentile of heights of college students would be 70 inches.\nStandard deviation is calculated in R for a sample of data using sd(data).\nThe standard deviation is a number that describes how spread out the data typically are from the mean of that data. A larger standard deviation means the data are more spread out from their mean than data with a smaller standard deviation. The standard deviation is never negative. A standard deviation of zero implies all values in the data set are exactly the same.\nTo compute any of the five-number summary values in R, use the R function favstats(data) which also includes the mean and standard deviation.\nThe five-number summary consists of (1) the minimum value in the data, (2) the first quartile (25th percentile) of the data, (3) the median of the data (50th percentile), (4) the third quartile (75th percentile) of the data, and (5) the maximum value occurring in the data.\nTo create a boxplot in R, use the boxplot(data) or for multiple columns boxplot(data1, data2, names=c(\"Name of Column 1\", \"Name of Column 2)).\nBoxplots are a visualization of the five-number summary of a data set."
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "In this section, we will review data types (categorical and quantitative) and demonstrate how to numerically and visually summarize a quantitative response variable for each level of a categorical explanatory variable.\n\n\n\nCreate a table of summary statistics (favstats()) for multiple groups\n\nCreate side-by-side boxplots comparing multiple groups\n\nInterpret side-by-side boxplots for group comparisons\n\n\n\n\nWe will use the Big 5 Personality data of a random sample of Brother Cannon’s students.\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#lesson-outcomes",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#lesson-outcomes",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "Create a table of summary statistics (favstats()) for multiple groups\n\nCreate side-by-side boxplots comparing multiple groups\n\nInterpret side-by-side boxplots for group comparisons"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#load-the-data-and-libraries",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#load-the-data-and-libraries",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "We will use the Big 5 Personality data of a random sample of Brother Cannon’s students.\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#summary-statistics",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#summary-statistics",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nWe can easily extend favstats() to output our favorite statistics for multiple groups.\nWe first must identify the quantitative response variable we want to compare, then tell R which categorical explanatory variable we would like to compare.\nFor example, we could compare agreeableness between the sexes. In this case, Agreeableness is the quantitative response variable and Sex(M/F) is the categorical explanatory variable.\n\n# This gives us the summary statistics for Agreeableness across all groups\nfavstats(big5$Agreeableness)\n\n min Q1 median Q3 max     mean       sd   n missing\n  21 67     75 81 100 73.43457 13.24909 405       0\n\n# Adding the '~' tells R to break the data into groups (determined by the right side of the '~') and calculate the means of the variable on the left\nfavstats(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n  big5$`Sex(M/F)` min Q1 median Q3 max     mean       sd   n missing\n1               F  21 69     77 85 100 75.92035 12.94640 226       0\n2               M  25 63     73 79  94 70.29609 12.99218 179       0"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#visual-summaries-by-group",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#visual-summaries-by-group",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Visual Summaries by Group",
    "text": "Visual Summaries by Group\nWe can use the exact same formula used for boxplot() as we used for favstats():\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n\n\n\n\n\n\n\nNOTE: We will use the formula data$response ~ data$explanatory for LOTS of functions this semester. They will always take the form y ~ x."
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#improving-graphs",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#improving-graphs",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Improving Graphs",
    "text": "Improving Graphs\nThroughout this course, we will ease into making better visualizations. For now, here are some basic techniques that will usually apply to all graphing functions in R:\n\n# Changing color by sepecifying the `col = c()`\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n# R also assigns a numerical value to `col = `.  Try different numbers\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(2,3))\n\n\n\n\n\n\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(4,6))\n\n\n\n\n\n\n\n# Adding better axis labels using `xlab = ` and `ylab = `:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, xlab = \"Biosex\", ylab = \"Trait Agreeableness\")\n\n\n\n\n\n\n\n# Adding a title:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, main = \"Comparing Agreeableness by Biosex\")\n\n\n\n\n\n\n\n# Putting it all together:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`,main = \"Comparing Agreeableness by Biosex\", xlab = \"Biosex\", ylab = \"Trait Agreeableness\", col = c(3, 4))"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html",
    "title": "Summarizing Bivariate Data",
    "section": "",
    "text": "Bivariate data refers to situations where you have a quantitative response variable and a quantitative explanatory variable.\nAs with other data summaries, we can describe the relationship between 2 quantitative variables numerically and visually.\nBy the end of this lesson, you should be able to:\n\nCreate scatterplots for 2 quantitative variables R\n\nDescribe what the correlation coefficient, \\(r\\), quantifies\n\nCalculate \\(r\\) using the cor() function\n\nTwo datasets will be used to illustrate these concepts. The first contains self-reported confidence in mathematics and test scores. The second contains eruption duration and time between eruptions of Old Faithful geyser in Yellowstone National Park.\n\n# Load the libraries and data\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot",
    "title": "Summarizing Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nReview the scatter plot showing the relationship between students’ self reported confidence rating and test score.\n\nplot(math$Score ~ math$ConfidenceRatingMean)\n\n\n\n\n\n\n\n\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship. (strength, direction)\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\n\ncor(math$Score ~ math$ConfidenceRatingMean)\n\n[1] 0.7278648\n\n\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot-1",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot-1",
    "title": "Summarizing Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nDeciding which is the response variable in this data is a bit tricky. To remain consistent, assume that the wait time between eruptions explains how long the subsequent eruption will last.\nCreate a scatterplot of the relationship between wait time and duration. Remember to use the formula y ~ x\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatter plot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html",
    "title": "Categorical Data Summaries - One Variable",
    "section": "",
    "text": "In this section, you will learn:\n\nHow to create a table in R with counts\n\nHow to create a table in R with proportions\n\nHow to create bar plots for counts\n\nWe will use the creativity data collected in class.\nDon’t forget to load the libraries and the data by running the code chunk below:\n\n# Load the libraries\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\n# Load the data\ncreativity &lt;- import('https://github.com/byuistats/Math221D_Course/raw/main/Data/creativity_scores.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html#creating-a-table",
    "href": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html#creating-a-table",
    "title": "Categorical Data Summaries - One Variable",
    "section": "Creating a Table",
    "text": "Creating a Table\nTo get a table of counts for a categorical variable, we use the table() function. For example, if we want to see a summary for “Major_Category”:\n\ntable(creativity$Major_Category)\n\n\n      CS       DS       LA     Math    Psych      SCI Wildlife \n       8       15        3        3       28       11       23 \n\n\nWARNING: I hope it isn’t too much of a stretch at this point in the semester to show that you can nest functions. I will build this up step by step. The tricky part is keeping track of the parentheses so that all the input line up. To help make sure things line up, I will sometimes but extra spaces inside the parentheses so I can see the input more clearly.\nYou can put the table created above inside of the sort() function to order the table from smallest to lowest:\n\nsort( table(creativity$Major_Category) )\n\n\n      LA     Math       CS      SCI       DS Wildlife    Psych \n       3        3        8       11       15       23       28 \n\n\nIf we want to reverse the order to make it largest to smallest, we have to tell the sort() function to arrange the numbers from largest to smallest:\n\nsort( table(creativity$Major_Category) , decreasing =TRUE)\n\n\n   Psych Wildlife       DS      SCI       CS       LA     Math \n      28       23       15       11        8        3        3 \n\n\nTo represent this data visually, you can use the barplot()\n\nbarplot(   sort( table(creativity$Major_Category) , decreasing=TRUE)   )\n\n\n\n\n\n\n\n\nNesting multiple functions as demonstrated above can get a little messy. To clean this up a bit, we can name the sorted table and refer back to it as needed.\n\nmaj_cat_table &lt;- sort(table(creativity$Major_Category), decreasing =TRUE)\n\nbarplot(maj_cat_table)\n\n\n\n\n\n\n\n\nSometimes the category labels are long and crowd out other names. If we want to change the font size of the labels, we can input the las = 2 argument into the barplot():\n\nbarplot(maj_cat_table, las=2)\n\n\n\n\n\n\n\n# NOTE: I believe `las` stands for \"label axis style\""
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Categorical_Data_Summaries_Practice.html",
    "href": "2-Descriptive_Statistics/6-Categorical_Data_Summaries_Practice.html",
    "title": "Practice: Summarizing Categorical Data",
    "section": "",
    "text": "Reasons why patients visit a chiropractor vary and may depend on where you live.\nUse the chiropractic dataset to practice summarizing categorical data.\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nchiropractic &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/chiropractic_data.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Categorical_Data_Summaries_Practice.html#load-the-libraries-and-data",
    "href": "2-Descriptive_Statistics/6-Categorical_Data_Summaries_Practice.html#load-the-libraries-and-data",
    "title": "Practice: Summarizing Categorical Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nchiropractic &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/chiropractic_data.csv')"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html",
    "title": "Introducing GGPlot!",
    "section": "",
    "text": "GGPlot is a data visualization library that follows Leland Wilkinson’s Grammar of Graphics. The Grammar of Graphics is a systematic approach to how we think about connecting raw data to visual elements.\nThink about a basic sentence in English: The boy threw the ball. This sentence has a subject (the boy), a verb (threw), and a direct object (the ball). While not all sentences include every part of speech, virtually all sentences have at least a subject and a verb.\nThe grammar of graphics has 3 essential components of distinct graphical elements that are needed to make basic “sentences.” They are like the subjects and verbs of English sentences. These elements are:\n\nData layer\nAesthetic mappings\nGeometry layers\n\nThe data layer identifies the data we wish to express visually.\nThe Aesthetic Mapping is a description of how we map specific data elements to specific chart elements. For example, what variable in the data do we want expressed on the X axis or Y axis. We can also map a data variable to the color element.\nLastly, the Geometry Layer tells the computer how to express those Aesthetic Mappings, such as a scatter plot, boxplot, bar chart, etc.\nAs in English, we can make more complex sentences with other graphical elements, but the three mentioned above will be common to all.\nThis sounds more complicated than it is in practice. So let’s look at a familiar example: the Personality data.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#adding-color",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#adding-color",
    "title": "Introducing GGPlot!",
    "section": "Adding Color",
    "text": "Adding Color\nTo see the relationship between sepal length and width for each species separately, map the column Species onto the color element in the aesthetic mapping.\nBecause Species is a variable inside the dataset, we put it INSIDE the aes(). This maps Species onto the chart element, color.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo change ALL the points to a single color, include a “color” statement in the geometry layer:\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\")"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#more-additions",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#more-additions",
    "title": "Introducing GGPlot!",
    "section": "More Additions",
    "text": "More Additions\nIt is easy to make more interesting graphs that combine multiple geometries or even multiple data layers. For example, if I want to include a trend line on top of the points, simply add a new geometry. The geom_smooth() geometry can add different types of trend lines. We can specify method = 'lm' meaning “linear model” to get a simple line.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#further-customizations",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#further-customizations",
    "title": "Introducing GGPlot!",
    "section": "Further Customizations",
    "text": "Further Customizations\nWithout changing the underlying “grammar” we can change the “font,” so to speak. To modify the axis labels or add a title and a subtitle, use a labs() layer.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  )"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#themes",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#themes",
    "title": "Introducing GGPlot!",
    "section": "Themes",
    "text": "Themes\nAn easy way to change many visual elements all at once, ggplot() has several pre-packaged themes() you can add to a plot.\nWe typically want high contrast between data points and the background. This makes it easier to perceive differences. Changing the theme of the chart can make lots of changes all at once. theme_bw() is a useful theme which drops the gray default background.\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are more themes to try. If you begin typing theme_ you will see a drop down with several other themes.\nExplore some of the themes. Who can come up with the wildest visualization?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#facets",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#facets",
    "title": "Introducing GGPlot!",
    "section": "Facets",
    "text": "Facets\nSometimes adding more things to a graph makes it too cluttered. When dealing with multiple groups, you may want to split the graph into several panels, one for each group.\nFacets allow us to split a graph up based on a variable in the data. For example, if we wanted a separate regression plot for each species, we could “add” a facet:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNotice that the x-axes are the same for each group by default. That is often how we want to visualize data. Sometimes, though, we want to have each graph only cover the range of the data. We can allow the x and y axes to accommodate different ranges of data by setting the “scales” parameter inside the facet_grid to “free”:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species, scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE: When we want to “add” something to a graph, we simply include a + and tell it what we want to add. If we want to learn more about any of the graphing elements and their customization, we can always use the question mark help prompts (eg. ?facet_grid)."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#your-turn",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#your-turn",
    "title": "Introducing GGPlot!",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a side-by-side boxplot using the iris dataset that looks at the distribution of Sepal.Length for each species type.\nBe sure to:\n1. Color the boxes by Species 2. Add theme_bw() to make the chart more high contrast 3. Add a title Sepal Length by Species\nNOTE: Using color = Species with boxplots doesn’t look great. Try fill = Species instead. Using BOTH is not a good idea, but different combinations and see.\n\nggplot() + \n\nError in parse(text = input): &lt;text&gt;:5:0: unexpected end of input\n3: \n4: \n  ^"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#conclusion",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#conclusion",
    "title": "Introducing GGPlot!",
    "section": "Conclusion",
    "text": "Conclusion\nGGplot provides many options for easily making complex visualizations. While there is far too much to cover in one lesson, the basic framework is fairly intuitive once you get the hang of it."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#histograms-and-density-plots",
    "href": "3-Data_Wrangling_Visualization/1-GGPlot_Intro.html#histograms-and-density-plots",
    "title": "Introducing GGPlot!",
    "section": "Histograms and Density Plots",
    "text": "Histograms and Density Plots\nHistograms are typically used When we want to look at the distribution of a single variable. Because this is a single variable, we only define an x without a y.\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram() +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n# We can modify the number of bins in a histogram:  Play around with the \"bin\" Parameter\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nThe above histogram includes data from all species. We can distinguish species in several ways. One is to color the bars by species. Compare the difference between “color=Species” and “fill=Species” inside the aesthetic.\nWARNING: Would not recommend:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill=Species)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nIt’s not usually a good idea to layer histograms like this because it can obscure what is happening behind the covered layers. This is a situation where faceting can be useful.\nRecall that by default the x-axis will be fixed to the same values for each facet. We can let the x axis scale be different for each group by including scales = \"free\" into the facet_grid argument as above.\nTry both and see which tells a more compelling story:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 10) +\n  facet_grid(~Species) +\n  #facet_grid(~Species, scales = \"free\") +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\n\nA Better Histogram\nWhile histograms are a fine way to express the distribution of quantitative variables, it is not the only way. A Density plot is a smooth version of a histogram. Density plots use data to calculate a smooth line that expresses the quantitative variable as a continuous value rather than using crude bins.\nBy making the smooth line, it is much easier to compare between groups on the same plot:\n\nggplot(iris, mapping = aes(x = Sepal.Length, color = Species)) +\n  geom_density(linewidth = 1.2) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/3-Filter.html",
    "href": "3-Data_Wrangling_Visualization/3-Filter.html",
    "title": "filter()",
    "section": "",
    "text": "Good scientists NEVER delete data from original records. The tidyverse allows us to create a new, clean dataset with a transparent set of steps from which we can create graphs, visualizations and analyses without losing any of the original data.\nWe here demonstrate how to define criteria for choosing which rows to include from the data. Consider the High School survey data which consists of responses to 60 questions from 312 high school students.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nLogical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/3-Filter.html#logical-operators",
    "href": "3-Data_Wrangling_Visualization/3-Filter.html#logical-operators",
    "title": "filter()",
    "section": "",
    "text": "Logical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/3-Filter.html#filtering-on-categorical-data",
    "href": "3-Data_Wrangling_Visualization/3-Filter.html#filtering-on-categorical-data",
    "title": "filter()",
    "section": "Filtering on Categorical Data",
    "text": "Filtering on Categorical Data\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\n# See what the distinct values are in the Handed column\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# We could also try using %in% instead of  \"!=\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"))\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nNOTE: The %in% and the != will not always give you the same results. If there are misspellings or other options, using %in% will limit the data to only those with the exact spelling in the list provided. For example, someone responding “left handed” (all lower case), would not be included in the clean data. Misspellings would, however, be included if I use != \"Ambitextrous\" because that only removes rows written exactly that way. Things like, ambidextrious or RightHanded would still be included.\nI could further limit my data to students from Florida and Missouri:\n\nnew_data &lt;- survey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\ndim(new_data)\n\n[1] 27 60\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/5-Exploring_New_Data_with_Tidyverse.html",
    "href": "3-Data_Wrangling_Visualization/5-Exploring_New_Data_with_Tidyverse.html",
    "title": "Into The Tidyverse",
    "section": "",
    "text": "Statistics is only as interesting as the research questions we create. However, we cannot hope to answer those questions appropriately without going through the trouble of wrangling data.\nThe more time we spend digging into the data and noticing irregularities, the more likely we are to make better research questions and more appropriate conclusions. It doesn’t matter how sophisticated our analysis becomes if it’s based on bad data.\nIn this activity, you will explore a survey about happiness and related factors. By reviewing the data at a high level and then drilling into specific variables, you will gain a better idea about what this survey contains, generate interesting research questions and, with clean data, make better-informed answers to those questions."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/5-Exploring_New_Data_with_Tidyverse.html#lying",
    "href": "3-Data_Wrangling_Visualization/5-Exploring_New_Data_with_Tidyverse.html#lying",
    "title": "Into The Tidyverse",
    "section": "Lying",
    "text": "Lying\nRepeat the above analysis comparing how peoples’ happiness scores depend on their attitudes about lying.\n\nCreate a new dataset called lying that excludes the #N/A values in Lying and the Happiness scores outliers, and includes only “Happiness_score” and “Lying”:\n\n\nlying_data &lt;- happiness %&gt;%\n  filter(Happiness_score &lt;= __, \n         _______________ &gt;= 0, \n         Lying __ \"#N/A\"\n  ) %&gt;%\n  ______(Happiness_score, Lying)\n\n#View(lying_data)\n\nError in parse(text = input): &lt;text&gt;:3:30: unexpected input\n2: lying_data &lt;- happiness %&gt;%\n3:   filter(Happiness_score &lt;= __\n                                ^\n\n\n\nCreate a side-by-side boxplot and summary statistics (using favstats()) table for each attitude about Lying:\n\n\nboxplot(____________________ ~ lying_data$Lying)\n\nfavstats(lying_data$Happiness_score ~ _______________)\n\nError in parse(text = input): &lt;text&gt;:2:10: unexpected input\n1: \n2: boxplot(__\n            ^"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_GroupBy_Summarise.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_GroupBy_Summarise.html",
    "title": "group_by() + summarise()",
    "section": "",
    "text": "Summarizing Data\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nThe mosaic library is great for numerical summaries of quantitative variables using the favstats() function. We can create tables of the 5 number summary, mean, standard deviation, sample size, and number of missing values with one line of code:\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\n\nWe can add a grouping variable to get the same summary for each level of a group, using ~\n\nfavstats(survey$Height_cm ~ survey$Gender)\n\n  survey$Gender  min  Q1 median     Q3   max     mean       sd   n missing\n1        Female 5.50 160  162.5 167.15 182.8 158.6461 24.53056 152       0\n2          Male 1.68 172  177.9 182.80 999.0 179.3065 69.47610 160       0\n\n\nThis works great if you want to do one response/dependent variable at a time. But we often want specific summaries of data (often by groups) of more than one variable in the dataset.\nWe can use a combination of tidyverse functions, group_by() and summarise() to create custom summary tables.\nThe group_by() signals to R that whatever follows should be done for each level of the column(s) identified inside the parentheses. We can then “pipe” the grouped dataset into a summarize function and define what summary statistics we would like. summarise() works very much like the mutate() function in that we create a name for our summary and tell R how to make it.\nEXAMPLE: Let’s calculate the means for Height_cm, Reaction_time, and Social_Websites_Hours for Males and Females:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 2 × 4\n  Gender mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        159.           0.717                  14.4\n2 Male          179.           0.639                  14.0\n\n\nPro Tip: Recall that the mean() function returns “NA” when there are missing values in the data. Adding na.rm=TRUE to your functions will make sure that you get a mean value.\nEXAMPLE: Let’s do the same means but for handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 3 × 4\n  Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Ambidextrous        255.           0.352                  20.2\n2 Left-Handed         164.           1.45                   14.5\n3 Right-Handed        167.           0.583                  13.9\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers for reaction times (reaction times greater than 1 second), and height outliers (taller than 7 feet tall), and social media hours (more than 100 hours).\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Reaction_time &lt; 1,\n         Social_Websites_Hours &lt; 100) %&gt;%\n  select(Gender, Height_cm, Reaction_time, Social_Websites_Hours)\n\n# Pipe the new clean dataset into the group_by() and summarise() as above:\n\nclean %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    sd_ht = sd(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    sd_react_time = sd(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    sd_social_hrs = sd(Social_Websites_Hours, na.rm=TRUE)\n  )\n\n# A tibble: 2 × 7\n  Gender mean_height sd_ht mean_react_time sd_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        158.  25.1           0.444         0.134                  12.9\n2 Male          174.  23.4           0.395         0.139                  12.5\n# ℹ 1 more variable: sd_social_hrs &lt;dbl&gt;\n\n\n\n\n\n\nGrouping by Multiple Variables\nIt is simple to get summary statistics for multiple grouping factors.\nEXAMPLE: Suppose we want the same means calculated above, but for gender and handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 6 × 5\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female Ambidextrous        134.           0.361                  27  \n2 Female Left-Handed         160.           0.524                  12.8\n3 Female Right-Handed        159.           0.750                  14.3\n4 Male   Ambidextrous        315.           0.348                  16.8\n5 Male   Left-Handed         167.           2.29                   16.1\n6 Male   Right-Handed        175.           0.420                  13.5\n\n\nI can also use the n() function without any inputs to count the number of observations in each group:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    N = n()\n  )\n\nclean\n\n# A tibble: 6 × 6\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs     N\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt; &lt;int&gt;\n1 Female Ambidextrous        134.           0.361                  27       3\n2 Female Left-Handed         160.           0.524                  12.8    17\n3 Female Right-Handed        159.           0.750                  14.3   132\n4 Male   Ambidextrous        315.           0.348                  16.8     6\n5 Male   Left-Handed         167.           2.29                   16.1    19\n6 Male   Right-Handed        175.           0.420                  13.5   135\n\n\nThis shows me that there are only 3 female ambidextrous students in the sample and 6 male ambidextrous students."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_Mutate.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_Mutate.html",
    "title": "mutate()",
    "section": "",
    "text": "Adding Columns\nIf we want to create a new column in our dataset, we use the tidy function, mutate(). Consider again the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nTo create a new column, “pipe” the raw data into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: It is widely known that arm span is typically very close to one’s height. Let’s create a column of the ratio of Height (Height_cm) to armspan (Armspan_cm) and call the new column, ht_to_span. If common knowledge is correct, we would expect the ratio to be close to 1 on average.\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nmean(clean$ht_to_span)\n\n[1] Inf\n\n\nNotice that the mean is Inf which means infinity, or undefined. This is likely due to R attempting to divide a number by zero, meaning someone answered that their arm span was zero. This is where filter() comes in handy. We can filter out the rows where Armspan_cm is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nIt’s difficult to imagine someone who is 40 times taller than his or her arm span. But this is sufficient to illustrate the mutate() function.\nEXAMPLE: Now let’s make a new column that converts Height_cm into inches:\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54)\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers of ht_to_wt ratio and select only the columns of interest.\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54) %&gt;%\n  select(Height_cm, Armspan_cm, ht_to_span, Height_in) %&gt;% \n  filter(ht_to_span &lt; 1.5,\n         ht_to_span &gt; .5)\nclean\n\n# A tibble: 268 × 4\n   Height_cm Armspan_cm ht_to_span Height_in\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1       182       142       1.28       71.7\n 2       190       192       0.990      74.8\n 3       172       167       1.03       67.7\n 4       163       160       1.02       64.2\n 5        51        52       0.981      20.1\n 6       181       187       0.968      71.3\n 7       160       159       1.01       63.0\n 8       156       142.      1.10       61.4\n 9       169       162       1.04       66.5\n10       160       160       1          63.0\n# ℹ 258 more rows\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nI decided on 1.5 and .5 as the filter values by trial and error. There are better ways to determine outliers. But I suspect it is rare indeed for someone to be 50% taller than their armspan or 50% shorter."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#lesson-outcomes",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#lesson-outcomes",
    "title": "The Normal Distribution",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#normal-distributions-and-normal-computations",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#normal-distributions-and-normal-computations",
    "title": "The Normal Distribution",
    "section": "Normal Distributions and Normal Computations",
    "text": "Normal Distributions and Normal Computations\n\nBaseball Batting Averages\n\nIn baseball, a player called the “pitcher” throws a ball to a player called the “batter.” The batter swings a wooden or metal bat and tries to hit the ball. A “hit” is made when the batter successfully hits the ball and runs to a point in the field called first base. A player’s batting average is calculated as the ratio of the number of hits a player makes divided by the number of times the player has attempted to hit the ball or in other words, been “at bat.” Sean Lahman reported the batting averages of several professional baseball players in the United States. (Lahman, 2010) The file BattingAverages.xlsx contains his data.\nThe following histogram summarizes the batting averages for these professional baseball players:\n\nNotice the bell-shaped distribution of the data.\nSuppose we want to estimate the probability that a randomly selected player will have a batting average that is greater than 0.280. One way to do this would be to find the proportion of players in the data set who have a batting average above 0.280. We can do this by finding the number of players who fall into each of the red-colored bins below and dividing this number by the total number of players.\n\nIn other words, we could find the proportion of the total area of the bars that is shaded red out of the combined area of all the bars. This gives us the proportion of players whose batting averages are greater than 0.280.\nOut of the 446 players listed, there are a total of 133 players with batting averages over 0.280. This suggests that the proportion of players whose batting average exceeds 0.280 is:\n\\[\\displaystyle{\\frac{133}{446}} = 0.298\\]\nAlternatively, we can use the fact that the data follow a bell-shaped distribution to find the probability that a player has a batting average above 0.280.\n\n\nDensity Curves\nThe bell-shaped curve superimposed on the histogram above is called a density curve. It is essentially a smooth histogram. Notice how closely this curve follows the observed data.\nThe density curve illustrated on the histogram of the batting average data is special. It is called a normal density curve. This density curve is symmetric and has a bell-shape.\nThe normal density curve is also referred to as a normal distribution or a “Gaussian” distribution (after Carl Friedrich Gauss.)\nThe normal density curve appears in many applications in business, nature, medicine, psychology, sociology, and more. We will use the normal density curve extensively in this course.\nAll density curves, including normal density curves, have two basic properties:\n\nThe total area under the curve equals 1.\nThe density curve always lies on or above the horizontal axis.\n\nBecause of these two properties, the area under the curve can be treated as a probability. If we want to find the probability that a randomly selected baseball player will have a batting average between some range of values, we only need to find the area under the curve in that range. This is illustrated by the region shaded in blue in the figure below.\n\nA normal density curve is uniquely determined by its mean, \\(\\mu\\), and its standard deviation, \\(\\sigma\\). So, if random variables follow a normal distribution with a known mean and standard deviation, then we can calculate any probabilities related to that variable by finding the area under the curve.\nWhen the mean of a normal distribution is 0 and its standard deviation is 1, we call it the standard normal distribution.\nWe will return to this example later and will find the area shaded in blue.\n\nCharacteristics of the Normal Curve\n\nIntroduction to \\(z\\)-scores\nIn Ghana, the mean height of young adult women is normally distributed with mean \\(159.0\\) cm and standard deviation \\(4.9\\) cm. (Monden & Smits, 2009) Serwa, a female BYU-Idaho student from Ghana, is \\(169.0\\) cm tall. Her height is \\(169.0 - 159.0 = 10\\) cm greater than the mean. When compared to the standard deviation, she is about two standard deviations (\\(\\approx 2 \\times 4.9\\) cm) taller than the mean.\nThe heights of men are also normally distributed. The mean height of young adult men in Brazil is \\(173.0\\) cm (“Oramento,” 2010), and the standard deviation for the population is \\(6.3\\) cm. (Castilho & Lahr, 2001) A Brazilian BYU-Idaho student, Gustavo, is \\(182.5\\) cm tall. Compared to other Brazilians, he is taller than the mean height of Brazilian men.\n\nAnswer the following question:\n\n\n\nApproximately how many standard deviations above the mean is Gustavo’s height?\n\n\n\nShow/Hide Solution\n\n\nGustavo’s height is \\(182.5 - 173.0 = 9.5 \\text{ cm  }\\) above the mean. The standard deviation of the height of Brazilian men is 6.3 cm, so his height is \\(\\displaystyle{ \\frac{9.5}{6.3} =1.508 }\\) standard deviations above the mean.\n\n\n\n\n\n\nComputing \\(z\\)-scores\nWhen we examined the heights of Serwa and Gustavo, we compared their height to the standard deviation. If we look carefully at the steps we did, we subtracted the mean height for people of the same gender and nationality from each individual’s height, respectively.\nThis shows how much taller or shorter the person is than the mean height. In order to compare the height difference to the standard deviation, we divide the difference by the standard deviation. This gives the number of standard deviations the individual is above or below the mean.\nFor example, Serwa’s height is 169.0 cm. If we subtract the mean and divide by the standard deviation, we get \\[z = \\frac{169.0 - 159.0}{4.9} = 2.041\\] We call this number a \\(z\\)-score. The \\(z\\)-score for a data value tells how many standard deviations away from the mean the observation lies. If the \\(z\\)-score is positive, then the observed value lies above the mean. A negative \\(z\\)-score implies that the value was below the mean.\nWe compute the \\(z\\)-score for Gustavo’s height similarly, and obtain \\[z = \\frac{182.5 - 173.0}{6.3} = 1.508\\] Gustavo’s \\(z\\)-score is 1.508. As noted above, this is about one-and-a-half standard deviations above the mean. In general, if an observation \\(x\\) is taken from a random process with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the \\(z\\)-score is \\[z = \\frac{x -\\mu }{\\sigma}\\]\nThe \\(z\\)-score can be computed for data from any distribution, but it is most commonly applied to normally distributed data.\n\n\n68-95-99.7% Rule for Bell-shaped Distributions\nHeights of women (or men) in a particular population follow a normal distribution. Most people’s heights are close to the mean. A few are very tall or very short. We would like to make a more precise statement than this.\n\nFor any bell-shaped distribution,\n\n68% of the data will lie within 1 standard deviation of the mean,\n95% of the data will lie within 2 standard deviations of the mean, and\n99.7% of the data will lie within 3 standard deviations of the mean.\n\nThis is called the 68-95-99.7% Rule for Bell-shaped Distributions. Some statistics books refer to this as the Empirical Rule. \n\n\n\nApproximately 68% of the observations from a bell-shaped distribution will be between the values of \\(\\mu -~\\sigma~\\)and \\(\\mu +~\\sigma\\). Consider the heights of young adult women in Ghana. We expect that about 68% of Ghanaian women have a height between the values of \\[\\mu -~\\sigma = 159.0 - 4.9 = 154.1~\\text{cm}\\] and \\[\\mu +~\\sigma = 159.0 + 4.9 = 163.9~\\text{cm}.\\]\nSo, if a female is chosen at random from all the young adult women in Ghana, about 68% of those chosen will have a height between 154.1 and 163.9 cm. Similarly, 95% of the women’s heights will be between the values of \\[\\mu - 2\\sigma = 159.0 - 2(4.9) = 149.2~\\text{cm}\\] and \\[\\mu + 2\\sigma = 159.0 + 2(4.9) = 168.8~\\text{cm}.\\]\nFinally, 99.7% of the women’s heights will be between \\[\\mu - 3\\sigma = 159.0 - 3(4.9) = 144.3~\\text{cm}\\] and \\[\\mu + 3\\sigma = 159.0 + 3(4.9) = 173.7~\\text{cm}.\\]\n\n\n\nUnusual Events\nIf a \\(z\\)-score is extreme (either a large positive number or a large negative number), then that suggests that that observed value is very far from the mean. The 68-95-99.7% rule states that 95% of the observed data values will be within two standard deviations of the mean. This means that 5% of the observations will be more than 2 standard deviations away from the mean (either to the left or to the right).\nWe define an unusual observation to be something that happens less than 5% of the time. For normally distributed data, we determine if an observation is unusual based on its \\(z\\)-score. We call an observation unusual if \\(z &lt; -2\\) or if \\(z &gt; 2\\). In other words, we will call an event unusual if the absolute value of its \\(z\\)-score is greater than 2.\n\nAnswer the following questions:\n\n\n\nOut of Serwa and Gustavo, who is physically taller?\n\n\n\nShow/Hide Solution\n\n\nGustavo is taller. He is 182.5 cm tall, and Serwa is 169.0 cm tall.\n\n\n\nRelative to their own gender and nationality, who is relatively taller?\n\n\n\nShow/Hide Solution\n\n\nRelative to other Ghanaian women, Serwa is very tall. Gustavo is tall relative to Brazilian men, but relative to people of his gender and nationality, he is not relatively taller than Serwa. Serwa has a higher \\(z\\)-score.\n\n\n\nAre either of these heights unusual?\n\n\n\nShow/Hide Solution\n\n\nSerwa’s height is unusual. Her \\(z\\)-score is: \\(z = 2.041\\) This is more than two standard deviations away from the mean. Gustavo’s height is not unusual. His \\(z\\)-score is less than two standard deviations away from the mean.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#normal-probability-computations",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#normal-probability-computations",
    "title": "The Normal Distribution",
    "section": "Normal Probability Computations",
    "text": "Normal Probability Computations\nA \\(z\\) score has what is called a Standard Normal Distribution. This is a special normal distribution with a mean, \\(\\mu=0\\), and a standard deviation, \\(\\sigma=1\\).\nThis makes it possible to take any normal distribution and standardize it by subtracting out the population mean and dividing by the standard deviation. Whether we’re talking about heights, reaction times, big toe lengths, or nano-meters we can use the standard normal distribution to calculate probabilities by first calculating a z-score.\n\nConverting a \\(z\\)-score to a Probability\nIn this class, we will use R to do the heavy lifting. The pnorm(x, \\(\\mu\\), \\(\\sigma\\)) function calculates areas under the curve the normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), for a given x.\nThe p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nBy default, pnorm() gives the area of the curve to the LEFT of the value, \\(x\\), for a normal distribution with a mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nWe can calculate probabilities directly in the original units of the data, or use the z-score with a \\(\\mu=0\\) and \\(\\sigma=1\\): pnorm(z).\nNOTE: pnorm() has a default value for the mean, \\(\\mu = 0\\) and standard deviation, \\(\\sigma = 1\\) so we don’t have to input those values when we use a z-score with the standard normal distribution.\n\nHeights of Ghanaian Women\nWe will use the example of Serwa’s height to find the proportion of young Ghanaian women who are shorter than Serwa. Recall that for the height of young Ghanaian women, the population mean is 159.0 cm and the population standard deviation is 4.9 cm. Serwa’s height is 169.0 cm. We found the \\(z\\)-score of Serwa’s height as:\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{169.0- 159.0}{4.9} = 2.041\\]\nWhat proportion of young Ghanaian women reach a height that is at or below 169 cm? To answer this question, we need to find the area under a normal density curve (i.e. the probability) that is to the left of \\(z = 2.041\\).\nTo find the area under a normal curve corresponding to a \\(z\\)-score of \\(2.041\\), do the following:\n\npnorm(2.041)\n\n[1] 0.9793746\n\n\nWe can use the mosaic library to visualize the areas under the normal distribution:\n\nlibrary(mosaic)\nmosaic::xpnorm(169.0, mean = 159.0, sd = 4.9) \n\n\n\n\n\n\n\n\n[1] 0.9793655\n\n\nNOTE: The area to the left of our chosen \\(z\\)-score is also the probability that a randomly selected woman will be shorter than Serwa. The probability that a randomly selected Ghanaian woman will be shorter than Serwa is \\(0.979\\), or \\(97.9\\%\\).\n\n\nBaseball Averages\nWe now return to the example of the baseball batting averages. We want to find the probability that a randomly selected player will have a batting average that is above 0.280. The population mean is 0.261 and the population standard deviation is 0.034. We can use this information to find a \\(z\\)-score. Then we use the 1-pnorm() to find the area under the normal curve to the right of this \\(z\\)-score.\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{0.280 - 0.261}{0.034} = 0.559\\]\n\nx &lt;- 0.280\nmu &lt;- 0.261\nsigma &lt;- 0.034\n\nz &lt;- (x-mu)/sigma\n\n# Area to the LEFT:\npnorm(x, mu, sigma)\n\n[1] 0.7118589\n\n## Equivilantly:\npnorm(z)\n\n[1] 0.7118589\n\n# Area to the RIGHT\n1-pnorm(x, mu, sigma)\n\n[1] 0.2881411\n\n1-pnorm(z)\n\n[1] 0.2881411\n\n\nUsing the xpnorm() in the mosaic library, type the \\(z\\)-score of \\(0.559\\). Click on the areas under the curve until only the region on the right is highlighted in purple.\nTo get xpnorm() to give the Right Tail probability, you need to specify lower.tail=FALSE:\n\nmosaic::xpnorm(0.559, lower.tail = FALSE)\n\n\n\n\n\n\n\n\n[1] 0.2880809\n\n\nThe area under the curve to the right of \\(z = 0.559\\) is \\(0.2881\\). This is the probability that a randomly selected player will have a batting average that is greater than 0.280. (Note: It is a coincidence that the area, 0.288, is close to the batting average of 0.280. There is no significance in this.)\n\n\n\nFinding the Probability of Being Between Two Values\n\nHeights of Ghanaian Women\nWhat is the probability that a randomly selected young Ghanaian women will be between 150.0 cm and 163.0 cm tall? Recall that the average height for young Ghanaian women is \\(\\mu=159.0\\) cm and the population standard deviation is \\(\\sigma=4.9\\) cm.\nWe want to find the probability that a randomly selected woman’s height is between \\(150.0\\) cm and \\(165.0\\) cm. To do this we find the \\(z\\)-score for both values:\n\\[z_1 = \\frac{x- \\mu}{\\sigma} = \\frac{150.0 - 159.0}{4.9} = -1.837\\] \\[z_2 = \\frac{x - \\mu}{\\sigma} = \\frac{165.0 -159.0}{4.9} = 1.22\\]\nWe now answer the question by finding the area under the normal density curve (i.e. the probability) to the left of \\(z = 1.22\\) which is \\(0.8888\\) and also the area under the normal density curve to the left of \\(z = -1.837\\) which is \\(0.033\\). To find the area between \\(z = 1.22\\) and \\(z = -1.837\\), we subtract the smaller area from the larger.\n\\[0.8888 - 0.0331 = 0.8557\\]\nSo the probability that the height of a randomly selected young Ghanaian woman will be between 150.0 cm and 165.0 cm is \\(0.8558\\). This is the same as the proportion of all young Ghanaian women who are between 150.0 and 165.0 cm tall.\nWe can easily get mosaic::xpnorm() to provide this for us automatically by providing a list of values between which we would like to calculate the area under the curve.\nTo find the probability of being between any 2 numbers for a normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), we can use the following R code:\n\nmosaic::xpnorm(c(150, 165), mean = 159, sd = 4.9)\n\n\n\n\n\n\n\n\n[1] 0.03312454 0.88961624\n\n\n\n\n\nCalculating Percentiles using a Normal Distribution\nA percentile is a number such that a specified percentage of the population are at or below this number. For example, the 25th percentile is the number in a data set that is greater than or equal to 25% of all the values in the data set.\nWe can find percentiles for a given dataset by using the quantile() function as described in the chapter on summarizing data\nHowever, to calculate percentiles from a normal distribution, we use the qnorm(percentile, mu, sigma) function.\nNOTE: R typically uses the word quantile when referring to percentiles. So the \\(q\\) in qnorm() stands for quantile.\nTo find the height of a Ghanaian woman corresponding to the 90th percentile:\n\nqnorm(.90, mean = 159, sd = 4.9)\n\n[1] 165.2796\n\n\nThe 90th percentile of the heights averages is 165.2796. That means that 90% of the Ghanaian women are shorter than 165.2796 cm tall.\nWe can also use xqnorm() in the mosaic library to visualize:\n\nmosaic::xqnorm(.9, mean=159, sd=4.9)\n\n\n\n\n\n\n\n\n[1] 165.2796"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#summary",
    "href": "4-Foundations_Statistical_Inference/2-The_Normal_Distribution.html#summary",
    "title": "The Normal Distribution",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA normal density curve is symmetric and bell-shaped with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\). The curve lies above the horizontal axis and the total area under the curve is equal to 1. A standard normal distribution has a mean of 0 and a standard deviation of 1.\nA z-score is calculated as:\n\n\\[\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{x-\\mu}{\\sigma}}\\]\n\nA z-score tells us how many standard deviations above (\\(+Z\\)) or below (\\(-Z\\)) the mean (\\(\\mu\\)) a given value (\\(x\\)) is.\nTo calculate probabilities for a normal distribution with mean, \\(\\mu\\) and standard deviation \\(\\sigma\\) for a given observation \\(x\\), use pnorm(x, mu, sigma) or 1-pnorm(x, mu, sigma) to get the desired probability (below, above). Alternatively, calculate the \\(z\\)-score and use pnorm(z) or 1-pnorm(z). In every case, the probability is given by the Area under the curve.\nThe 68-95-99.7% rule states that when data are normally distributed, approximately 68% of the population lies within \\(z=1\\) standard deviation (\\(\\sigma\\)) from the mean, approximately 95% of the data lie within \\(z=2\\) standard deviations from the mean, and approximately 99.7% of the data lie within \\(z=3\\) standard deviations from the mean. For example, this rule approximates that 2.5% of observations will be less than a z-score of \\(z=-2\\).\nPercentiles can be calculated using the qnorm(percentile, mu, sigma) function."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html",
    "href": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html",
    "title": "Distribution of Sample Means",
    "section": "",
    "text": "This lesson explores the idea of a sampling distribution which is the theoretical distribution of a sample statistic. We discuss the theoretical concepts that allow us to make confident conclusions about an entire population based on a single sample.\n\n\nBy the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#when-is-bar-x-normal",
    "href": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#when-is-bar-x-normal",
    "title": "Distribution of Sample Means",
    "section": "When is \\(\\bar x\\) Normal?",
    "text": "When is \\(\\bar x\\) Normal?\nHow many observations are required so that the Central Limit Theorem will assure that the distribution of sample means will be approximately normal? The answer is, “it depends.”\n\nIf the parent population is normal, then the sampling distribution of \\(\\bar x\\) will always be normally distributed, no matter how many observations are selected.\nIf the parent population is not normal, then the sampling distribution \\(\\bar x\\) will be approximately normally distributed, if the sample size is large enough.\n\nIf the parent population is almost normal (e.g. mound-shaped and nearly symmetrical), then a sample of size n = 5 will probably be sufficient to assure that \\(\\bar x\\) will be approximately normally distributed.\nIf the parent population is heavily skewed, then it will require a larger sample size to be assured that \\(\\bar x\\) will be normally distributed. For most moderately skewed distributions, a sample size of around 30 is traditionally thought to be sufficiently large to assure that \\(\\bar x\\) will be approximately normally distributed. This is not a definitive number but is a rule of thumb.\nFor tremendously skewed distributions (e.g., the distribution of lottery payouts), a much larger sample will be required before the distribution of sample means is approximately normal. This may require billions of observations. Simulation can be used to determine if a particular sample size is sufficient.\n\n\nFor this course, if the sample size is at least 30, we will conclude that the sampling distribution of \\(\\bar x\\) will be approximately normal.\n\n\nAnswer the following question:\n\n\n\nThere are two ways that \\(\\bar x\\) can be (approximately) normally distributed. What are they?\n\n\n\nShow/Hide Solution\n\n\n\\(\\bar x\\) will be normally distributed if the data were drawn from a normal population.\n\\(\\bar x\\) will be approximately normally distributed if the sample size is sufficiently large."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "href": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "title": "Distribution of Sample Means",
    "section": "Mean and Standard Deviation of the Distribution of Sample Means",
    "text": "Mean and Standard Deviation of the Distribution of Sample Means\nThe following facts are always true. They do not depend on the Central Limit Theorem. They do not depend on the sample size. These facts hold for the sample mean \\(\\bar x\\) of any simple random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\nThe mean of the sample means is \\(\\mu\\)\nThe standard deviation of the sample means is \\(\\sigma / \\sqrt{n}\\).\n\n_KEY DEFINOTION__: We call the standard deviation of the sample mean, \\(\\sigma_{\\bar{x}}\\), the standard error.\nRemember, these facts are always true. They do not depend on normality or the sample size. So, even though these facts are often used in conjunction with the Central Limit Theorem, they do not depend on it.\n\nAnswer the following questions:\n\n\n\nThe amount of time passengers spend waiting for a bus on a particular urban route follows a distribution that has a mean of 8.7 minutes with a standard deviation of 2.2 minutes. Transportation officials observed the waiting times for a random sample of \\(n=121\\) individual passengers and recorded the sample mean, \\(\\bar x\\). We can think of this sample mean as one value observed out of all the possible sample means that could have been observed. What is the mean of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\mu = 8.7~\\text{minutes}\n\\]\n\n\nUse the information in the previous problem to answer this question: What is the standard deviation of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{2.2}{\\sqrt{121}} = 0.2~\\text{minutes}\n\\]"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#review-of-key-concepts",
    "href": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#review-of-key-concepts",
    "title": "Distribution of Sample Means",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\nIn order to do statistics with a sample mean,\\(\\bar{x}\\), we need the sampling distribution of \\(\\bar{x}\\) to be normal. There are two situations when the distribution of \\(\\bar{x}\\) is guaranteed to be normal (or at least very close to normal). They are:\n\nIf the parent population is normal, the distribution of the sample means \\(\\bar{x}\\) will be normal, for every sample size \\(n\\).\nEven if the parent population is not normal, the Central Limit Theorem guarantees that the distribution of the sample mean \\(\\bar{x}\\) will be approximately normal if the sample size \\(n\\) is large enough. For this course, if \\(n \\geq 30\\), we will say the distribution of the sample means will be approximately normal (even if the parent population is not normal).\n\nThe mean and standard deviation of \\(\\bar{x}\\) are:\n\nThe mean of the sample means, \\(\\mu_{\\bar{x}}\\), is the population mean \\(\\mu\\).\nThe standard deviation \\(\\sigma_{\\bar{x}}\\) of the sample means is the population standard deviation \\(\\sigma\\) divided by the square root of \\(n\\), \\(\\sigma / \\sqrt{n}\\)"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#summary",
    "href": "4-Foundations_Statistical_Inference/3-Sampling_Distributions_CLT_LLN.html#summary",
    "title": "Distribution of Sample Means",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe distribution of sample means is a distribution of all possible sample means (\\(\\bar x\\)) for a particular sample size.\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large. In this class, \\(n\\ge 30\\) is considered to be sufficiently large.\nThe mean of the distribution of sample means is the mean \\(\\mu\\) of the population: \\(\\mu_{\\bar{x}} = \\mu\\).\nThe standard deviation of the distribution of sample means is the standard deviation \\(\\sigma\\) of the population divided by the square root of \\(n\\): \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\).\nThe distribution of sample means is normal in either of two situations: (1) when the data is normally distributed or (2) when, thanks to the Central Limit Theorem (CLT), our sample size (\\(n\\)) is large.\nThe Law of Large Numbers states that as the sample size (\\(n\\)) gets larger, the sample mean (\\(\\bar x\\)) will get closer to the population mean (\\(\\mu\\)). This can be seen in the equation for \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Notice as \\(n\\) increases, then \\(\\sigma_\\bar{x}\\) will get smaller."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html",
    "title": "Distribution of Sample Statistics",
    "section": "",
    "text": "This in-class activity will explore the distribution of sample statistics and the Central Limit Theorem.\nTo demonstrate, we will be exploring a dataset about different species of penguins collected on different islands. Background about the study and data can be found here.\nPractice the process for approaching a dataset outlined in class:\n\nLoad the data and libraries\nExplore the data and generate hypotheses\nPrepare the data for analysis\nPerform the appropriate analysis\n\nData preparation will include using the filter() function. For now, analysis means creating good visualizations that tell a story using ggplot() and base R."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#explore-the-data-on-your-own",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#explore-the-data-on-your-own",
    "title": "Distribution of Sample Statistics",
    "section": "Explore the data on your own",
    "text": "Explore the data on your own\nQUESTION: What is/are some potential dependent/response variables?\nANSWER:\nQUESTION: What are some potential explanatory variables?\nANSWER:\nLet’s look deeper into bill_depth_mm."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-table-of-summary-statistics-favstats-for-bill_depth_mm",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-table-of-summary-statistics-favstats-for-bill_depth_mm",
    "title": "Distribution of Sample Statistics",
    "section": "Create a table of summary statistics (favstats()) for bill_depth_mm:",
    "text": "Create a table of summary statistics (favstats()) for bill_depth_mm:\nQUESTION: What is your sample mean?\nANSWER:\nQUESTION: What is your sample standard deviation?\nANSWER:\nQUESTION: What is your sample median?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-frequency-table-of-sex-table",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-frequency-table-of-sex-table",
    "title": "Distribution of Sample Statistics",
    "section": "Create a frequency table of sex (table()):",
    "text": "Create a frequency table of sex (table()):\nQUESTION: How many females in your sample?\nANSWER:\nQUESTION: How many males in your sample?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-table-of-species",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-table-of-species",
    "title": "Distribution of Sample Statistics",
    "section": "Create a table of species",
    "text": "Create a table of species\nQUESTION: How many Adelie penguins do you have in your sample?\nANSWER:\nQUESTION: How many Chinstrap penguins do you have in your sample?\nANSWER:\nQUESTION: How many Gentoo penguins do you have in your sample?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-species",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-species",
    "title": "Distribution of Sample Statistics",
    "section": "Create a side-by-side boxplot of bill_depth_mm for all species:",
    "text": "Create a side-by-side boxplot of bill_depth_mm for all species:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-sex",
    "href": "4-Foundations_Statistical_Inference/5-Sampling_Distribution_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-sex",
    "title": "Distribution of Sample Statistics",
    "section": "Create a side-by-side boxplot of bill_depth_mm for all sex:",
    "text": "Create a side-by-side boxplot of bill_depth_mm for all sex:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html",
    "href": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html",
    "title": "Introducing Hypothesis Tests",
    "section": "",
    "text": "In this lesson, we explore the components of statistical hypothesis testing in a general way. These general principles will apply to all of the specific tests we will learn about during the rest of the semester.\nLesson Objectives:\n\nExplain what the null hypothesis, \\(H_0\\), is and why we use it\n\nExplain what the alternative hypothesis, \\(H_a\\), is\n\nExplain the concept of a test statistic as a “signal-to-noise” ratio\n* Explain what is meant by “signal”\n* Explain what is meant by “noise”\n\nExplain the concept of a p-value"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-null-hypothesis-h_0",
    "href": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-null-hypothesis-h_0",
    "title": "Introducing Hypothesis Tests",
    "section": "The Null Hypothesis, \\(H_0\\)",
    "text": "The Null Hypothesis, \\(H_0\\)\nThe null hypothesis is the initial claim or assumption about a population parameter. It often represents the status quo or “no effect”.\nFor example, we assume that the new medication has no effect until we find enough evidence to prove otherwise.\nWe write the null hypothesis as ‘\\(H_0\\)’, and refer to it as “H-Naught” or “H-O”.\nMany of the statistical tests we cover in this course follow the generic form:\n\\[H_0: \\text{There is no relationship between }X \\text{ and }Y\\]"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-alternative-hypothesis-h_a",
    "href": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-alternative-hypothesis-h_a",
    "title": "Introducing Hypothesis Tests",
    "section": "The Alternative Hypothesis, \\(H_a\\)",
    "text": "The Alternative Hypothesis, \\(H_a\\)\nWe typically engage in scientific inquiry because we do NOT believe the null hypothesis. For example, we believe that there IS an effect of a new medication.\nThe “burden of proof” is on the researcher to prove the null hypothesis wrong. This means that it is the researcher’s responsibility to present enough evidence to contradict the null hypothesis.\nThe counter-proposal for the null hypothesis is called the alternative hypothesis.\nWe write the null hypothesis as ‘\\(H_a\\)’, and refer to it as “H-A”.\nThe statistical tests we will cover in the rest of the semester follow the generic form:\n\\[H_a: \\text{There is a relationship between }X \\text{ and }Y\\]"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#visualizing-the-null-hypothesis",
    "href": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#visualizing-the-null-hypothesis",
    "title": "Introducing Hypothesis Tests",
    "section": "Visualizing the Null Hypothesis",
    "text": "Visualizing the Null Hypothesis\nWe always begin a study under the assumption that the null hypothesis is true.\nWhat would a true null hypothesis look like? As we’ve explored data using descriptive statistics, we’ve seen many examples where there is no relationship between x and y.\nFor example, if there was no relationship between Height and Extroversion, we would expect to see something like:\n \nBecause of sample-to-sample variability, the points wouldn’t look exactly like this every time, but it would generally look like random scatter.\nIf the null hypothesis is true, it would be very unlikely, though possible, to observe a scatter plot like the following, just by chance, because of sample-to-sample variation:\n\n\nIf the null hypothesis is true, it would be ASTRONOMICALLY unlikely, though theoretically possible, to observe a relationship like the following just by chance, because of sample-to-sample variation:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html",
    "title": "Confidence Intervals for a Mean",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the meaning of a level of confidence\nCreate a confidence interval for a single mean with \\(\\sigma\\) known using the following steps:\n\nFind the point estimate (\\(\\bar{x}\\))\nCalculate the margin of error for the given level of confidence\nCalculate a confidence interval from the point estimate and the margin of error\nInterpret the confidence interval\nCheck the requirements for the confidence interval\n\nExplain how the margin of error is affected by the sample size and level of confidence\n\nStatistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nThis chapter lays the foundation for confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#lesson-outcomes",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#lesson-outcomes",
    "title": "Confidence Intervals for a Mean",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the meaning of a level of confidence\nCreate a confidence interval for a single mean with \\(\\sigma\\) known using the following steps:\n\nFind the point estimate (\\(\\bar{x}\\))\nCalculate the margin of error for the given level of confidence\nCalculate a confidence interval from the point estimate and the margin of error\nInterpret the confidence interval\nCheck the requirements for the confidence interval\n\nExplain how the margin of error is affected by the sample size and level of confidence\n\nStatistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nThis chapter lays the foundation for confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#point-estimators",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#point-estimators",
    "title": "Confidence Intervals for a Mean",
    "section": "Point Estimators",
    "text": "Point Estimators\nWe have learned about several statistics. Remember, a statistic is any number computed based on data. The sample statistics we have discussed are used to estimate population parameters.\n\n\n\n\n\n\n\n\nSample Statistic\n\n\n\n\nPopulation Parameter\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\\(\\bar x\\)\n\n\n\n\n\\(\\mu\\)\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\\(s\\)\n\n\n\n\n\\(\\sigma\\)\n\n\n\n\n\n\nVariance\n\n\n\n\n\\(s^2\\)\n\n\n\n\n\\(\\sigma^2\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\nThe statistics above are called point estimators because they are just one number (one point on a number line) that is used to estimate a parameter. Parameters are generally unknown values. Think about the mean. If \\(\\mu\\) is unknown, how do we know if \\(\\bar{x}\\) is close to it?\nThe short answer is that we will never know for sure if \\(\\bar{x}\\) is close to \\(\\mu\\). This does not mean that we are helpless. The normal distribution provides a way for us to create a range of plausible values for a parameter (e.g. \\(\\mu\\)) based on a statistic (e.g. \\(\\bar{x}\\))."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#review-distribution-of-sample-means",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#review-distribution-of-sample-means",
    "title": "Confidence Intervals for a Mean",
    "section": "Review: Distribution of Sample Means",
    "text": "Review: Distribution of Sample Means\nConfidence intervals rely on the validity of the assumption that the distribution of the sample mean is normally distributed.\nRecall that the distribution of sample means is normal when:\n\nThe underlying population is normally distributed\nThe sample size, n, is sufficiently large (\\(n&lt;30\\) for this class) for the Central Limit Theorem to apply\n\nThought Question: If we have a good sample from a population and can trust that the sampling distribution of the mean is approximately normal, how frequently would a sample mean be within 2 standard deviations from the true population mean?\nRemember, the standard deviation of \\(\\bar x\\) is \\(\\frac{\\sigma}{\\sqrt{n}}\\). For the variable \\(\\bar x\\), two standard deviations would be equal to \\(2 \\frac{\\sigma}{\\sqrt{n}}\\).\nANSWER: If we collect a random sample from a population and \\(\\bar x\\) is normally distributed, then about 95% of the time the sample mean \\(\\bar x\\) will be less than \\(2 \\frac{\\sigma}{\\sqrt{n}}\\) units away from the population mean \\(\\mu\\). Notice that this is true, whether or not we know \\(\\mu\\).\n\n\n\n\n\n\n\n\n\nThis means the 95% of the time, we will get a sample mean within 2 Standard Deviations of the true population mean.\nFlipping this around, if we take our sample mean and make an interval 2 standard deviations above the mean and 2 below, the interval will overlap with the true population mean about 95% of the time."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#an-approximate-95-confidence-interval",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#an-approximate-95-confidence-interval",
    "title": "Confidence Intervals for a Mean",
    "section": "An Approximate 95% Confidence Interval",
    "text": "An Approximate 95% Confidence Interval\nThe equation of an approximate 95% confidence interval would be:\n\\[ CI = \\bar{x} \\pm 2 \\frac{\\sigma}{\\sqrt{n}}\\]\nThe part that we are adding and subtracting from our point estimate is called the Margin of Error. We use the letter \\(m\\) to denote the margin of error:\n\\[m = 2 \\frac{\\sigma}{\\sqrt{n}}\\]\nUsing this definition for \\(m\\), our confidence interval can be written as\n\\[( \\bar x - m, ~ \\bar x + m )\\]"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#interpretation",
    "href": "5-Statistical_Tests_Part1/0-Confidence_Intervals_for_Means.html#interpretation",
    "title": "Confidence Intervals for a Mean",
    "section": "Interpretation",
    "text": "Interpretation\nConfidence intervals are typically reported using parentheses like: (lower limit, upper limit). We say that we are \\(100*(1-\\alpha)\\%\\) confident that the true population mean is between [lower limit] and [upper limit].\n\nAverage GRE Scores of BYU-I Students\nThe published population standard deviation of the quantitative portion of the Graduate Record Examination (GRE) scores is \\(\\sigma=8.3\\).\nSuppose we take a random sample of \\(n=100\\) BYU-I students who have taken the GRE and find that their average score was \\(\\bar{x}=162.1\\)\nWe can calculate the 99% confidence interval:\n\\[ 162.1 \\pm 2.576\\frac{8.3}{\\sqrt{100}} = (159.96, 164.24)\\]\nThe interpretation of the above confidence interval would be:\nI am 99% confident that the true population mean GRE score for BYU-I students is between 159.96 and 164.24."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html",
    "title": "One-Sample T-test",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean inferential procedure is appropriate\n\nPerform a hypothesis test for one mean using the following steps:\n\nState the null and alternative hypotheses\n\nCalculate the test-statistic, degrees of freedom and P-value using R\n\nAssess statistical significance in order to state the conclusion for the hypothesis test in context of the research question\n\nCheck the requirements for the hypothesis test\n\n\nCreate a confidence interval for one mean using the following steps:\n\nCalculate a confidence interval for a given level of confidence using R\n\nInterpret the confidence interval\n\nCheck the requirements of the confidence interval\n\n\nState the properties of the Student’s t-distribution"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#statistical-inference",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#statistical-inference",
    "title": "One-Sample T-test",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nStatistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nWhen we know what the population standard deviation, \\(\\sigma\\), for individuals, and are confident that the distribution of sample means is approximately normal, we can use a Z-score and the Standard Normal Distribution to calculate probabilities.\n\\[ z= \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nThough rare, there are situations where we might know the population standard deviation from published research or census data. For example, standardized test organizations publish population-level summaries which would allow us to test how our sample compares to the general population using the Z formula."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#confidence-interval",
    "title": "One-Sample T-test",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nWe can also calculate a confidence interval for the above example.\nRecall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "title": "One-Sample T-test",
    "section": "Calculating P-values by hand with the T-distribution (One out of ten. Would NOT RECOMMENDED)",
    "text": "Calculating P-values by hand with the T-distribution (One out of ten. Would NOT RECOMMENDED)\nSuppose we have 25 test scores defined as “data” in the code chunk below. We can calculate the \\(t\\)-statistic just like we did with the \\(z\\)-score.\nSuppose we believe that the mean score of these 25 students is significantly higher than 50. Our null and alternative hypothesis are as follows:\n\\[ H_0:  \\mu = 50 \\] \\[ H_a: \\mu &gt; 50 \\]\n\n# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom\n\ndata &lt;- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)\n\n# Hypothesized Mean:\nmu_0 &lt;- 50\n\n# Sample size, sample Mean and sample SD\n#  The length function tells us how many data points are in the list.  \nn &lt;- length(data)\nxbar &lt;- mean(data)\ns &lt;- sd(data)\n\ns_xbar = s / sqrt(n)\n\nt &lt;- (xbar - mu_0) / s_xbar\n\n# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true\n1-pt(t, n-1)\n\n[1] 0.00151866\n\n\nThis means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (P-value)."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-1-read-in-the-data",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-1-read-in-the-data",
    "title": "One-Sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Read in data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-2-review-the-data",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-2-review-the-data",
    "title": "One-Sample T-test",
    "section": "Step 2: Review the Data",
    "text": "Step 2: Review the Data\nIn this step, we are looking to see if the data are as expected. Are the columns we’re interested in numeric? Categorical? and do these match expectations. We can also start to look for strange data and outliers. Visualizations can help.\nOther common issues to look for include: negative numbers that should only be positive, date values that shouldn’t exist, missing values, character variables inside what should be a number.\nIn the real world, data are messy. Reviewing the data is a critical part of an analysis.\nTake a look through the personality dataset and see if there are any anomalies that might need to be addressed."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-3-visulize-the-data",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-3-visulize-the-data",
    "title": "One-Sample T-test",
    "section": "Step 3: Visulize the data",
    "text": "Step 3: Visulize the data\nSo far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc. When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.\nCreate a histogram of Extroversion. Describe some features of the data. Is it symmetric? Skewed? Are there outliers?\n\nhistogram(big5$Extroversion, main = \"Extroversion Scores\", xlab = \"Extroversion\")\n\n\n\n\n\n\n\nggplot(big5, aes(x = Extroversion)) +\n  geom_histogram(bins = 15, fill=\"lightblue\") +\n  labs(\n    x = \"Extroversion\",\n    y = \"\",\n    title = \"Extroversion Scores\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "One-Sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\nIn this example, we will be testing the hypothesis that Brother Cannon’s students are similar to the general population. We suspect that these youthful BYU-I students are, on average, more extroverted.\n\nHypothesis Test\nWrite out the Null and alternative hypotheses.\n\\[ H_0: \\mu_{extroversion} = 50\\]\n\\[ H_A: \\mu_{extroversion}  &gt; 50\\]\n\\[\\alpha = 0.05\\]\nPerform the one-sample t-test using the “t.test()” function in R. If you ever get stuck remembering how to use a function in R, you can run ?t.test to see documentation. The question mark will open up the help files for any given function in R.\nThe t.test() function takes as input, the data, the hypothesized mean, mu, and the direction of the alternative hypothesis.\nThe default parameters for the t.test() function are: t.test(data, mu = 0, alternative = \"two.sided\").\n\n#?t.test\n\n# One-sided Hypothesis Test\nt.test(big5$Extroversion, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\nQuestion: What is the P-value?\nAnswer: p-value = 4.697e-11\nQUESTION: What is your conclusion based on \\(\\alpha=0.05\\)?\nANSWER: Because P-value &lt; 0.05 we reject the null hypothesis in favor of the alternative.\nQuestion: State your conclusion in context of our research question?\nAnswer: We have sufficient evidence to conclude that Brother Cannon’s students are more extroverted than the general population, on average.\n\n\nConfidence Intervals\nWe can also use the t.test() function to create confidence intervals. Recall that confidence intervals are always 2-tailed. Confidence intervals are typically written in the form: (lower limit, upper limit).\n\n# Confidence Intervals are by definition 2-tailed\n# We can also change the confidence level\n\nt.test(big5$Extroversion, mu = 50, alternative = \"two.sided\", conf.level = .99)\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 9.394e-11\nalternative hypothesis: true mean is not equal to 50\n99 percent confidence interval:\n 54.26631 59.69903\nsample estimates:\nmean of x \n 56.98267 \n\n\nTo get only the output for the confidence interval to be shown, we can use a $ to select specific output. Much like when pulling a specific column from a dataset, the $ can pull specific output from an analysis.\nBecause the option for the alternative = in the t.test function is “two.sided”, we don’t actually need to include it when getting confidence intervals.\nAlso, confidence intervals do not require an assumed mu value. So a more efficient way to get a confidence interval for a given set of data is:\n\nt.test(big5$Extroversion, conf.level = .99)$conf.int\n\n[1] 54.26631 59.69903\nattr(,\"conf.level\")\n[1] 0.99\n\n\nQuestion: Describe in words the interpretation of the confidence interval in context of Extroversion.\nAnswer: I am 95% confident that the true population mean of extroversion scores for Brother Cannon’s students is between 54.266 and 59.699.\n\n\nChecking Requirements\nWe still rely on the assumption that the distribution of sample means is normally distributed.\nRecall that the mean is normally distributed if:\n\nThe underlying population is normally distributed\n\nWe have a sufficiently large sample size (\\(n&gt;30\\))\n\nFor the above Extroversion data, we have \\(n=404\\) which is much larger than 40.\nIf my sample size was small, I could check the qqPlot(), which I demonstrate here:\n\nqqPlot(big5$Extroversion)\n\n\n\n\n\n\n\n\n[1] 143 163"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#body-temperature-data",
    "href": "5-Statistical_Tests_Part1/1-One_Sample_ttest.html#body-temperature-data",
    "title": "One-Sample T-test",
    "section": "Body Temperature Data",
    "text": "Body Temperature Data\nThe dataset below contains information about body temperatures of healthy adults.\n\nLoad the data:\n\n# These lines load the data into the data frame body_temp:\n\nbody_temp &lt;- import(\"https://byuistats.github.io/M221R/Data/body_temp.xlsx\")\n\n\n\nReview the Data\nCreate a table of summary statistics for temperature:\n\n\nVisualize the Data\nCreate a histogram to visualize the body temperature data.\nQuestion: Describe the general shape of the distribution.\nAnswer:\n\n\nAnalyze the Data\nIt’s widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.\nSuppose we suspect that the average temperature is different than 98.6\nUse a significance level of \\(\\alpha = 0.01\\) to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.\nQuestion: What is the P-value?\nAnswer:\nQUESTION: What is your conclusion?\nANSWER:\n\nConfidence Interval\nCreate a 99% confidence interval for the true population average temperature of healthy adults.\nCheck the requirements for the t-test (\\(n&gt;30\\) or qqPlot()):\nQUESTION: Are the requirements for the t-test satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html",
    "title": "Confidence Intervals and Hypothesis Tests for Proportions",
    "section": "",
    "text": "At the end of this lesson, you should be able to:\n\nUnderstand the conditions where the sampling distribution of \\(\\hat p\\) is normal\nCalculate a confidence intervals for a population proportion\nCheck the normality requirements for a confidence interval\nPerform a hypothesis test for a population proportion\nCheck the normality requirements for a hypothesis test for one proportion"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#handedness",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#handedness",
    "title": "Confidence Intervals and Hypothesis Tests for Proportions",
    "section": "Handedness",
    "text": "Handedness\nSuppose we want to calculate estimate the true proportion of left-handed students in visual arts. We can take a random sample of 100 art majors and ask if they are left-handed. Suppose we find 15 art majors are left handed.\nWe can calculate a 95% confidence interval for the true population proportion of left-handed art students as follows:\n\nprop.test(x=15,n=100, conf.level=.95)$conf.int\n\n[1] 0.0891491 0.2385308\nattr(,\"conf.level\")\n[1] 0.95\n\n\nInterpretation: I am 95% confident that the true proportion of left-handed art majors is between 0.089 and 0.239.\nThe above confidence interval assumes that the conditions for \\(\\hat p\\) to be normally distributed are met. We have to check that:\n\n\\(n\\hat{p} \\ge 10\\)\n\\(n(1-\\hat{p}) \\ge 10\\)\n\nThat is, there are at least 10 yeses and 10 no’s in our sample, which is the case."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#your-turn",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#your-turn",
    "title": "Confidence Intervals and Hypothesis Tests for Proportions",
    "section": "Your Turn",
    "text": "Your Turn\nSuppose you want to estimate the proportion of students who regularly drink energy drinks on campus. You randomly select 235 students and find that 88 consume energy drinks multiple times a week.\nCreate a 99% confidence interval for the true proportion of students who regularly consume energy drinks:\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\nQUESTION: Are the assumptions for the normality of \\(\\hat p\\) are met? Why?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#example-1-one-sample-proportion-test",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#example-1-one-sample-proportion-test",
    "title": "Confidence Intervals and Hypothesis Tests for Proportions",
    "section": "Example 1: One Sample Proportion Test",
    "text": "Example 1: One Sample Proportion Test\nSuppose we want to test whether the proportion of students who passed an exam is significantly less than 0.75. We have a sample of 100 students, of which 72 passed.\n\\[\\hat{p} = \\frac{X}{N} = \\frac{72}{100} = .72\\]\nIf we want to test if this is significantly less than 75%, we can use the prop.test() which is very similar to t.test(). Instead of putting in a sample mean, \\(\\bar{x}\\), with a hypothesized \\(\\mu\\), we put in \\(X\\) and \\(N\\) and a hypothesized \\(p\\). Setting the alternative and confidence level operates the same as t.test().\nConfidence intervals for proportions for proportions can also be obtained just as with t.test().\n\n# One Sample Proportion Test Example\n# Hypothesized proportion: 0.75\n# Sample size: 100\n# Number of successes: 72\n\nprop.test(x = 72, n = 100, p = 0.75, alternative = \"less\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  72 out of 100, null probability 0.75\nX-squared = 0.33333, df = 1, p-value = 0.2819\nalternative hypothesis: true p is less than 0.75\n95 percent confidence interval:\n 0.0000000 0.7917861\nsample estimates:\n   p \n0.72 \n\nprop.test(x = 72, n = 100, conf.level = .9)$conf.int\n\n[1] 0.6358512 0.7917861\nattr(,\"conf.level\")\n[1] 0.9\n\n\n\nThe distribution of \\(\\hat{p}\\)\nRecall that the sampling distribution for \\(\\hat{p}\\) is approximately normally distributed when our sample has more than 10 expeted “successes” and more than 10 expected “failures”. We test this by looking at:\nHypothesis Test Requirements:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nWe use p, not \\(\\hat{p}\\) for hypothesis testing because hypothesis testing always assumes the null hypothesis is true. Confidence intervals, on the other hand, make no such assumption.\nTo see if the calculated confidence interval is appropriate, we use \\(\\hat{p}\\).\nConfidence Interval Requirements: \\[n\\hat{p} \\geq 10\\] \\[n(1-\\hat{p}) \\geq 10\\] Can we trust the p-value and confidence interval?\nYou can use a calculator for this, or simply use R as a calculator:\n\nx &lt;- 72\nn &lt;- 100\np_hat &lt;- x/n\np &lt;- .75\n\n# For Hypothesis Testing:\nn*p &gt;= 10\n\n[1] TRUE\n\nn*(1-p) &gt;= 10\n\n[1] TRUE\n\n# For Confidence Intervals:\nn*p_hat &gt;= 10\n\n[1] TRUE\n\nn*(1-p_hat) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#example-2-handedness",
    "href": "5-Statistical_Tests_Part1/2-Conf_Int_Hyp_Test_for_Proportions.html#example-2-handedness",
    "title": "Confidence Intervals and Hypothesis Tests for Proportions",
    "section": "Example 2: Handedness",
    "text": "Example 2: Handedness\nSuppose the United States national average percent of left-handed people is 11%. A researcher wants to know if visual arts majors are significantly more likely to be left handed. She samples 250 visual arts majors and finds that 36 are left handed.\nPerform a one-sample proportion to see if visual arts majors are significantly more left-handed than the general population.\nState the null and alternative hypotheses and your significance level.\n\\[H_0: p = \\] \\[H_a: p \\] \\[\\alpha = \\]\nQuestion: What is the value of the test statistics for this test?\nAnswer:\nQuestion: What is the P-Value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:\nMake a \\((1-\\alpha)\\) level confidence interval for the true population proportion.\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:\nQuestion: Are the hypothesis test requirements for the normality of \\(\\hat{p}\\) satisfied?\nAnswer:\nQuestion: Are the requirements for a confidence interval for \\(p\\) satisfied?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html",
    "title": "Independent Two-sample T-test",
    "section": "",
    "text": "In many situations we would like to compare averages from different populations. In these situations, we take 2 random samples from each population and perform statistical tests to determine if the population means are significantly different. Because these two groups of individuals are sampled independently, we call this analysis Independent 2-Sample t-test.\nAlternatively, in many experimental designs, participants are randomly assigned into a treatment and a control group. The randomization process ensures that there is no association between participants in either group. They are independent.\nWhen 2 random samples are taken from 2 separate populations, or when a group of people are randomly assigned into treatment groups, the samples are independent.\nSome examples include:\n\nComparing salaries of men and women (randomly sample men and women separately)\nTesting a new medication compared to a placebo (participants randomly assigned to treatment groups)\nComparing average GPA of Math majors and Economics majors (randomly select from each population)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#response-and-explanatory-variables",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#response-and-explanatory-variables",
    "title": "Independent Two-sample T-test",
    "section": "Response and Explanatory Variables",
    "text": "Response and Explanatory Variables\nThe above statistical hypothesis, \\(H_o: \\mu_1 = \\mu_2\\), is equivalent to stating that the explanatory variable has no relationship to the response variable.\nFor example, suppose we want to compare reaction times between 2 groups, athletes and non-athletes. We suspect that athletes have quicker reaction times than non-athletes. We could express the null and alternative hypotheses:\n\\[H_o: \\mu_\\text{athletes} = \\mu_{non-athletes}\\]\n\\[H_a: \\mu_\\text{athletes} &lt; \\mu_\\text{non-athletes}\\] where \\(\\mu_\\text{athletes}\\) is the population average reaction time for athletes.\nNow look at how the data might be organized:\n\n\n\n\n\n\n\n\n\nReaction_Time\nGroup\n\n\n\n\n0.23\nAthlete\n\n\n0.45\nNon-athlete\n\n\n0.18\nAthlete\n\n\n0.56\nNon-athlete\n\n\n0.31\nAthlete\n\n\n0.72\nNon-athlete\n\n\n0.27\nAthlete\n\n\n0.39\nNon-athlete\n\n\n0.21\nAthlete\n\n\n0.64\nNon-athlete\n\n\n\n\n\nThinking of the data this way makes it easy to see that we can phrase our null hypothesis as: There is no relationship between group membership and reaction time."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#independent-t-test-in-r",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#independent-t-test-in-r",
    "title": "Independent Two-sample T-test",
    "section": "Independent t-test in R",
    "text": "Independent t-test in R\nRecall that when we created a boxplot() or favstats() for multiple groups, the R code looked like:\nboxplot(data$response_variable ~ data$grouping_variable)\nfavstats(data$response_variable ~ data$grouping_variable)\nWe use the exact same format for a t-test with 2 groups:\nt.test(data$response_variable ~ data$grouping_variable, alterntive = \"\")\nWe do not need to specify mu in this case because the t.test() function uses mu=0 as a default. This relates to the null hypothesis of the difference between the means, which we expect to be zero if the null hypothesis is true.\nWhen specifying an alternative hypothesis in R (alternative=\"\"), a reference group must be used. The reference serves as the baseline for comparison. R will assess how the mean of the second group differs from that of the first.\nBy default, R decides which group is the reference group alphabetically. Whichever category label is first alphabetically is the reference group.\nNOTE: In R, group 1 and 2 are determined alphabetically according to the labels in the explanatory variable column.\nEXAMPLE: If my explanatory variable is sex, and they are labeled “Female” and “Male”, then “Female” would be the reference group. If I suspect that Females had smaller feet than males I would use alternative = \"less\"."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#checking-requirements",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#checking-requirements",
    "title": "Independent Two-sample T-test",
    "section": "Checking Requirements",
    "text": "Checking Requirements\nWhen we looked at 1-sample t-tests, we had to check that the sample mean was normally distributed. Recall that the sample mean is approximately normal if:\n\nThe source population is normally distributed\nThe sample size, \\(n \\ge 30\\)\n\nFor a 2-sample t-test, we must check that this is true for both samples. If both sample sizes are bigger than 30 then we can trust the CLT and the statistics based on it.\nIf the sample sizes are small, we can make a qqPlot() for both groups:\nqqPlot(data$response_variable ~ data$explanatory_variable)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#confidence-intervals",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#confidence-intervals",
    "title": "Independent Two-sample T-test",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that confidence intervals are necessarily two-sided. So the code for a 99% confidence interval looks like:\nt.test(data$response_variable ~ data$grouping_variable, conf.level = .99)$conf.int\nWe interpret a confidence interval for the difference of means as follows:\n\nI am 99% confident that the true difference of the means is between [lower limit] and [upper limit].\n\nWe can usually do better within the context of a research question:\n\nClass 1 did, on average, between 3.21 and 5.67 percent better than class 2 on the last exam.\n\nStore A outperforms Store B by between $27,022 and $36,977 on average"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-1-read-in-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-1-read-in-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\n\nfifa_heart_attacks &lt;- import(\"https://byuistats.github.io/M221R/Data/fifa_heart_attacks.xlsx\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-2-review-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-2-review-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review Data",
    "text": "Step 2: Review Data\nLook at the data.\nCreate summary statistics tables of the number of heart attacks for each group.\nCreate a side-by-side boxplot for the during the World Cup and the Control.\nDo you notice any outliers or data that may need to be omitted for analysis?\nCheck to see if the means from both groups are normally distributed:\n\nIs n &gt; 30 for both groups?\nCreate a qqPlot()\n\n\nqqPlot(fifa_heart_attacks$heart_attacks ~ fifa_heart_attacks$time_period)\n\n\n\n\n\n\n\n\nCan we trust that the central limit theorem applies?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-3-prepare-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThese data look ready for analysis."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nAre the individuals in each group dependent or independent of each other?\nWrite out your null and alternative hypotheses.\nHo: Ha:\nWhich group is considered group 1 and which is group 2 in R?\nCheck the alphabetical order:\n\nunique(fifa_heart_attacks$time_period)\n\n[1] \"Control\"   \"World Cup\"\n\n\nPerform the appropriate t-test.\nWhat is your test statistic?\nWhat is your p-value?\nState your conclusion:\n\n\n97% Confidence interval\nCalculate the 97% confidence interval for the difference of the means.\nIn context of the research question, interpret the confidence interval."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#new-zealand-rugby",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#new-zealand-rugby",
    "title": "Independent Two-sample T-test",
    "section": "New Zealand Rugby",
    "text": "New Zealand Rugby\nRugby is a popular sport in the United Kingdom, France, Australia, New Zealand and South Africa. It is gaining popularity in the US, Canada, Japan and parts of Europe. Some of the rules of the game have recently been changed to make play more exciting. In a study to examine the effects of the rule changes, Hollings and Triggs (1993) collected data on some recent games.\nTypically, a game consists of bursts of activity that terminate when points are scored, if the ball is moved out of the field of play or if a violation of the rules occurs. In 1992, the investigators gathered data on ten international matches which involved the New Zealand national team, the All Blacks. The first five games were the last international games played under the old rules, and the second set of five were the first internationals played under the new rules.\nFor each of the ten games, the data give the successive times (in seconds) of each passage of play in that game.\nYou will investigate whether the mean duration of the passages has dropped under the new rules.\nUse a level of significance of 0.01.\n\nLoad the Data\n\nrugby &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/nz_rugby.csv\")\n\n\n\nExplore the Data\nCreate a side-by-side boxplot for the amount of reported passage of play before and after the rule changes.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics of play time for before and after the rule change. (favstats()):\n\n\nPerform the Appropriate Analysis\n\nHypothesis Test\nState your null and alternative hypotheses:\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = NewRules\nCompare the the time per play under the new and old rules:\n\nqqPlot(rugby$time, groups = rugby$period)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nWhat is the value of the test statistic?\nHow many degrees of freedom for this test?\nWhat is the p-value?\nWhat do you conclude?\n\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-1-read-in-the-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-1-read-in-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\ncopd &lt;- import(\"https://byuistats.github.io/M221R/Data/copd_rehab.xlsx\") %&gt;% pivot_longer(cols=c(\"community\", \"hospital\"), names_to = \"Treatment\", values_to = \"Steps\") %&gt;% select(Treatment, Steps) %&gt;% arrange(Treatment)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-2-review-the-data",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-2-review-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nCreate side-by-side boxplots and summary statistics for the community and hospital groups:\nCheck to see if the means are expected to be normally distributed.\nCan trust the CLT for our test statistic and P-value?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-3-prepare-data-for-analysis-1",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-3-prepare-data-for-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThe data cleansing has been performed for you. You’re welcome."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "5-Statistical_Tests_Part1/3-Two_sample_Independent_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nHo:\nHa:\nWhich group is considered group 1 in this data?\nRun the appropriate t-test.\n\n#t.test()\n\nState your conclusion about the hypothesis test.\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference between the means\nInterpret the 95% confidence interval for the mean difference between the community-based and hospital-based groups."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html",
    "title": "Matched Pairs T-test",
    "section": "",
    "text": "A matched pairs design is used in statistics to compare 2 treatments or conditions measured on subjects that are logically connected in a meaningful way. In the simplest case, measurements are collected on the same subject as in a before-and-after evaluation.\nMatched pairs designs are often called “dependent samples” because knowing who or what is in the first treatment group determines who will be in the second. In the case of a before-and-after situation, if you are selected to be in the “pre” group, then you will also be in the “post” group. But pairs are not always the same subjects.\n\n\n\nYou want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item.\n\n\n\n\nAs with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality.\n\n\n\nWhen we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#examples",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#examples",
    "title": "Matched Pairs T-test",
    "section": "",
    "text": "You want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#requirements-for-a-matched-pairs-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#requirements-for-a-matched-pairs-analysis",
    "title": "Matched Pairs T-test",
    "section": "",
    "text": "As with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#watchouts",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#watchouts",
    "title": "Matched Pairs T-test",
    "section": "",
    "text": "When we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-1-read-in-data",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-1-read-in-data",
    "title": "Matched Pairs T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nweight_loss &lt;- import(\"https://byuistats.github.io/M221R/Data/weight_loss.xlsx\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Matched Pairs T-test",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms summary statistics for the pre and post weight measurements:\n\nglimpse(weight_loss)\n\nRows: 27\nColumns: 4\n$ subject  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ pre      &lt;dbl&gt; 62.5, 88.8, 74.7, 98.6, 78.1, 73.8, 87.0, 62.1, 60.7, 76.2, 8…\n$ post     &lt;dbl&gt; 56.1, 80.2, 70.8, 97.0, 69.1, 63.8, 81.9, 53.3, 56.5, 71.8, 7…\n$ comments &lt;chr&gt; \"Data collected by Annie Mahon in a\", \"weight loss study. Sub…\n\n# Pre-weight histogram\nhistogram(weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$pre)\n\n  min    Q1 median   Q3  max mean       sd  n missing\n 60.7 72.75     77 80.7 99.6 76.9 10.20671 27       0\n\n# Post-weight histogram\nhistogram(weight_loss$post)\n\n\n\n\n\n\n\nfavstats(weight_loss$post)\n\n  min    Q1 median   Q3 max    mean       sd  n missing\n 53.3 63.45   69.9 75.2  97 70.0963 10.63273 27       0"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-3-prepare-the-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-3-prepare-the-data-for-analysis",
    "title": "Matched Pairs T-test",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you’re going to define the difference (\\(post - pre\\) or \\(pre - post\\)).\nQuestion: Based on your definition, what does a negative number mean?\nAnswer:\nCreate a histogram and a qqPlot of the differences to determine if you will be able to trust the statistical analyses:\n\n# histogram of the differences\nhistogram(weight_loss$post - weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$post - weight_loss$pre)\n\n   min   Q1 median   Q3  max      mean       sd  n missing\n -13.6 -8.9   -6.7 -4.2 -1.6 -6.803704 3.172051 27       0\n\n# Check if the differences are normally distributed:\nqqPlot(weight_loss$post - weight_loss$pre)\n\n\n\n\n\n\n\n\n[1] 14 18"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Matched Pairs T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\n\\[ H_0: \\mu_{differences} = 0\\]\n\\[H_a:  \\mu_{differences} &lt; 0\\]\n\\[ \\alpha = 0.01\\]\nQuestions to consider:\n\nHow did you define your difference?\nBased on your decision for the difference, what does a negative number mean? a positive number?\nAre you expecting the difference to be greater than, less than, or not equal to 0?\n\nNOTE: One way to verify that you have used the correct alternative is to look at the difference as defined (weight_loss$post - weight_loss$pre) and decide which one you think is supposed to be bigger. Swap out the subtraction sign for the inequality that makes sense (weight_loss$post &lt; weight_loss$pre). Therefore, your alternative should be “less” because \\(&lt;\\) corresponds to “less than”.\nPerform a t-test of the differences:\n\n# Hypothesis t.test()\nt.test(weight_loss$post - weight_loss$pre, mu = 0, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  weight_loss$post - weight_loss$pre\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean of x \n-6.803704 \n\n# Equivalently:  \nt.test(weight_loss$post, weight_loss$pre, paired=TRUE, mu = 0, alternative = \"less\")\n\n\n    Paired t-test\n\ndata:  weight_loss$post and weight_loss$pre\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean difference \n      -6.803704 \n\n\nState your conclusion in context of the research question:\nBecause P-value &lt; 0.01 we reject the null hypothesis. We have SUFFICIENT evidence to suggest that participation in the weight loss program led to weight loss.\n\n\nConfidence Interval\n\n# Confidence interval using t.test()\nt.test(weight_loss$post - weight_loss$pre, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\nt.test(weight_loss$post, weight_loss$pre, paired = TRUE, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\n\nI am 99% confident that the true average weight loss during the program was between -8.5 and -5.11 kilograms."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-1-read-in-data-1",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-1-read-in-data-1",
    "title": "Matched Pairs T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-2-check-data",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-2-check-data",
    "title": "Matched Pairs T-test",
    "section": "Step 2: Check Data",
    "text": "Step 2: Check Data\nReview the data to see if the variables look correct. Calculate summary statistics and histograms for chol_day2 and chol_day4."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-3-prepare-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Matched Pairs T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nDecide how to define your difference. What does a negative number indicate? a positive number?\nCreate a histogram and summary statistics of the differences:\nBecause \\(n &lt; 30\\) we need to check the qqPlot() to assess the normality of the differences."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "5-Statistical_Tests_Part1/4-Matched_Pairs_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Matched Pairs T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nWhat is your confidence level, $(1-) = $?\nPerform a matched pairs t-test for the difference in cholesterol at day 2 and day 4:\nState your conclusion:\n\n\nConfidence Interval\nCalculate a confidence interval for the average difference.\nState your conclusions and interpret the confidence interval in context of the research question."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html",
    "title": "2-Sample Proportions",
    "section": "",
    "text": "In statistics, two sample proportion tests are used to compare proportions or percentages between two independent groups. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should not be surprising:\n\\[H_0: p_1 = p_2\\]\n\\[H_a: p_1\\; (&lt;,&gt;,\\neq) \\: p_2\\] where \\(p_1\\) represents the unknown population proportion for group 1 and \\(p_2\\) represents the unknown population proportion for group 2."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html#example-1-voting-behaviour-by-gender",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html#example-1-voting-behaviour-by-gender",
    "title": "2-Sample Proportions",
    "section": "Example 1: Voting Behaviour by Gender",
    "text": "Example 1: Voting Behaviour by Gender\nSuppose we want to test if women are more likely to identify as Democrat than men. We sample 250 men and 250 women and measure their political affiliation. We find that 80 men identify as Democrat and 102 females identify as Democrat.\nJust as with the two-sample t-test for means, we must define a reference group. In this example, we will use females as the reference group so that our alternative will be relative to that group.\n\\[H_0: p_{femaleDem} = p_{maleDem}\\]\n\\[H_a: p_{femaleDem} &gt; p_{maleDem}\\]\nWe will use \\(\\alpha = 0.05\\)\n\nprop.test(x = c(102, 80), n = c(250, 250), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(102, 80) out of c(250, 250)\nX-squared = 3.8099, df = 1, p-value = 0.02548\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01350993 1.00000000\nsample estimates:\nprop 1 prop 2 \n 0.408  0.320 \n\n\nWe can also create a confidence interval for the difference:\n\nprop.test(x = c(102, 80), n = c(250, 250))$conf.int\n\n[1] 5.904086e-06 1.759941e-01\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals for differences can be positive and negative. In this example, a negative number would indicate that Females are less likely to be Democrat and a positive number means they are more likely to be Democrat.\nOur confidence interval is just above zero on the lower end. We are 95% confident that females are between 0.000% and 17.6% more likely to be Democrat than men.\n\nTest Requirments\nJust as with 1-sample proportion tests, we must validate that we have a large enough sample size to ensure that \\(\\hat{p}\\) is approximately normally distributed. When we have 2 samples, however, we must check both \\(\\hat{p}\\)’s. For both hypothesis testing and confidence intervals we check:\nRequirements for Hypothesis Testing and Confidence Intervals\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nAn easy R calculator to check this is:\n\n# All must be true:\n\nx1 &lt;- 102\nn1 &lt;- 250\nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] TRUE\n\nn1*(1-phat1) &gt;=10\n\n[1] TRUE\n\nx2 &lt;- 80\nn2 &lt;- 250\nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] TRUE\n\nn2*(1-phat2) &gt;=10\n\n[1] TRUE\n\n\nIf all conditions are greater than or equal to 10, we can trust our p-values and confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html#example-2-favorite-sports",
    "href": "5-Statistical_Tests_Part1/5-Two_sample_Proportions.html#example-2-favorite-sports",
    "title": "2-Sample Proportions",
    "section": "Example 2: Favorite Sports",
    "text": "Example 2: Favorite Sports\nSoccer is becoming much more popular in the United States. We would like to test if this is being driven by demographic shifts in the population where the younger generation is more likely to favor soccer.\nA researcher samples 524 individuals under 40 and 655 individuals older than 40 and asks what their preferred sport is. Of the 524 respondents under 40, 44 identified soccer as their favorite sport. Of the 655 respondents over 40, 27 identified soccer as their favorite sport.\nPerform a 2-sample proportion test to determine if significantly more younger people identify soccer as their favorite sport.\nCreate and interpret the confidence interval for the difference in the proportions.\nAre the requirements for the hypothesis test and confidence interval satisfied?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html",
    "title": "One- and Two-sample Statistical Inference",
    "section": "",
    "text": "This activity is designed to provide opportunities for practicing one- and two-sample hypothesis tests and confidence intervals.\nFrom context in the background information, you should be able determine what statistical test is appropriate and what the null and alternative hypotheses should be.\nFor each problem:\n\nCarefully read the background information\n\nDefine the null and alternative hypotheses\n\nPerform the relevant statistical test in R\n\nCheck the test requirements\n\nConstruct and interpret the confidence interval\n\nNOTE: If not stated, use \\(\\alpha=0.05\\). Also, if not specified, you can create graphs using base R or ggplot().\nDon’t forget to load the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#test-requirements",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#test-requirements",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Test Requirements",
    "text": "Test Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 90% confidence interval for the population average sodium in the cans.\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#summarize-the-data",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#summarize-the-data",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Summarize the Data",
    "text": "Summarize the Data\nCreate a table of summary statistics comparing cholesterol levels for each treatment group:\nQUESTION: What is the difference between the sample averages of cholesterol between the 2 groups?\nANSWER:\nUse ggplot() to create a side-by-side boxplot of the cholesterol levels of each treatment group:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\n\nTest Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 95% confidence interval for the difference between the mean cholesterol levels:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#summarize-the-data-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#summarize-the-data-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Summarize the Data",
    "text": "Summarize the Data\nCreate histograms for before and after weights separately:\nCreate a histogram of the differences (after-before):\nQUESTION: What is the average difference between after and before weights?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\n\nTest Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-2",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-2",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 95% confidence interval for the mean difference between before and after weights:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#data-summarization",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#data-summarization",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Data Summarization",
    "text": "Data Summarization\nQUESTION: What is your sample proportion of units that pass quality control, \\(\\hat{p}\\):\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-2",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-2",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\nQUESTION: What are the test requirements for a 1-sample proportion hypothesis test, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-3",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-3",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 99% confidence interval for the true proportion of items that pass inspection.\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:\nQUESTION: What are the test requirements for a 1-sample proportion confidence interval, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#data-summarization-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#data-summarization-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Data Summarization",
    "text": "Data Summarization\nQUESTION: What is your sample proportion of success for each treatment?\n\\[\\hat{p}_A = \\]\n\\[\\hat{p}_B = \\]"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-3",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#hypothesis-test-3",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\n\nTest Requirements\nQUESTION: What are the test requirements for a 2-sample proportion hypothesis test, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-4",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference.html#confidence-interval-4",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 99% confidence interval for the difference in success rate for each treatment:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:\nQUESTION: What are the test requirements for a confidence interval for the difference between 2 proportions, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\).\n\n\n\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")\n\n\n\n\nCreate a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:\n\n\n\n\nState your null and alternative hypotheses:\nHo:\nHa:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#introduction",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#introduction",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#load-the-data-and-libraries",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#load-the-data-and-libraries",
    "title": "ANOVA Practice",
    "section": "",
    "text": "library(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#explore-the-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#explore-the-data",
    "title": "ANOVA Practice",
    "section": "",
    "text": "Create a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "title": "ANOVA Practice",
    "section": "",
    "text": "State your null and alternative hypotheses:\nHo:\nHa:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#introduction-1",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#introduction-1",
    "title": "ANOVA Practice",
    "section": "Introduction",
    "text": "Introduction\nA study was conducted to determine if different types of material can reduce the amount of mosquito human contact. The researchers evaluated five different types of patches 1=Odomos, 2=Deltamethrin, 3=Cyfluthrin, 4=D+O, 5=C+O.\nThe amount of mosquito human contact was measured to assess any differences between the five different types of material. Use a level of significance of 0.05."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#load-the-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#load-the-data",
    "title": "ANOVA Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\nMosquitoPatch &lt;- rio::import(\"https://raw.githubusercontent.com/rdcromar/Math221D/main/MosquitoPatch.csv\") %&gt;% mutate(Treatment = factor(Treatment))"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#review-the-data",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#review-the-data",
    "title": "ANOVA Practice",
    "section": "Review the Data",
    "text": "Review the Data\nCreate a side-by-side boxplot for human contact for each of the treatment groups.\nAdd a title and change the colors of the boxes.\nCreate a summary statistics table for human contact for each of the treatment groups:\nQuestion: What do you observe?\nAnswer:\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "href": "6-Statistical_Tests_Part2/1-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "title": "ANOVA Practice",
    "section": "Perform the Appropriate Analysis",
    "text": "Perform the Appropriate Analysis\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic (F-value)?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\n\nNumerator (between Groups) Degrees of Freedom\n\nDenominator (within groups) Degrees of Freedom\nAnswer:\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html",
    "title": "Regression Practice",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data with a regression line\nCalculating and interpreting the correlation coefficient, r\nFitting a linear regression analysis\nVerifying if a linear model is model is adequate:\n\nChecking for linearity (scatterplot)\nChecking for constant variance (plot(lm_output, which=1))\nChecking for normality of residuals (qqPlot(lm_output$residuals))\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#good-price",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#good-price",
    "title": "Regression Practice",
    "section": "Good Price?",
    "text": "Good Price?\nLastly, the car you’re interested in buying has around 100,000 miles and costs $11,200. Could this be considered a good deal? Why?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model-1",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model-1",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements-1",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements-1",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model-2",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#fit-a-linear-regression-model-2",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements-2",
    "href": "6-Statistical_Tests_Part2/2-Regression_Practice.html#check-model-requirements-2",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence_Practice.html",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence_Practice.html",
    "title": "Chi-Square Practice",
    "section": "",
    "text": "Complete the following questions about testing for independence between 2 categorical variables. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence_Practice.html#hypothesis-test",
    "href": "6-Statistical_Tests_Part2/3-Chi_Square_Test_of_Independence_Practice.html#hypothesis-test",
    "title": "Chi-Square Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nNOTE: The way the data were imported into bp_alcohol, it contains a column, V1, which is the row label. It should not be included in the input table in the chisq.test() function. Create a new dataset that only includes the columns: zero, one_two, three_four_five and six_or_more.\n\nalcohol_tbl &lt;- bp_alcohol %&gt;%\n  select()\n\nOnce you’ve created the table with only the counts, perform the Chi-square test for independence:\n\n# Run the Chi-square test:  \n\n\n# Check the requirements:\n\nQuestion: Are the requirements satisfied for the \\(\\chi\\)-square test for independence?\nAnswer:\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:"
  },
  {
    "objectID": "7-Semester_Project/Data_Search.html",
    "href": "7-Semester_Project/Data_Search.html",
    "title": "Finding a Dataset",
    "section": "",
    "text": "Introduction\nSo far in class, we have encountered only fairly clean data examples carefully managed and stored in an easy to access location. We have been able to use the import() function from the rio library with a link to a well-contained data resource.\nWhen we encounter data in the wild, it can be much more complicated to extract and clean up. However, good research should be transparent, with data sources cited and, where possible, published. We can often find links to raw data for graphs.\nFor this assignment, you will find a dataset that interests you online and submit a link to the location of the data.\nLesson Objectives:\n\nStart with a reaserch question that interests you. It can be about any topic (finance, music, video games, hunting, weather, mental health, AI…seriously anything!)\n\nUse the resources provided in this document to find a related dataset to your research question\nRefine your research question\n\n\n\nResearch Question\nFor this project, a good research question will be interesting to you AND feasible to find available data. You can certainly think of exciting research questions for which the data are impossible to find or collect. Expect to refine your research question as you begin the data search.\nOnce you have a research question in mind, start looking for data relating to it.\n\n\nFinding Data\nThere are several online resources for data searches. Below are some good places to start.\nGoogle actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face.\n\n\nRefining the Question\nWe are not always able to find the exact data that addressess our research question. This is a limitation of not designing your own study from scratch.\nIf you are unable to find the perfect data, refine your question.\nIt’s still a good idea to start with a question that interests you. You’re more likely to end up with a dataset that you enjoy working with.\nOnce you’ve found a dataset of interest, submit the link to the website into Canvas."
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html",
    "href": "7-Semester_Project/Semester_Project_Template.html",
    "title": "Semester Project",
    "section": "",
    "text": "[Provide a brief overview of your research question. Explain why you’re interested in the topic, where you found your data and the objectives of your analysis.]"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#data-import-and-preparation",
    "href": "7-Semester_Project/Semester_Project_Template.html#data-import-and-preparation",
    "title": "Semester Project",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\n# Be sure to make this using ggplot()"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#visualizations",
    "href": "7-Semester_Project/Semester_Project_Template.html#visualizations",
    "title": "Semester Project",
    "section": "Visualizations",
    "text": "Visualizations\n[Use ggplot() to create presentation-worth graphs (histograms, scatterplots, bar charts, boxplots, etc). Include an overall visualization of your response variable and one or two exploratory graphs showing the relationship between explanatory variables and your response.\nNOTE: The type of graphs and summaries you do depends entirely on the type of data (categorical or quantitative) that you have for your project.]\n\n# Create Visuals Here"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#numerical-summaries",
    "href": "7-Semester_Project/Semester_Project_Template.html#numerical-summaries",
    "title": "Semester Project",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n[Create numerical summaries (favstats(), prop.table() with row or column totals, etc.) of your response variable. Include a table of summary statistics broken out by your explanatory variable. Include a discussion of what you observe.]\n\n# Create numerical summaries"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#state-ho-and-ha",
    "href": "7-Semester_Project/Semester_Project_Template.html#state-ho-and-ha",
    "title": "Semester Project",
    "section": "State Ho and Ha",
    "text": "State Ho and Ha"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#hypothesis-test",
    "href": "7-Semester_Project/Semester_Project_Template.html#hypothesis-test",
    "title": "Semester Project",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\n# Perform the appropriate test for the data selected"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#confidence-interval-where-appropriate",
    "href": "7-Semester_Project/Semester_Project_Template.html#confidence-interval-where-appropriate",
    "title": "Semester Project",
    "section": "Confidence Interval (where appropriate)",
    "text": "Confidence Interval (where appropriate)"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html#check-test-requirements",
    "href": "7-Semester_Project/Semester_Project_Template.html#check-test-requirements",
    "title": "Semester Project",
    "section": "Check Test Requirements",
    "text": "Check Test Requirements"
  },
  {
    "objectID": "8-References/Test_Requirements.html",
    "href": "8-References/Test_Requirements.html",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "",
    "text": "Understanding the assumptions and requirements for different statistical tests is crucial to ensure valid conclusions. Below is a summary of the key conditions that must be checked for common tests used in introductory statistics."
  },
  {
    "objectID": "8-References/Test_Requirements.html#one-sample-z-test",
    "href": "8-References/Test_Requirements.html#one-sample-z-test",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "One-Sample z-test",
    "text": "One-Sample z-test\nRequirements:\n\nRandomization ensures observations are independent\nSample Size:\n\nExpected counts of successes and failures in each group should be at least 10:\n\n\n\\[np \\ge 10\\]\n\\[n(1-p) \\ge 10\\] where p is the hypothesized value for the population proportion"
  },
  {
    "objectID": "8-References/Test_Requirements.html#confidence-intervals",
    "href": "8-References/Test_Requirements.html#confidence-intervals",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nObserved number of yeses and nos are both greater than 10:\n\\[n\\hat{p} \\ge 10\\] \\[n(1-\\hat{p}) \\ge 10\\]"
  },
  {
    "objectID": "8-References/Test_Requirements.html#two-sample-proportions-z-test",
    "href": "8-References/Test_Requirements.html#two-sample-proportions-z-test",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "Two-Sample Proportions z-test",
    "text": "Two-Sample Proportions z-test\n\nObserved number of yeses and nos are both greater than 10 for BOTH groups:\n\n\\[n_1\\hat{p_1} \\ge 10\\] \\[n_1(1-\\hat{p_1}) \\ge 10\\]\nAND\n\\[n_2\\hat{p_2} \\ge 10\\] \\[n_2(1-\\hat{p_2}) \\ge 10\\]"
  },
  {
    "objectID": "8-References/Test_Requirements.html#confidence-interval",
    "href": "8-References/Test_Requirements.html#confidence-interval",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nSame for the Hypothesis Test for the Difference between the 2 proportions."
  },
  {
    "objectID": "8-References/Test_Requirements.html#chi-square-test-of-independence",
    "href": "8-References/Test_Requirements.html#chi-square-test-of-independence",
    "title": "Hypothesis Test Requirements for Various Statistical Tests",
    "section": "5. Chi-square Test of Independence",
    "text": "5. Chi-square Test of Independence\n\nTest:\n\nChi-square test for independence in contingency tables\n\n\n\nRequirements:\n\nSample Size:\n\nExpected cell counts should be at least 5 in each cell.\n\nIndependence:\n\nObservations should be independent."
  },
  {
    "objectID": "Spaced_Learning_Activities/Confidence_Intervals_for_Proportions_Practice.html",
    "href": "Spaced_Learning_Activities/Confidence_Intervals_for_Proportions_Practice.html",
    "title": "Confidence Intervals and Hypothesis Tests For Proportions - Practice",
    "section": "",
    "text": "Introduction\nIn this exercise, you will practice creating and interpreting confidence intervals for proportions.\n\n\nBird Watching\nA wildlife researcher is studying a rare bird species in a southeast Idaho. They identify a sample of 500 birds, and only 6 are found to be of this rare species. Estimate the proportion of the entire bird population that is this rare species using a 95% confidence interval.\n\nprop.test(x = , n = , conf.level=)$conf.int\n\nError in prop.test(x = , n = , conf.level = ): argument \"x\" is missing, with no default\n\n\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\nQUESTION: Are the assumptions for the normality of \\(\\hat p\\) are met? (Check \\(n\\hat{p}\\) and \\(n(1-\\hat{p})\\))\nANSWER:\nQUESTION: Perform a statistical hypothesis test to see if the population proportion, \\(p\\), is less than 3%. Use \\(\\alpha=0.05\\).\n\nprop.test(x=, n=, p=, alternative=)\n\nError in prop.test(x = , n = , p = , alternative = ): argument \"x\" is missing, with no default\n\n\nQUESTION: What is the p-value for the above test?\nANSWER:\nQUESTION: State your conclusion using the technical requirements and in context of the research question.\nANSWER:\nQUESTION: Are the test requirements for a statistical hypothesis for a proportion met? (Check \\(np\\) and \\(n(1-p)\\))\nANSWER:\n\n\nCustomer Satisfaction Survey\nA company surveyed 150 customers about their satisfaction, and 105 responded that they were satisfied. Estimate the proportion of all customers who are satisfied with the company’s service using a 99% confidence level.\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\nQUESTION: Are the assumptions for the normality of \\(\\hat p\\) are met? Why?\nANSWER:\n\n\nHealth Study\nA health study surveyed 250 individuals about their exercise habits, and 140 reported exercising regularly. Construct a 97.25% confidence interval for the proportion of the general population that exercises regularly.\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\nQUESTION: Are the assumptions for the normality of \\(\\hat p\\) are met? Why?\nANSWER:"
  },
  {
    "objectID": "Spaced_Learning_Activities/DataCleaning1.html",
    "href": "Spaced_Learning_Activities/DataCleaning1.html",
    "title": "Data Cleaning Practice",
    "section": "",
    "text": "Introduction\nIn this exercise, you’ll practice the basics of data cleaning and visualization.\nLoad and explore the data then respond to the questions below.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nreal_estate &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/MadisonCountyRealEstateA.csv')\n\nQUESTION: Make a Histogram of ListPrice\nQUESTION: What is the shape of the distribution of list prices?\nANSWER:\nQUESTION: Make a table of summary statistics for ListPrice\nQUESTION: What is the minimum value for list prices?\nANSWER:\nCreate a clean dataset that:\n\nRemoves houses that are more than $1M\nHas only 1 Story houses\nNo Missing Elementary school information\nIs in Rexburg\nContains only the columns: ListPrice, Style, Elementary, City, and IsFixerUpper\n\nQUESTION: What percent of homes in the clean dataset are fixer-uppers?\nQUESTION: Use GGPLOT to create a boxplot for ListPrice that compares Elementary Schools?\nQUESTION: Which Elementary school appears to have the lowest distribution of housing prices?\nANSWER:"
  },
  {
    "objectID": "Spaced_Learning_Activities/DataCleaning2.html",
    "href": "Spaced_Learning_Activities/DataCleaning2.html",
    "title": "Data Cleaning 2",
    "section": "",
    "text": "Clean the Madison County real estate data and answer the following questions.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nreal_estate &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/MadisonCountyRealEstateA.csv')\n\nCreate a dataset that:\n\nHas houses with 2 or more stories including Split Entry (Style)\nHas 5 or more bedrooms\n\n\nclean &lt;- real_estate %&gt;%\n\nError in parse(text = input): &lt;text&gt;:5:0: unexpected end of input\n3: \n4: \n  ^\n\n\nQUESTION: What are the mean and standard deviation of Taxes for these houses?\nCheck to see if Taxes are normally distributed for these houses:"
  },
  {
    "objectID": "Spaced_Learning_Activities/ProportionTable_Therapies.html",
    "href": "Spaced_Learning_Activities/ProportionTable_Therapies.html",
    "title": "Table Proportions: Therapy",
    "section": "",
    "text": "Introduction\nThis is a synthetic dataset that compares the effectiveness of 2 different psychotherapies, Psychodynamic Therapy (PDT) and Cognitive Behavioral Therapy (CBT).\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\ntherapy &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/CBT_vs_PDT_Treatment_Data.csv')\n\nglimpse(therapy)\n\nRows: 254\nColumns: 2\n$ ImprovementLevel &lt;chr&gt; \"Improved\", \"Improved\", \"Improved\", \"Improved\", \"Impr…\n$ Treatment        &lt;chr&gt; \"CBT\", \"CBT\", \"CBT\", \"CBT\", \"CBT\", \"CBT\", \"CBT\", \"CBT…\n\n\nCreate a proportion table that shows the improvement level of each treatment. (Should sum to 1 across treatment groups):\nQUESTION: What percent of CBT patients improved?\nANSWER:\nQUESTION: What percent of CBT patients worsened?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-process",
    "href": "4-Foundations_Statistical_Inference/7-Introducing_Hypothesis_Tests.html#the-process",
    "title": "Introducing Hypothesis Tests",
    "section": "The Process",
    "text": "The Process\nEvery statistical hypothesis test will follow the form:\n\nState the null and alternative hypotheses and choose the significance level, \\(\\alpha\\)\n\nCalculate the Test Statistic using your sample data\n\nCalculate the P-value, the probability of observing the test statistic that we observed (or more extreme) if the null hypothesis were true.\n\nMake your conclusion by comparing the P-value to \\(\\alpha\\)\n\nCheck the test requirements\n\nThe math behind the above calculations will vary depending on context. We will only touch lightly on the underlying math and rely on R to execute it. This class will focus on the application of the above process.\nYou may not yet understand the terminology above, but we introduce the process now as a roadmap. We will revisit this each time we introduce a new hypothesis test, and before you know it, this will be second nature.\nNOTE: This is a new paradigm for most students. It’s perfectly fine to be confused at first. But just as with learning to code in R, we will make sense of this framework for statistical inference."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html",
    "title": "Statistical Process Practice Problems",
    "section": "",
    "text": "These questions will help deepen your understanding of the statistical process that we refer to throughout the semester."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html#background",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html#background",
    "title": "Statistical Process Practice Problems",
    "section": "Background",
    "text": "Background\nA sample of 504 patients in early stages of Alzheimer’s disease is divided into two groups. One group receives an experimental drug and the other a placebo. The advance of the disease is tracked at one-month intervals over the next year.\nQUESTION: Is this a designed experiment or an observational study? Explain your answer.\nANSWER:"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html#background-1",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html#background-1",
    "title": "Statistical Process Practice Problems",
    "section": "Background",
    "text": "Background\nSay that you are a researcher for a large pharmaceutical company that has produced a new drug that is believed to help reduce the risk of developing arthritis. You test the drug and compare it to a placebo by randomly choosing one group of men and women in their 50’s to take the drug and one group of men and women in their 50’s to take a placebo. Each participant takes the drug or placebo for a period of 10 years, after which the effects of the drug are measured against the effects of the placebo.\nQUESTION: What (or who) are the experimental units or subject?\nANSWER:\nQUESTION: What is the population of the study?\nANSWER:"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html#background-2",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html#background-2",
    "title": "Statistical Process Practice Problems",
    "section": "Background",
    "text": "Background\nEach presidential election, many different groups want to know how popular each candidate is among the electorate. In the recent election, one marketing research firm was asked by a special interest group to help estimate the proportion of Americans who supported a particular candidate. From a list of all registered voters, 8000 names were randomly selected. An automated phone-call was made to each of the 8000 individuals selected asking them to answer a few questions indicating their voting preferences. The marketing firm received 2969 responses.\nQUESTION: What is the population of this study?\nANSWER:\nQUESTION: What is the sample of this study?\nANSWER:"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html#new-drug",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html#new-drug",
    "title": "Statistical Process Practice Problems",
    "section": "New Drug",
    "text": "New Drug\n\nBackground\nSay that you are a researcher for a large pharmaceutical company that has produced a new drug that is believed to help reduce the risk of developing arthritis. You test the drug and compare it to a placebo by randomly choosing one group of men and women in their 50’s to take the drug and one group of men and women in their 50’s to take a placebo. Each participant takes the drug or placebo for a period of 10 years, after which the effects of the drug are measured against the effects of the placebo.\nQUESTION: What (or who) are the experimental units or subject?\nANSWER:\nQUESTION: What is the population of the study?\nANSWER:"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process_Practice.html#elections",
    "href": "1-Getting_Started/5-Stat_Process_Practice.html#elections",
    "title": "Statistical Process Practice Problems",
    "section": "Elections",
    "text": "Elections\n\nBackground\nEach presidential election, many different groups want to know how popular each candidate is among the electorate. In the recent election, one marketing research firm was asked by a special interest group to help estimate the proportion of Americans who supported a particular candidate. From a list of all registered voters, 8000 names were randomly selected. An automated phone-call was made to each of the 8000 individuals selected asking them to answer a few questions indicating their voting preferences. The marketing firm received 2969 responses.\nQUESTION: What is the population of this study?\nANSWER:\nQUESTION: What is the sample of this study?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html",
    "title": "One- and Two-sample Statistical Inference",
    "section": "",
    "text": "This activity is designed to provide opportunities for practicing one- and two-sample hypothesis tests and confidence intervals.\nFrom context in the background information, you should be able determine what statistical test is appropriate and what the null and alternative hypotheses should be.\nFor each problem:\n\nCarefully read the background information\n\nDefine the null and alternative hypotheses\n\nPerform the relevant statistical test in R\n\nCheck the test requirements\n\nConstruct and interpret the confidence interval\n\nNOTE: If not stated, use \\(\\alpha=0.05\\). Also, if not specified, you can create graphs using base R or ggplot().\nDon’t forget to load the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#test-requirements",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#test-requirements",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Test Requirements",
    "text": "Test Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 90% confidence interval for the population average sodium in the cans.\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#summarize-the-data",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#summarize-the-data",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Summarize the Data",
    "text": "Summarize the Data\nCreate a table of summary statistics comparing cholesterol levels for each treatment group:\nQUESTION: What is the difference between the sample averages of cholesterol between the 2 groups?\nANSWER:\nUse ggplot() to create a side-by-side boxplot of the cholesterol levels of each treatment group:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#hypothesis-test",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\n\nTest Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 95% confidence interval for the difference between the mean cholesterol levels:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#summarize-the-data-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#summarize-the-data-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Summarize the Data",
    "text": "Summarize the Data\nCreate histograms for before and after weights separately:\nCreate a histogram of the differences (after-before):\nQUESTION: What is the average difference between after and before weights?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#hypothesis-test-1",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the appropriate statistical test:\nQUESTION: What is the P-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question.\nASNWER:\n\nTest Requirements\nCheck the test requirements for this hypothesis test:\nQUESTION: What are the test requirements, and are they satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval-2",
    "href": "5-Statistical_Tests_Part1/AA_One_and_Two_Sample_Inference_Means_Only.html#confidence-interval-2",
    "title": "One- and Two-sample Statistical Inference",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCalculate a 95% confidence interval for the mean difference between before and after weights:\nQUESTION: Interpret the confidence interval in context of the research question.\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html",
    "href": "6-Statistical_Tests_Part2/1-Regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Consider the relationship between Score on a math exam and a student’s self-reported Confidence Rating.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\n\nQuestion: What is the explanatory variable, \\(x\\)?\nAnswer:\nQuestion: What is the response variable, \\(y\\)?\nAnswer:\nPlot the relationship:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Based on the scatterplot, does the relationship appear roughly linear?\nAnswer:\nQuestion: What is the direction of the relationship? Answer:\nQuestion: What do you think is the strength of the relationship? (Strong/Moderate/Weak) Answer:\nQuestion: What is the correlation coefficient, r? Answer:\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html#plotting-the-regression-line",
    "href": "6-Statistical_Tests_Part2/1-Regression.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\n\nBase R\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function in base R, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html#ggplot",
    "href": "6-Statistical_Tests_Part2/1-Regression.html#ggplot",
    "title": "Simple Linear Regression",
    "section": "GGPlot",
    "text": "GGPlot\nUsing ggplot(), we can simply add a geom_smooth() geometry and specify the method of “smoothing” as a linear model:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\nBy default, this gives us a confidence interval for the slope of the regression line. We can turn that off by forcing the “standard error” to be FALSE:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html#hypothesis-testing-for-regression",
    "href": "6-Statistical_Tests_Part2/1-Regression.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most often only interested in the slope:\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese hypotheses are the same for all simple linear regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\nTechnically correct: 95% Confident that the true population slope is within (10.674297, 14.71535)\nContextual Explanation: For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html#regression-requirements-is-our-p-value-sus",
    "href": "6-Statistical_Tests_Part2/1-Regression.html#regression-requirements-is-our-p-value-sus",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements (Is our P-value sus?)",
    "text": "Regression Requirements (Is our P-value sus?)\nThere are certain requirements for all statistical tests to be valid. For regression analysis, we have to have 5 requirements for our statistics to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot(lm_output).\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is done by: plot(lm_output, which=1). This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\).\n\n\n\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")\n\n\n\n\nCreate a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:\n\n\n\n\nState your null and alternative hypotheses:\nHo:\nHa:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#introduction",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#introduction",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#load-the-data-and-libraries",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#load-the-data-and-libraries",
    "title": "ANOVA Practice",
    "section": "",
    "text": "library(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#explore-the-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#explore-the-data",
    "title": "ANOVA Practice",
    "section": "",
    "text": "Create a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "title": "ANOVA Practice",
    "section": "",
    "text": "State your null and alternative hypotheses:\nHo:\nHa:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#introduction-1",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#introduction-1",
    "title": "ANOVA Practice",
    "section": "Introduction",
    "text": "Introduction\nA study was conducted to determine if different types of material can reduce the amount of mosquito human contact. The researchers evaluated five different types of patches 1=Odomos, 2=Deltamethrin, 3=Cyfluthrin, 4=D+O, 5=C+O.\nThe amount of mosquito human contact was measured to assess any differences between the five different types of material. Use a level of significance of 0.05."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#load-the-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#load-the-data",
    "title": "ANOVA Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\nMosquitoPatch &lt;- rio::import(\"https://raw.githubusercontent.com/rdcromar/Math221D/main/MosquitoPatch.csv\") %&gt;% mutate(Treatment = factor(Treatment))"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#review-the-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#review-the-data",
    "title": "ANOVA Practice",
    "section": "Review the Data",
    "text": "Review the Data\nCreate a side-by-side boxplot for human contact for each of the treatment groups.\nAdd a title and change the colors of the boxes.\nCreate a summary statistics table for human contact for each of the treatment groups:\nQuestion: What do you observe?\nAnswer:\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "href": "6-Statistical_Tests_Part2/2-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "title": "ANOVA Practice",
    "section": "Perform the Appropriate Analysis",
    "text": "Perform the Appropriate Analysis\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic (F-value)?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\n\nNumerator (between Groups) Degrees of Freedom\n\nDenominator (within groups) Degrees of Freedom\nAnswer:\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html",
    "title": "Regression Practice",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data with a regression line\nCalculating and interpreting the correlation coefficient, r\nFitting a linear regression analysis\nVerifying if a linear model is model is adequate:\n\nChecking for linearity (scatterplot)\nChecking for constant variance (plot(lm_output, which=1))\nChecking for normality of residuals (qqPlot(lm_output$residuals))\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#good-price",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#good-price",
    "title": "Regression Practice",
    "section": "Good Price?",
    "text": "Good Price?\nLastly, the car you’re interested in buying has around 100,000 miles and costs $11,200. Could this be considered a good deal? Why?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model-1",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model-1",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements-1",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements-1",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nUse ggplot() to create a scatter plot with the regression line, then calculate r:\n\n# geom_smooth(method=\"lm\")\n\nQUESTION: Does the relationship look linear?\nANSWER:\nQUESTION: What is the correlation coefficient, r?\nANSWER:\nQUESTION: What does r show?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model-2",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#fit-a-linear-regression-model-2",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nQUESTION: What is the slope of the regression line, and what does it mean?\nANSWER:\nQUESTION: What is the intercept and what does it mean?\nANSWER:\nQUESTION: What is the p-value?\nANSWER:\nQUESTION: State your conclusion in context of the research question:\nANSWER:\nQUESTION: What is the confidence interval for the slope?\nANSWER:\nQUESTION: Explain the confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements-2",
    "href": "6-Statistical_Tests_Part2/1-Regression_Practice.html#check-model-requirements-2",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\nQUESTION: Do the test requirements appear to be satisfied? Why?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The proper analysis for situations with a quantitative response variable and a categorical explanatory variable with 3 or more levels/categories is called Analysis of Variance. This has the unfortunate acronym, ANOVA."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#test-statistic",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#test-statistic",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Statistic",
    "text": "Test Statistic\nThe Test Statistic for testing the relationship between a quantitative response variable and a categorical explanatory variable with &gt;2 categories is called \\(F\\).\nThe formula for \\(F\\) is complicated. But we can consider the signal-to-noise ratio as follows:\nSIGNAL: The variation between the means, ie. how far apart the group means are spread out.\nNOISE: The variation within groups. This variation represents the natural variation inherent in the data independent of any differences between the means.\n\\[ F=\\frac{\\text{Between-group Variation}}{\\text{Within-group Variation}}\\]\nLet’s look at an example to illustrate.\n\nSeed Treatment Experiment\nImagine you’re comparing 4 pesticide seed treatments for corn seeds. These treatments are all supposed to improve emergence uniformity and crop yield.\nThe null hypothesis is that seed treatment has no effect on crop yield. This is typically expressed as a statistical hypothesis by stating that the average emergence is the same regardless of which treatment you use.\nWe would express this as:\n\\[H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\\] PONDER: Can you see the connection between the treatment means being the same and “no relation” between treatment and emergence uniformity?\nThe alternative hypothesis for an \\(F\\)-statistics is:\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\]\nNote: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that at least one mean is different.\nA good experiment would run multiple experimental trials on different farms. Ideally, we’d want a good random sample of farms in the area we expect to be planting.\nIf the null hypothesis were true, any differences observed between the treatment means should be due to random variation or “noise” inherent in growing corn. The within-treatment variation is a good estimate of the “noise” because it reflects the natural variability in seed yields when the null hypothesis is true.\nIf the null hypothesis is false, we would expect the treatment means to be much more spread out relative to the within-treatment variation.\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more evidence we have against \\(H_0\\).\nA large F-statistics means the group averages are much more spread out than the natural variability in crop emergence.\n\n\nVisualizing Between-group and Within-group Variation\nWe can get a sense the signal and noise visually."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#the-f-distribution",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "The F-Distribution",
    "text": "The F-Distribution\nThe shape of the probability distribution of the F-statistic changes shape depending on how much data we have AND how many groups we have. These parameters in the \\(F\\)-distribution are called Degrees of Freedom, and the F-distribution has 2 sets.\nThe numerator (signal), or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe will get the values of both degrees of freedom directly from the R output.\nUnlike the standard normal distribution, the \\(F\\)-distribution is not centered around zero and can never be negative because \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests.\n\nThe P-value\nThe P-value is the probability of observing an \\(F\\)-statistic higher than the one we observed if the null hypothesis were true.\nFor a cutoff a given \\(\\alpha\\):\n\nWhen \\(P\\)-value is less than \\(\\alpha\\), we REJECT the null hypothesis and say we have SUFFICIENT evidence to suggest that at least one of the group means is different\nWhen \\(P\\)-value is greater than \\(\\alpha\\), we FAIL TO REJECT the null hypothesis and say we have INSUFFICIENT evidence to suggest that any of the means are different"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#test-requirements",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Requirements",
    "text": "Test Requirements\nJust as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\nTest Equal Variances\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\nNormality of Residuals\nResiduals are defined as the difference between an observation and it’s “expected” value:\n\\[residual = \\text{observed}-\\text{expected}\\]\nIn the case of ANOVA, the expected value is simply the group average. We get the residual for each observation by taking the observed value and subtracting its group mean. Once again, R will provide this for us.\nFor our analysis to be valid, we need to see if the residuals are normally distributed. We can do a qqPlot() of the residuals to assess this requirement.\nIf both requirements are met, the P-value is sound."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#analysis-in-r",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Analysis in R",
    "text": "Analysis in R\nTo perform an ANOVA, we use the aov() function with the familiar formula data$response ~ data$explanatory.\nThe aov() function by itself isn’t very useful on its own. We can use the summary() function to give us all the output we need to perform a hypothesis test.\nWe typically name our output using the assignment operator &lt;- to make it easier to get the information we would like.\nThe inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\naov_output &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(aov_output)\n\n\nCheck ANOVA Requirments:\nJust as with the other statistical tests, ANOVA has certain requirements for us to be able to trust the P-value.\n\nChecking Equal Standard Deviations:\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n# extract only the standard deviations from favstats output using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\n# if max/min &lt; 2, then we're ok\n\nThis gives us a valuable rule by which to decide if there is a violation of the constant variance requirement for the F-test.\nWe can visualize this just as we did with the regression analysis by plotting the residuals against their predicted values.\nplot(aov_output, which = 1)\nIn an ANOVA, the predicted value is the group mean. Because every observation in the group has the same predicted value, the residuals line up on vertical lines according to their group average. We can visually assess if one of the groups looks like there is much smaller variability than the others.\n\n\nAssessing Normality of the Residuals\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n# Name the output of the aov() function `output`:\n\naov_output &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# do a qqPlot of the residuals (observation values - group mean) that R calculates.  We can use th `$` to tell R what part of the output we want to make into a qqPlot():  \n\nqqPlot(aov_output$residuals)\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed.\n\n\nWhich Means are Different?\nIf the overall \\(F\\)-test is statistically significant (\\(P\\)-value &lt; \\(\\alpha\\)) then we can use what is called Tukey’s Honest Significant Difference to look at pairwise comparisons between the groups. We use the TukeyHSD() function by including the aov() model output:\nTukeyHSD(aov_output)\n\nCAUTION: Only use TukeyHSD() when the overall \\(F\\)-test is statistically significant.\nWe can easily plot the confidence"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#step-1-read-in-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#step-2-review-the-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#step-2-explore-the-data",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4), ylab=\"Sepal Length\", main = \"Sepal Length by Species\")\n\n\n\n\n\n\n\n\nUsing GGPlot:\n\nggplot(iris, aes(x=Species, y = Sepal.Length)) +\n  geom_boxplot(fill=c(2,3,4)) +\n  theme_bw() +\n  labs(\n    y = \"Sepal Length\",\n    title = \"Sepal Length by Species\"\n  )"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/2-ANOVA.html#step-4-perform-the-appropriate-analysis",
    "href": "6-Statistical_Tests_Part2/2-ANOVA.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\nplot(aov_sepal, which=1)\n\n\n\n\n\n\n\n\nQuestion: What is the F-statistics?\nAnswer: 119.3\nQuestion: What are the between-groups degrees of freedom?\nAnswer: 2\nQuestion: What are the within-groups degrees of freedom?\nAnswer: 147\nQuestion: What is the P-value?\nAnswer: &lt;2e-16\nQuestion: What is your conclusion?\nAnswer: Because p is less than alpha, we reject the null hypothesis. We conclude that at least one of the means of Sepal Length is different from the other group means.\n\nWhich Means are Different\nIn the event that the overall test statistics is significant, we can look deeper into the pair-wise differences using the TukeyHSD() function as follows:\n\nTukeyHSD(aov_sepal)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nThe output gives us confidence intervals and P-values for each of the pairwise differences between the means."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/1-Regression.html#regression-requirements",
    "href": "6-Statistical_Tests_Part2/1-Regression.html#regression-requirements",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements",
    "text": "Regression Requirements\nThere are certain requirements for all statistical tests to be valid. For regression analysis, we have to have 5 requirements for our statistics to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon_i\\), are normally distributed\nThe variance of the \\(\\epsilon_i\\) is constant for all values of \\(x\\)\nThe \\(x_i\\) are fixed and measured without error (i.e. \\(x_i\\) can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot(lm_output).\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is done by: plot(lm_output, which=1). This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  }
]