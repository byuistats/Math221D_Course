[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BYU-Idaho PSYCH 302: Research Methods and Statistics Using R",
    "section": "",
    "text": "Testing Testing Testing"
  },
  {
    "objectID": "index.html#an-interactive-textbook",
    "href": "index.html#an-interactive-textbook",
    "title": "BYU-Idaho PSYCH 302: Research Methods and Statistics Using R",
    "section": "An Interactive Textbook",
    "text": "An Interactive Textbook\nThis course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook. You can download the notes, practice problems and application activities which can be edited locally on your computer. Completed assignments will be submitted in Canvas as web-page reports.\n\nDownload Course Material\nTo download the folder containing all of the exercises, notes and application activities by following these steps:\n\nCreate a class folder where you will save all your work throughout this semester. This can be on your computer locally or in cloud-based storage such as One-Drive or iCloud.\nClick on the link below to download the .zip file containing all contents of the course website:\n\n\n Download Course Files \n\n\nOpen the .zip file\nSave the Student_Work folder to your class folder created in step 1. This can typically be done by right clicking the folder then copying and pasting it into the new location.\n\nThese files will be the basis of your coursework.\n\n\nHow to Use This Book\nEach unit contains:\n\nBackground readings\nClass notes including example code\nPractice problems\nApplication Activities\n\nUnit 1 introduces R and summarizing data. It’s a gentle enough introduction to enable even those who have never coded before to start working with R and begin creating lovely web-based reports.\nUnit 2 expands on the R basics of Unit 1 and introduces the Tidyverse. This is a powerful way to wrangle data and create even more beautiful data visualizations.\nUnit 3 covers the fundamentals of statistics including elementary probability, the Normal Distribution, sampling distributions and the Central Limit Theorem.\nUnit 4 introduces hypothesis testing, confidence intervals and statistical inference for means.\nUnit 5 introduces hypothesis tests for regression and categorical data.\nThe Semester Project is an opportunity for students to demonstrate the skills acquired during the course. You will be able to find and import data into R, clean the data, create top notch visualizations, perform appropriate statistical analysis and create a beautiful report.\nR Help provides additional resources for learning how to use R.\n\nCourse Objectives\nBy the end of this semester, students will confidently be able to:\n\nImport data and apply basic data wrangling to moderately messy data\nDescribe data with numerical and graphical summaries\nMake evidence-based decisions regarding situations with inherent randomness\nInvestigate questions through the application of probability distributions\nPerform the appropriate statistical analysis based on specific data being analyzed including hypothesis tests and confidence intervals\nCreate web-based reports to communicate the results of statistical analyses to relevant audiences"
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html",
    "href": "6-Semester_Project/Semester_Project_Instructions.html",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "For this project, you will create an html report that is an original analysis based on data that you find.\nStart with a research question that interests you. It can be about any topic (finance, music, video games, hunting, weather, mental health, AI…seriously anything!)\nFor this project, a good research question will be interesting to you AND feasible to find available data. You can certainly think of exciting research questions for which the data are impossible to find or collect. Expect to refine your research question as you begin the data search.\nOnce you have a research question in mind, start looking for data relating to it. Below are some links that might be helpful for finding datasets.\n\n\nGoogle actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#helpful-links",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#helpful-links",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "Google actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#define-the-problem",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#define-the-problem",
    "title": "Semester Project Instructions",
    "section": "1. Define the Problem",
    "text": "1. Define the Problem\nInclude an introduction that describes why you were interested in the topic and what you envisioned for an analysis. Some questions to address:\n\nWhat is the population of your research?\nWhat do you think is the nature of the relationship you hope to discover?\nWhat type of data are you looking for (quantitative? categorical?)\n\nWhat type of analysis are you expecting to do (t-test, regression? ANOVA? Chi-square test for independence? etc.)"
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#collect-the-data",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#collect-the-data",
    "title": "Semester Project Instructions",
    "section": "2. Collect the Data",
    "text": "2. Collect the Data\nTalk about the process of how you found the data and whether you had to make adjustments to your research question."
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#describe-the-data",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#describe-the-data",
    "title": "Semester Project Instructions",
    "section": "3. Describe the Data",
    "text": "3. Describe the Data\nInclude summary statistics (favstats, proportions, contingency tables, etc.)\nInclude GGPlot visualizations (boxplots, histograms/density plots, bar charts, etc.). Make the charts as understandable as possible without having to read the text description. That means make sure:\n\nAxes are labeled appropriately\nIt has a descriptive title\nLabel lines or points that are highlighted in the graph"
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#analyze-the-data",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#analyze-the-data",
    "title": "Semester Project Instructions",
    "section": "4. Analyze the Data",
    "text": "4. Analyze the Data\nPerform the appropriate analysis for the data collected (t-test, F-test, regression, Chi-square, etc.)"
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Instructions.html#take-action",
    "href": "6-Semester_Project/Semester_Project_Instructions.html#take-action",
    "title": "Semester Project Instructions",
    "section": "5. Take Action",
    "text": "5. Take Action\nBased on your statistical inference, what recommendations or actions would you take?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "In this section we will show how to summarize data numerically and visually.\nWe typically summarize categorical variables with counts and proportions. Visually, an ordered bar chart is the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\nLet’s look at survey carried out by FiveThirtyEight about the first 6 Star Wars films.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')\n\n\n\nUse the table() function to tabulate counts for a categorical variable. For example, if we want to tabulate the favorability of Han Solo\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nYou can also get proportions by inputting a table into the prop.table() function:\n\nprop.table(table(sw$`Favorability_Han Solo`))\n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\n\n\nThe table() function can make a “cross table” of 2 categorical variables. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages.\n\n\n\n\nWe can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nWe first select only the columns that we will use in the visualization, then drop out all the missing values using drop_na() in the tidy fashion.\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\n\nshot_first &lt;- sw %&gt;%\n  select(who_shot_first, Gender) %&gt;%\n  drop_na()\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Use the table() function to tabulate counts for a categorical variable. For example, if we want to tabulate the favorability of Han Solo\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nYou can also get proportions by inputting a table into the prop.table() function:\n\nprop.table(table(sw$`Favorability_Han Solo`))\n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\n\n\nThe table() function can make a “cross table” of 2 categorical variables. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "We can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nWe first select only the columns that we will use in the visualization, then drop out all the missing values using drop_na() in the tidy fashion.\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\n\nshot_first &lt;- sw %&gt;%\n  select(who_shot_first, Gender) %&gt;%\n  drop_na()\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "title": "Summarizing Categorical Data",
    "section": "Visualization",
    "text": "Visualization\nCreate a bar chart for favorability of Han Solo by whether or not they are fans of Star Trek (fan_of_star_trek).\nStart by making a new dataset called trekky that only includes the 2 relevant columns and drops the missing values.\n\n\ntrekky &lt;- sw %&gt;%\n\nError: &lt;text&gt;:4:0: unexpected end of input\n2: trekky &lt;- sw %&gt;%\n3: \n  ^\n\n\nQuestion: What observations can you make based on the visualization?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "title": "Summarizing Categorical Data",
    "section": "Proportion Table",
    "text": "Proportion Table\nWe would like to compare what percent of female respondents do not understand the question compared to the percent of males who do not understand the question.\nCreate a proportion table that can answer this question:\nQuestion: What percent of female respondents do not understand the question?\nAnswer:\nQuestion: What percent of male respondents do not understand the question?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/12-Chi_Square_Practice.html",
    "href": "5-Statistical_Tests_Part2/12-Chi_Square_Practice.html",
    "title": "Chi-Square Practice",
    "section": "",
    "text": "Complete the following questions about testing for independence between 2 categorical variables. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/12-Chi_Square_Practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part2/12-Chi_Square_Practice.html#hypothesis-test",
    "title": "Chi-Square Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the Chi-square test for independence:\nHINT: The way the data were imported into bp_alcohol, it contains a columns, V1, which is a categorical variable. You only want to include the counts from the table. Run chisq.test() dropping the first column. Also, remember to name the output so you can easily extract information needed to check the requirements.\nQuestion: Are the requirements satisfied for the \\(\\chi\\)-square test for independence?\nAnswer:\nQuestion: How many degrees of freedom does this \\(\\chi\\)-square test have?\nAnswer:\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion in context of thise problem:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html",
    "href": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html",
    "title": "2-Sample Proportion Practice",
    "section": "",
    "text": "Complete the following questions about testing differences between 2 proportions. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval",
    "href": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of females to males who prefer to keep the penny:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nYou also want to see if there is a difference between the proportion of women who want to keep the penny and the proportion of men who want to keep the penny. Use a level of significance of \\(\\alpha = 0.05\\).\nState your null and alternative hypotheses:\n\\[H_0: p_{female}???p_{male}\\]\n\\[H_a: p_{female}???p_{male}\\]\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test-1",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nConstruct a null and alternative hypothesis for the study:\n\\[H_0: \\]\n\\[H_a: \\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval-1",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of divorces between communicative and non-communicative couples:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html",
    "href": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html",
    "title": "One-sample Proportion Practice",
    "section": "",
    "text": "Recall that we can use the normal distribution to approximate the distribution of \\(\\hat{p}\\) under certain conditions.\nQuestion: What to we need to check before we assume normality for a hypothesis test?\nAnswer:\nQuestion: What to we need to check before we assume normality for confidence intervals?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#hypothesis-test",
    "title": "One-sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState the null and alternative hypotheses (replace the ??? with the correct information):\n\\[H_0: p=???\\]\n\\[H_a: p???0.15\\] Choose your confidence level:\n\\[\\alpha = 0.\\]\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your selected \\(\\alpha\\) and P-value, what is your conclusion?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#confidence-interval",
    "href": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#confidence-interval",
    "title": "One-sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 99% confidence interval for the true population proportion of teenagers who smoke?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#perscription-use",
    "href": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#perscription-use",
    "title": "One-sample Proportion Practice",
    "section": "Perscription Use",
    "text": "Perscription Use\nA survey of doctors is planned to see what percentage prescribe a certain medication. Find the sample size required to achieve a 2% margin of error if the confidence level is 95%. Assume there are no prior estimates for p.\nQuestion: How many doctors would we need to survey?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#proportion-of-left-handed-artists",
    "href": "5-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#proportion-of-left-handed-artists",
    "title": "One-sample Proportion Practice",
    "section": "Proportion of Left-handed Artists",
    "text": "Proportion of Left-handed Artists\nSuppose you would like to study creativity of left-handed people. Your favorite generative AI told you that the estimated population of left-handed people in the United States is 11%. We would like to have a Margin of Error less \\(\\pm3\\%\\) (0.03) with 90% confidence.\nQuestion: How many artists would you have to survey?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "href": "5-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "title": "Distribution of P-hat Practice",
    "section": "",
    "text": "Instructions\nComplete the following questions about the sampling distribution of \\(\\hat{p}\\). When completed, Render the qmd file and submit the html.\n\n\nQuestions\nQuestion: When can we use the normal distribution to approximate the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the mean of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the standard deviation of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nSuppose the true population proportion, \\(p\\), of people who support a candidate for office is 52%. We would like to learn something about a sample proportion, \\(\\hat{p}\\), with a sample size \\(n=1000\\) suggesting that the candidate will lose the election (\\(\\hat{p}&lt;0.50\\)).\nUse the following R code to answer the questions below:\n\nphat &lt;- \np &lt;-\nn &lt;- \n\nsigma_phat &lt;- sqrt(p*(1-p)/n)  \n\nError in eval(expr, envir, enclos): object 'p' not found\n\nz &lt;- (phat-p) / sigma_phat\n\nError in eval(expr, envir, enclos): object 'phat' not found\n\n# Left Tail:\npnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n# Right tail\n1-pnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n\nQuestion: What is the \\(z\\)-score associated with \\(\\hat{p}&lt;0.5\\)?\nAnswer:\nQuestion: What is the standard deviation of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the probability of a sample of size \\(n=1000\\) suggesting that the candidate will lose even if the true population support, \\(p=0.52\\)?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html",
    "title": "Regression Practice",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data with a regression line\nCalculating and interpreting the correlation coefficient, r\nFitting a linear regression analysis\nVerifying if a linear model is model is adequate:\n\nChecking for linearity (scatterplot)\nChecking for constant variance (plot(lm_output, which=1))\nChecking for normality of residuals (qqPlot(lm_output$residuals))\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nAdd the regression line to your chart:\n\n# Sometimes you have to run the whole chunk with plot() and the abline() together:\n\n#plot()\n#abline(lm_output$coefficients)\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nLastly, the car you’re interested in buying has around 100,000 miles and costs $11,200. Could this be considered a good deal? Why?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "title": "Introducing the Bivariate Data",
    "section": "",
    "text": "Bivariate data refers to situations where you have one quantitative response variable and one quantitative explanatory variable. This requires a different approach to analysis and visualization.\nBy the end of this lesson, you should be able to:\n\nCreate scatterplots for 2 quantitative variables in base R and GGPlot\n\nDescribe what the correlation coefficient, \\(r\\), quantifies\n\nCalculate \\(r\\) using the cor() function\n\nTwo datasets will be used to illustrate these concepts. The first contains self-reported confidence in mathematics and test scores. The second contains eruption duration and time between eruptions of Old Faithful geyser in Yellowstone National Park.\n\n# Load the libraries and data\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between students’ self reported confidence rating and test score.\nQuestion: Which variable is the Explanatory variable, \\(x\\)?\nAnswer:\nQuestion: Which is the Response variable, \\(y\\)?\nAnswer:\n\n# Base R\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\n# ggplot\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648\n\n\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nWhich variable is the Explanatory variable? Which is the Response?\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\).\n\n\n\n\nlibrary(rio)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following object is masked from 'package:rio':\n\n    factorize\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")\n\n\n\n\nCreate a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:\n\n\n\n\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic? Answer:\nQuestion: What are the degrees of freedom for your analysis? a. Numerator (between Groups) Degrees of Freedom b. Denominator (within groups) Degrees of Freedom Answer:\nQuestion: What is the P-value? Answer:\nQuestion: Do you reject the null hypothesis? Why? Answer:\nQuestion: State your conclusion in context of the problem. Answer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot()) Answer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "title": "ANOVA Practice",
    "section": "",
    "text": "library(rio)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following object is masked from 'package:rio':\n\n    factorize\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "title": "ANOVA Practice",
    "section": "",
    "text": "Create a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "title": "ANOVA Practice",
    "section": "",
    "text": "State your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic? Answer:\nQuestion: What are the degrees of freedom for your analysis? a. Numerator (between Groups) Degrees of Freedom b. Denominator (within groups) Degrees of Freedom Answer:\nQuestion: What is the P-value? Answer:\nQuestion: Do you reject the null hypothesis? Why? Answer:\nQuestion: State your conclusion in context of the problem. Answer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot()) Answer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "title": "ANOVA Practice",
    "section": "Introduction",
    "text": "Introduction\nA study was conducted to determine if different types of material can reduce the amount of mosquito human contact. The researchers evaluated five different types of patches 1=Odomos, 2=Deltamethrin, 3=Cyfluthrin, 4=D+O, 5=C+O.\nThe amount of mosquito human contact was measured to assess any differences between the five different types of material. Use a level of significance of 0.05."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "title": "ANOVA Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\nMosquitoPatch &lt;- rio::import(\"https://raw.githubusercontent.com/rdcromar/Math221D/main/MosquitoPatch.csv\") %&gt;% mutate(Treatment = factor(Treatment))"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "title": "ANOVA Practice",
    "section": "Review the Data",
    "text": "Review the Data\nCreate a side-by-side boxplot for human contact for each of the treatment groups.\nAdd a title and change the colors of the boxes.\nCreate a summary statistics table for human contact for each of the treatment groups:\nQuestion: What do you observe? Answer:\nQuestion: What is the maximum standard deviation? Answer:\nQuestion: What is the minimum standard deviation? Answer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "title": "ANOVA Practice",
    "section": "Perform the Appropriate Analysis",
    "text": "Perform the Appropriate Analysis\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic (F-value)? Answer:\nQuestion: What are the degrees of freedom for your analysis?\n\nNumerator (between Groups) Degrees of Freedom\nDenominator (within groups) Degrees of Freedom Answer:\n\nQuestion: What is the P-value? Answer:\nQuestion: Do you reject the null hypothesis? Why? Answer:\nQuestion: State your conclusion in context of the problem. Answer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot()) Answer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "title": "2-Sample Independent T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing 2-sample independent t-tests using R. For each question, you will:\n\nRead in data\nIdentify the Response and Independent variable\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals\n\nWhen you finish, render this document and submit the .html in Canvas.\n\n# Load the libraries\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\ndating &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/dating_attractive_longformat.csv')"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Explore the Data",
    "text": "Explore the Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate a side-by-side boxplot for the amount of reported importance of attractiveness for each biosex.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics for each group (favstats()):"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the ??? with the appropriate symbol):\n\\[H_0:  \\mu_{F}???\\mu_{M}\\]\n\\[H_a:\\mu_{F}???\\mu_{M}\\]\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = Female.\nCheck that the samples for both groups are normally distributed\n\nqqPlot(dating$Importance~dating$Biosex)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: How many degrees of freedom for this test?\nAnswer:\nQuestion: What is the p-value?\nAnswer:\nQuestion: What do you conclude?\nAnswer:\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of dental costs for each office:\nCreate a side-by-side boxplot for dental costs for each office.\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{IF}???\\mu_{R}\\]\n\\[H_a:\\mu_{IF}???\\mu_{R}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the difference in costs between the IF and Rexburg offices:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of birth weights for each country:\nCreate a side-by-side boxplot for birth weights for each country:\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{A}???\\mu_{IL}\\]\n\\[H_a:\\mu_{A}???\\mu_{IL}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the average difference in weights between babies born to mothers in Africa and Illinois:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html",
    "title": "Paired T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing dependent samples using R. For each question, you will:\n\nRead in data\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data",
    "title": "Paired T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nhelmet_fit &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/helmet_fit.csv\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-Test Practice",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms and summary statistic tables for the measurements for each type of tool:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nGive the summary statistics (favstats()) for the differences in the measured head diameters.\nQuestion: What does a negative number mean given the definition of the difference?\nAnswer:\nCreate a qqPlot() of the differences:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "title": "Paired T-Test Practice",
    "section": "Step 4: Perform the Appropriate Analyses",
    "text": "Step 4: Perform the Appropriate Analyses\n\nHypothesis Test\nMake the following null and alternative hypothesis correct by deleting what doesn’t belong:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\n\n# Perform a t-test for the mean of the differences between cardboard and caliper data\n\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Can we trust this P-value? (eg. How many differences in our sample? Check the qqPlot() of the differences for normality)\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Intervals\nCreate a 95% confidence interval for the true average difference between the cardboard and the metal measurement tools:\nQuestion: Give a one-sentence explanation of your confidence interval.\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data-1",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data-1",
    "title": "Paired T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "title": "Paired T-Test Practice",
    "section": "Step 2: Review the Data Generate Hypotheses",
    "text": "Step 2: Review the Data Generate Hypotheses\nCreate histograms and summary statistics tables for the cholesterol measurements at 2 days and at 4 days."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "title": "Paired T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you are going to define the difference (chol_day2 - chol_day4 or chol_day4 - chol_day2).\nWhat does a negative number mean:\nWhat is your null and alternative hypotheses:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\nCreate a qqPlot() of the differences and determine if you can trust the statistical tests."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-Test Practice",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nPerform a t-test for the differences.\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference in cholesterol scores:\nQuestion: Explain your confidence interval in context of the research question\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "href": "4-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "title": "Don’t Take it Personally",
    "section": "",
    "text": "Introduction\nIn this activity, you will execute statistical hypothesis tests and generate confidence intervals for each of the Big 5 personality traits using data collected from a random sample of Brother Cannon’s Math 221 students.\nQuestion: What is the population of this analysis?\nAnswer:\nFor each personality trait, include:\n\nA statement of the null and alternative hypotheses and why you chose the alternative you did.\nChoose alpha, $= $\nCheck that you can trust the normality of the mean (n &gt; 30 or qqPlot(respons_variable))\nRun the one-sample t-test and state your conclusion (technical and contextual explanation)\nCalculate a \\(1-\\alpha\\) level confidence interval and describe in words what it means in context of the research question\n\nRecall that we can use favstats() to get summary statistics, boxplot() and histogram() to get visualizations, and the t.test() function to get hypothesis tests and confidence intervals. Be sure to label your plots’ axes and include a title.\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\n\n\nExtroversion\nState your null and alternative hypotheses:\n\\[H_o:  \\mu = 50\\]\n\\[H_a:  \\mu &gt; 50\\]\n\\[\\alpha = 0.025 \\]\n1. Create a table of summary statistics:\n\n# Extroversion\n\nfavstats(big5$Extroversion) %&gt;% knitr::kable()\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n1\n42\n58\n73\n100\n56.98267\n21.09599\n404\n1\n\n\n\n\n\n\nCreate a histogram of Extroversion:\n\n\n### I'm making a single variable called \"extrov\" that drops the missing values.  You can do the same thing with the other traits to make analysis a little easier\nextrov &lt;- na.omit(big5$Extroversion) \n\nhistogram(extrov, xlab = \"Extroversion\", main = \"Histogram of Extroversion Percentiles\")\n\n\n\n\n\n\n\n\n\nPerform the one-sample t.test:\n\n\n# For Hypothesis Test:\nt.test(extrov, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  extrov\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\n\nExplain your conclusion:\n\nTechnical: Because the p-value is less than \\(\\alpha\\), I reject the null hypothesis.\nContextual: I have sufficient evidence to suggest that Brother Cannon’s students are, on average, more Extroverted than the general population.\n\nCreate a Confidence Interval for the average extroversion of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test(extrov, conf.level = 1-.025)$conf.int\n\n[1] 54.62135 59.34399\nattr(,\"conf.level\")\n[1] 0.975\n\n\n\nExplain your confidence interval:\n\nI am 97.5% confident that the true average extroversion of Brother Cannon’s students is somewhere between the 54.62 and 59.34 percentiles.\n\n\nAgreeableness\nState your null and alternative hypotheses. For example, do you think Brother Cannon’s students are more, less, or just different than the general population?\n\\[H_o:  \\mu  \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Agreeableness:\n\nCreate a histogram of Agreeableness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average agreeableness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nOpenness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n\nCreate a table of summary statistics for Openness:\n\n\nCreate a histogram of Openness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average openness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nNeuroticism\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Neuroticism:\n\nCreate a histogram of Neuroticism:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average neuroticism of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nConscientiousness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n1. Create a table of summary statistics:\n\nCreate a histogram of Conscientiousness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average conscientiousness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:"
  },
  {
    "objectID": "3-Stat_Fundamentals/10-Assessing_Normality.html",
    "href": "3-Stat_Fundamentals/10-Assessing_Normality.html",
    "title": "Assessing Normality",
    "section": "",
    "text": "Assessing Normality\nIn practice, we must confirm that the distribution of sample means is normally distributed. This is true when:\n\nThe population is normally distributed\n\\(n &gt; 30\\) because of the Central Limit Theorem\n\nBut how do you know if a population is normally distributed? In the real world, there is no teacher to tell you when to assume a population is normal.\nIf our sample size is large enough, we don’t have to worry. We can trust the Central Limit Theorem.\nIf our sample size is &lt; 30, we can assess the normality of our sample to decide if we can still trust output of our hypothesis tests and confidence intervals.\nPreviously, we’ve used histograms to help visualize the distribution of a sample. However, when sample sizes are small, even samples from a standard normal distribution can look skewed.\nAll of the examples below are histograms of random samples from actual standard normal distributions:\n\n\nA New Way to Assess Normality\nStatisticians use something called a QQPlot which works better at assessing normality. QQPlots plot the sorted data of each point in a dataset with the theoretical percentile from a normal distribution. If the data and theoretical percentiles line up, then we can be reasonably sure the population is normally distributed.\nThese are easier to use than to explain. We use the car library and the function qqPlot() to create a chart. (Note the Capital P in the middle.)\nKey Point: If most of the data points line up in the shaded region, we can consider the population as normally distributed.\nBelow are examples of QQPlots for a normal distribution and a right skewed distribution.\n\n\nThese work much better for small sample sizes. Below are several examples of QQPlots for small sample sizes:\n\nWhile not perfect, these are a vastly better tool to assess normality than a histogram.\n\n\n\nPractice\nLet’s try assessing the normality of some data. Below are 3 datasets. Find the response variable(s) from each and determine if the data are sufficiently normally distributed:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(car)\n\nold_faithful &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/OldFaithful.xlsx')\n\nqqPlot(old_faithful$Duration)\n\n\n\n\n\n\n\n\n[1] 19 58\n\nqqPlot(old_faithful$Wait)\n\n\n\n\n\n\n\n\n[1] 265 127\n\nrent &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/Rent.csv')\n\n\nmcat_gpa &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')"
  },
  {
    "objectID": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html",
    "href": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html",
    "title": "Probability Calculations for Means (Class)",
    "section": "",
    "text": "The Central Limit Theorem states that for a large enough sample size (\\(n&gt;30\\)) the distribution of sample means is approximately normal with mean, \\(\\mu_{\\bar{x}} = \\mu\\) and \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) regardless of the distribution of the population.\nWe can assume the distribution of sample means is approximately normal if:\n\nThe population is normally distributed\nn &gt; 30\n\nDon’t forget, that if the population is normally distributed, so is the distribution of sample means regardless of sample size."
  },
  {
    "objectID": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html#gpas",
    "href": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html#gpas",
    "title": "Probability Calculations for Means (Class)",
    "section": "GPA’s",
    "text": "GPA’s\nSuppose the mean GPA of BYU-Idaho students is 3.5 and the standard deviation is 0.7. It is well known that this distribution is left-skewed. A random sample of n = 45 students will be drawn.\nQuestion: What is the mean of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population of GPAs?\nAnswer:\nQuestion: What is the standard deviation of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population?\nAnswer:\nQuestion: What is the probability that the mean GPA for 45 randomly selected BYU-Idaho students will be less than 3.3?\nAnswer:\nQuestion: What is the shape of the distribution of sample means, \\(\\bar{x}\\), when 45 students are selected?\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html#gre-scores",
    "href": "3-Stat_Fundamentals/08-CLT_Normal_Prob_for_Means.html#gre-scores",
    "title": "Probability Calculations for Means (Class)",
    "section": "GRE Scores",
    "text": "GRE Scores\nScores on the quantitative portion of the GRE are approximately normally distributed with mean, \\(\\mu=150.8\\), and standard deviation \\(\\sigma = 8.8\\).\nQuestion: Dianne earned a score of 160 on the quantitative portion of the GRE. What is the z-score corresponding to Dianne’s score?\nAnswer:\nQuestion: What is the probability that a randomly selected student will score above 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What is the probability that a randomly selected group of 18 students will have an average less than 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What is the probability that a randomly selected group of 18 students will have an average between 145 and 160 on the quantitative portion of the GRE?\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/06-Central_Limit_Theorem_Practice.html",
    "href": "3-Stat_Fundamentals/06-Central_Limit_Theorem_Practice.html",
    "title": "CLT Practice",
    "section": "",
    "text": "Instructions\nAnswer the following questions, render the document and submit the .html report.\n\n\nQuestions\nSuppose you take a sample of size \\(n=5\\) from a right skewed distribution with a population mean, \\(\\mu=125\\) and a standard deviation, \\(\\sigma=12\\)\nQuestion: What is the mean of the distribution of sample means (\\(\\mu_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the standard deviation of sample means (\\(\\sigma_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the shape of the distribution of sample means and why?\nAnswer:\nNow suppose you increase the sample size to \\(n=100\\). What is:\nMean of sample means:\nStandard deviation of sample means:\nShape of the distribution of sample means:\nDescribe the difference between the Law of Large Numbers and the Central Limit Theorem.\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html",
    "href": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html",
    "title": "Normal Probability Practice",
    "section": "",
    "text": "Recall that the normal distribution is a probability distribution defined by its center (\\(\\mu\\)) and its spread (\\(\\sigma\\))."
  },
  {
    "objectID": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html#brisket-competition",
    "href": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html#brisket-competition",
    "title": "Normal Probability Practice",
    "section": "Brisket Competition",
    "text": "Brisket Competition\nCompetition was tight at a Saint Louis BBQ competition. Brisket scores were normally distributed with an average score of 5.941 with a standard deviation of 0.04.\nQuestion: What’s the probability of getting a score GREATER than 6?\nAnswer:\nQuestion: What’s the probability of getting a score LESS than 4?\nAnswer:\nQuestion: What is the 99th percentile of this Brisket Competition?\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html#prbability-of-a-false-start",
    "href": "3-Stat_Fundamentals/04-Normal_Probability_Practice.html#prbability-of-a-false-start",
    "title": "Normal Probability Practice",
    "section": "Prbability of a “False Start”",
    "text": "Prbability of a “False Start”\n“At high level meets, the time between the gun and first kick against the starting block is measured electronically, via sensors built in the gun and the blocks. A reaction time less than 0.1 s is considered a false start. The 0.2-second interval accounts for the sum of the time it takes for the sound of the starter’s pistol to reach the runners’ ears, and the time they take to react to it.” (https://en.wikipedia.org/wiki/100_metres) Let’s suppose that reaction times are normally distributed with a mean of 0.2 seconds and a standard deviation of 0.03.\nWhat’s the probability of a false start? (meaning a reaction time LESS than 0.1)"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Distributions and Normal Computations",
    "text": "Normal Distributions and Normal Computations\n\nBaseball Batting Averages\n\nIn baseball, a player called the “pitcher” throws a ball to a player called the “batter.” The batter swings a wooden or metal bat and tries to hit the ball. A “hit” is made when the batter successfully hits the ball and runs to a point in the field called first base. A player’s batting average is calculated as the ratio of the number of hits a player makes divided by the number of times the player has attempted to hit the ball or in other words, been “at bat.” Sean Lahman reported the batting averages of several professional baseball players in the United States. (Lahman, 2010) The file BattingAverages.xlsx contains his data.\nThe following histogram summarizes the batting averages for these professional baseball players:\n\nNotice the bell-shaped distribution of the data.\nSuppose we want to estimate the probability that a randomly selected player will have a batting average that is greater than 0.280. One way to do this would be to find the proportion of players in the data set who have a batting average above 0.280. We can do this by finding the number of players who fall into each of the red-colored bins below and dividing this number by the total number of players.\n\nIn other words, we could find the proportion of the total area of the bars that is shaded red out of the combined area of all the bars. This gives us the proportion of players whose batting averages are greater than 0.280.\nOut of the 446 players listed, there are a total of 133 players with batting averages over 0.280. This suggests that the proportion of players whose batting average exceeds 0.280 is:\n\\[\\displaystyle{\\frac{133}{446}} = 0.298\\]\nAlternatively, we can use the fact that the data follow a bell-shaped distribution to find the probability that a player has a batting average above 0.280.\n\n\nDensity Curves\nThe bell-shaped curve superimposed on the histogram above is called a density curve. It is essentially a smooth histogram. Notice how closely this curve follows the observed data.\nThe density curve illustrated on the histogram of the batting average data is special. It is called a normal density curve. This density curve is symmetric and has a bell-shape.\nThe normal density curve is also referred to as a normal distribution or a “Gaussian” distribution (after Carl Friedrich Gauss.)\nThe normal density curve appears in many applications in business, nature, medicine, psychology, sociology, and more. We will use the normal density curve extensively in this course.\nAll density curves, including normal density curves, have two basic properties:\n\nThe total area under the curve equals 1.\nThe density curve always lies on or above the horizontal axis.\n\nBecause of these two properties, the area under the curve can be treated as a probability. If we want to find the probability that a randomly selected baseball player will have a batting average between some range of values, we only need to find the area under the curve in that range. This is illustrated by the region shaded in blue in the figure below.\n\nA normal density curve is uniquely determined by its mean, \\(\\mu\\), and its standard deviation, \\(\\sigma\\). So, if random variables follow a normal distribution with a known mean and standard deviation, then we can calculate any probabilities related to that variable by finding the area under the curve.\nWhen the mean of a normal distribution is 0 and its standard deviation is 1, we call it the standard normal distribution.\nWe will return to this example later, and we will find the area shaded in blue.\n\nCharacteristics of the Normal Curve\n\nIntroduction to \\(z\\)-scores\nIn Ghana, the mean height of young adult women is normally distributed with mean \\(159.0\\) cm and standard deviation \\(4.9\\) cm. (Monden & Smits, 2009) Serwa, a female BYU-Idaho student from Ghana, is \\(169.0\\) cm tall. Her height is \\(169.0 - 159.0 = 10\\) cm greater than the mean. When compared to the standard deviation, she is about two standard deviations (\\(\\approx 2 \\times 4.9\\) cm) taller than the mean.\nThe heights of men are also normally distributed. The mean height of young adult men in Brazil is \\(173.0\\) cm (“Oramento,” 2010), and the standard deviation for the population is \\(6.3\\) cm. (Castilho & Lahr, 2001) A Brazilian BYU-Idaho student, Gustavo, is \\(182.5\\) cm tall. Compared to other Brazilians, he is taller than the mean height of Brazilian men.\n\nAnswer the following question:\n\n\n\nApproximately how many standard deviations above the mean is Gustavo’s height?\n\n\n\nShow/Hide Solution\n\n\nGustavo’s height is \\(182.5 - 173.0 = 9.5 \\text{ cm  }\\) above the mean. The standard deviation of the height of Brazilian men is 6.3 cm, so his height is \\(\\displaystyle{ \\frac{9.5}{6.3} =1.508 }\\) standard deviations above the mean.\n\n\n\n\n\n\nComputing \\(z\\)-scores\nWhen we examined the heights of Serwa and Gustavo, we compared their height to the standard deviation. If we look carefully at the steps we did, we subtracted the mean height for people of the same gender and nationality from each individual’s height, respectively.\nThis shows how much taller or shorter the person is than the mean height. In order to compare the height difference to the standard deviation, we divide the difference by the standard deviation. This gives the number of standard deviations the individual is above or below the mean.\nFor example, Serwa’s height is 169.0 cm. If we subtract the mean and divide by the standard deviation, we get \\[z = \\frac{169.0 - 159.0}{4.9} = 2.041\\] We call this number a \\(z\\)-score. The \\(z\\)-score for a data value tells how many standard deviations away from the mean the observation lies. If the \\(z\\)-score is positive, then the observed value lies above the mean. A negative \\(z\\)-score implies that the value was below the mean.\nWe compute the \\(z\\)-score for Gustavo’s height similarly, and obtain \\[z = \\frac{182.5 - 173.0}{6.3} = 1.508\\] Gustavo’s \\(z\\)-score is 1.508. As noted above, this is about one-and-a-half standard deviations above the mean. In general, if an observation \\(x\\) is taken from a random process with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the \\(z\\)-score is \\[z = \\frac{x -\\mu }{\\sigma}\\]\nThe \\(z\\)-score can be computed for data from any distribution, but it is most commonly applied to normally distributed data.\n\n\n68-95-99.7% Rule for Bell-shaped Distributions\nHeights of women (or men) in a particular population follow a normal distribution. Most people’s heights are close to the mean. A few are very tall or very short. We would like to make a more precise statement than this.\n\nFor any bell-shaped distribution,\n\n68% of the data will lie within 1 standard deviation of the mean,\n95% of the data will lie within 2 standard deviations of the mean, and\n99.7% of the data will lie within 3 standard deviations of the mean.\n\nThis is called the 68-95-99.7% Rule for Bell-shaped Distributions. Some statistics books refer to this as the Empirical Rule. \n\n\n\nApproximately 68% of the observations from a bell-shaped distribution will be between the values of \\(\\mu -~\\sigma~\\)and \\(\\mu +~\\sigma\\). Consider the heights of young adult women in Ghana. We expect that about 68% of Ghanaian women have a height between the values of \\[\\mu -~\\sigma = 159.0 - 4.9 = 154.1~\\text{cm}\\] and \\[\\mu +~\\sigma = 159.0 + 4.9 = 163.9~\\text{cm}.\\]\nSo, if a female is chosen at random from all the young adult women in Ghana, about 68% of those chosen will have a height between 154.1 and 163.9 cm. Similarly, 95% of the women’s heights will be between the values of \\[\\mu - 2\\sigma = 159.0 - 2(4.9) = 149.2~\\text{cm}\\] and \\[\\mu + 2\\sigma = 159.0 + 2(4.9) = 168.8~\\text{cm}.\\]\nFinally, 99.7% of the women’s heights will be between \\[\\mu - 3\\sigma = 159.0 - 3(4.9) = 144.3~\\text{cm}\\] and \\[\\mu + 3\\sigma = 159.0 + 3(4.9) = 173.7~\\text{cm}.\\]\n\n\n\nUnusual Events\nIf a \\(z\\)-score is extreme (either a large positive number or a large negative number), then that suggests that that observed value is very far from the mean. The 68-95-99.7% rule states that 95% of the observed data values will be within two standard deviations of the mean. This means that 5% of the observations will be more than 2 standard deviations away from the mean (either to the left or to the right).\nWe define an unusual observation to be something that happens less than 5% of the time. For normally distributed data, we determine if an observation is unusual based on its \\(z\\)-score. We call an observation unusual if \\(z &lt; -2\\) or if \\(z &gt; 2\\). In other words, we will call an event unusual if the absolute value of its \\(z\\)-score is greater than 2.\n\nAnswer the following questions:\n\n\n\nOut of Serwa and Gustavo, who is physically taller?\n\n\n\nShow/Hide Solution\n\n\nGustavo is taller. He is 182.5 cm tall, and Serwa is 169.0 cm tall.\n\n\n\nRelative to their own gender and nationality, who is relatively taller?\n\n\n\nShow/Hide Solution\n\n\nRelative to other Ghanaian women, Serwa is very tall. Gustavo is tall relative to Brazilian men, but relative to people of his gender and nationality, he is not relatively taller than Serwa. Serwa has a higher \\(z\\)-score.\n\n\n\nAre either of these heights unusual?\n\n\n\nShow/Hide Solution\n\n\nSerwa’s height is unusual. Her \\(z\\)-score is: \\(z = 2.041\\) This is more than two standard deviations away from the mean. Gustavo’s height is not unusual. His \\(z\\)-score is less than two standard deviations away from the mean.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Probability Computations",
    "text": "Normal Probability Computations\nAn important part of the practice of statistics is finding areas under a normal curve. The area under a normal curve, say, to the left of a value, gives the probability of obtaining an observation less than (or equal to) that value. This is an example of converting a value to an area. It is also important to convert an area to a value. For example, if you want to find the 40th percentile for data that follow a normal distribution, you find the value of the observation such that the area (under the curve) to the left of this value is 0.40.\n\nIntroduction to the Normal Probability Applet\nThe Normal Probability Applet is a visualization program offering statistics students insights and computations for the relationship between \\(z\\)-scores and areas under the standard normal curve. You can find a link to this applet here. This app is also compatible to use on your phone, iPad, and other mobile devices. The app is stored at . If you want a copy you can open from your desktop, just right click the link and save it to your computer.\nTo use this applet, follow these instructions:\n\nClick on an area below the curved line to shade/unshade the region.\nClick and drag the red lines to adjust the \\(z\\)-scores and obtain the area.\nType a \\(z\\)-score into one of the bottom input boxes and hit “Enter” to get an area.\nType in an area and hit “Enter” to get a \\(z\\)-score.\n\n\n\n\n\n\n\n\n\nConverting a \\(z\\)-score to a Probability\nUsing this applet we can calculate proportions and probabilities based on the area under the normal curve. For the following examples, please open the Normal Probability Applet and practice using it to find areas under the curve.\nThe Normal Probability Applet is nice for visualizing areas under the curve. However, it has significant limitations. In this class, we will use R to do the heavy lifting. The pnorm(x, \\(\\mu\\), \\(\\sigma\\)) function calculates areas under the curve the normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), for a given x. The p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nBy default, pnorm() gives the area of the curve to the LEFT of the value, \\(x\\), for a normal distribution with a mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nWe can calculate probabilities directly in the original units of the data, or use the z-score with a \\(\\mu=0\\) and \\(\\sigma=1\\): pnorm(z).\nNOTE: pnorm() has a default value for the mean, \\(\\mu = 0\\) and standard deviation, \\(\\sigma = 1\\) so we don’t have to input those values when we use a z-score with the standard normal distribution.\n\nHeights of Ghanaian Women\nWe will use the example of Serwa’s height to find the proportion of young Ghanaian women who are shorter than Serwa. Recall that for the height of young Ghanaian women, the population mean is 159.0 cm and the population standard deviation is 4.9 cm. Serwa’s height is 169.0 cm. We found the \\(z\\)-score of Serwa’s height as:\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{169.0- 159.0}{4.9} = 2.041\\]\nWhat proportion of young Ghanaian women reach a height that is at or below 169 cm? To answer this question, we need to find the area under a normal density curve (i.e. the probability) that is to the left of \\(z = 2.041\\).\nTo find the area under a normal curve corresponding to a \\(z\\)-score of \\(2.041\\), do the following:\n\npnorm(2.041)\n\n[1] 0.9793746\n\n\n\n\n\n\n\n\n\n\nNOTE: The area to the left of our chosen \\(z\\)-score is also the probability that a randomly selected woman will be shorter than Serwa. The probability that a randomly selected Ghanaian woman will be shorter than Serwa is \\(0.979\\), or \\(97.9\\%\\).\n*Remember that in this course, unless otherwise specified, we round to three decimal places.\n\n\nBaseball Averages\nWe now return to the example of the baseball batting averages. We want to find the probability that a randomly selected player will have a batting average that is above 0.280. The population mean is 0.261 and the population standard deviation is 0.034. We can use this information to find a \\(z\\)-score. Then we use the applet to find the area under the normal curve to the right of this \\(z\\)-score.\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{0.280 - 0.261}{0.034} = 0.559\\]\n\nx &lt;- 0.280\nmu &lt;- 0.261\nsigma &lt;- 0.034\n\nz &lt;- (x-mu)/sigma\n\n# Area to the LEFT:\npnorm(x, mu, sigma)\n\n[1] 0.7118589\n\n## Equivilantly:\npnorm(z)\n\n[1] 0.7118589\n\n# Area to the RIGHT\n1-pnorm(x, mu, sigma)\n\n[1] 0.2881411\n\n1-pnorm(z)\n\n[1] 0.2881411\n\n\nUsing the Applet, type the \\(z\\)-score of \\(0.559\\) in one of the boxes below the horizontal axis in the applet. Click on the areas under the curve until only the region on the right is highlighted in blue.\n\nThe area under the curve to the right of \\(z = 0.559\\) is \\(0.288\\). This is the probability that a randomly selected player will have a batting average that is greater than 0.280. (Note: It is a coincidence that the area, 0.288, is close to the batting average of 0.280. There is no significance in this.)\nNotice that the area shaded in blue above 0.288 is very close to the area we found when we looked at the area represented by the bars of the histogram 0.298 that was shaded in red above.\n\n\n\nFinding the Probability of Being Between Two Values\nThe normal probability applet allows us to find the probability of being between two values as long as they are on opposite sides of the mean and equally distanced from the mean. By calculating two z-scores, one for each value, and then shading the area between them on the applet, we can find the probability. This is severely limiting.\nBelow, we demonstrate how to find the area between any two values in a normal distribution.\n\nHeights of Ghanaian Women\nWhat is the probability that a randomly selected young Ghanaian women will be between 150.0 cm and 163.0 cm tall? Recall that the average height for young Ghanaian women is \\(\\mu=159.0\\) cm and the population standard deviation is \\(\\sigma=4.9\\) cm.\nWe want to find the probability that a randomly selected woman’s height is between \\(150.0\\) cm and \\(165.0\\) cm. To do this we find the \\(z\\)-score for both values:\n\\[z_1 = \\frac{x- \\mu}{\\sigma} = \\frac{150.0 - 159.0}{4.9} = -1.837\\] \\[z_2 = \\frac{x - \\mu}{\\sigma} = \\frac{165.0 -159.0}{4.9} = 1.22\\]\nWe now answer the question by finding the area under the normal density curve (i.e. the probability) to the left of \\(z = 1.22\\) which is \\(0.8888\\) and also the area under the normal density curve to the left of \\(z = -1.837\\) which is \\(0.033\\). To find the area between \\(z = 1.22\\) and \\(z = -1.837\\), we subtract the smaller area from the larger.\n\\[0.8888 - 0.0331 = 0.8557\\]\nSo the probability that the height of a randomly selected young Ghanaian woman will be between 150.0 cm and 165.0 cm is \\(0.8558\\). This is the same as the proportion of all young Ghanaian women who are between 150.0 and 165.0 cm tall.\nTo find the probability of being between any 2 numbers for a normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), we can use the following R code:\n\nvalue1 &lt;- 150\nvalue2 &lt;- 165\nmu &lt;- 159\nsigma &lt;- 4.9\n\npnorm(value2, mean = mu, sd = sigma) - pnorm(value1, mean = mu, sd = sigma)\n\n[1] 0.8564917\n\n\nNOTE: Value 1 is the lower of the two values and Value 2 is the higher value. Also, this answer is slightly different than above due to when you round. This answer is more accurate because we rounded at the end of the probability calculations rather than rounding the z-scores, then calculating probabilities.\n\n\n\nCalculating Percentiles using a Normal Distribution\nA percentile is a number such that a specified percentage of the population are at or below this number. For example, the 25th percentile is the number in a data set that is greater than or equal to 25% of all the values in the data set.\nWe can find percentiles for a given dataset by using the quantile() function as described in the chapter on summarizing data\nHowever, to calculate percentiles from a normal distribution, we use the qnorm(percentile, mu, sigma) function.\nNOTE: R typically uses the word quantile when referring to percentiles. So the \\(q\\) in qnorm() stands for quantile.\nTo find the height of a Ghanaian woman corresponding to the 90th percentile:\n\nqnorm(.90, mean = 159, sd = 4.9)\n\n[1] 165.2796\n\n\nThe 90th percentile of the heights averages is 165.2796. That means that 90% of the Ghanaian women are shorter than 165.2796 cm tall."
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "title": "The Normal Distribution (Reading)",
    "section": "Roughly Assessing Normality Using a Histogram",
    "text": "Roughly Assessing Normality Using a Histogram\n\nBaseball Batting Averages\nConsider the data on the batting averages of Major League Baseball players. The histogram of the batting averages showed a distinct bell-shaped curve.\n\nLater in the course, we will learn a better way to assess normality. For now, we use histograms as a rough way to look for symmetry.\n\nR Instructions\n\n\nHere is a refresher of how to make a Histogram in R\nFor more detailed instructions revisit Summarizing Data\n\nRead in the data\n\n\nlibrary(rio)\nlibrary(mosaic)\nbat_avg &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BattingAverages.xlsx\")\n\n\nUse histogram(data$column_name) where column_name refers to the column you would like to use to create a histogram.\n\n\nhistogram(bat_avg$BattingAvg)\n\n\n\n\n\n\n\n\n\nCustomize your graph to make it presentation worthy by adding labels and modifying colors:\n\n\nhistogram(bat_avg$BattingAvg, main = \"Distribution of Batting Averages\", xlab = \"Batting Average\", ylab = \"\", col = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBody Temperatures\nA group of researchers led by Philip A. Mackowiak, MD, conducted a study to assess the true mean body temperatures of healthy adults. They selected n = 148 subjects between the ages of 18 and 40 years old, representative of the general population.\nEach volunteer was given a physical to assure that they were not ill at the time of the data collection. Their axillary (under the arm) body temperature was measured and reported in a paper published in the Journal of the American Medical Association. [1] These data were extracted and are presented in the file BodyTemp. The body temperatures are given in degrees Fahrenheit.\n\nAnswer the following questions:\n\n\n\nMake a histogram of the body temperature data.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\nlibrary(mosaic)\nbody_temp &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BodyTemp.xlsx\")\nhistogram(body_temp$BodyTemp)\n\n\n\n\n\n\n\n\n\n\nBased on your histogram, what is the shape of the distribution of these data?\n\n\n\nShow/Hide Solution\n\n\nThe data appear to be normally distributed.\n\n\n\nCalculate the following numerical summaries of the data: sample mean, sample standard deviation, and sample size.\n\n\n\nShow/Hide Solution\n\n\nfavstats(body_temp$BodyTemp)\n\n  min   Q1 median   Q3   max     mean        sd   n missing\n 96.2 97.8   98.3 98.7 100.8 98.23446 0.7375924 148       0\n\n\nThe sample mean is 98.23, sample standard deviation is 0.738, and sample size is n = 148."
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#summary",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#summary",
    "title": "The Normal Distribution (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA normal density curve is symmetric and bell-shaped with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\). The curve lies above the horizontal axis and the total area under the curve is equal to 1. A standard normal distribution has a mean of 0 and a standard deviation of 1.\nA z-score is calculated as:\n\n\\[\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{x-\\mu}{\\sigma}}\\]\n\nA z-score tells us how many standard deviations above (\\(+Z\\)) or below (\\(-Z\\)) the mean (\\(\\mu\\)) a given value (\\(x\\)) is.\nTo calculate probabilities for a normal distribution with mean, \\(\\mu\\) and standard deviation \\(\\sigma\\) for a given observation \\(x\\), use pnorm(x, mu, sigma) or 1-pnorm(x, mu, sigma) to get the desired probability (below, above). Alternatively, calculate the \\(z\\)-score and use pnorm(z) or 1-pnorm(z). In every case, the probability is given by the Area under the curve.\nThe 68-95-99.7% rule states that when data are normally distributed, approximately 68% of the population lies within \\(z=1\\) standard deviation (\\(\\sigma\\)) from the mean, approximately 95% of the data lie within \\(z=2\\) standard deviations from the mean, and approximately 99.7% of the data lie within \\(z=3\\) standard deviations from the mean. For example, this rule approximates that 2.5% of observations will be less than a z-score of \\(z=-2\\).\nPercentiles can be calculated using the qnorm(percentile, mu, sigma) function."
  },
  {
    "objectID": "2-Tidy_Data/Bonus_Mutate.html",
    "href": "2-Tidy_Data/Bonus_Mutate.html",
    "title": "mutate()",
    "section": "",
    "text": "Adding Columns\nIf we want to create a new column in our dataset, we use the tidy function, mutate(). Consider again the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nTo create a new column, “pipe” the raw data into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: It is widely known that arm span is typically very close to one’s height. Let’s create a column of the ratio of Height (Height_cm) to armspan (Armspan_cm) and call the new column, ht_to_span. If common knowledge is correct, we would expect the ratio to be close to 1 on average.\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nmean(clean$ht_to_span)\n\n[1] Inf\n\n\nNotice that the mean is Inf which means infinity, or undefined. This is likely due to R attempting to divide a number by zero, meaning someone answered that their arm span was zero. This is where filter() comes in handy. We can filter out the rows where Armspan_cm is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nIt’s difficult to imagine someone who is 40 times taller than his or her arm span. But this is sufficient to illustrate the mutate() function.\nEXAMPLE: Now let’s make a new column that converts Height_cm into inches:\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54)\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers of ht_to_wt ratio and select only the columns of interest.\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54) %&gt;%\n  select(Height_cm, Armspan_cm, ht_to_span, Height_in) %&gt;% \n  filter(ht_to_span &lt; 1.5,\n         ht_to_span &gt; .5)\nclean\n\n# A tibble: 268 × 4\n   Height_cm Armspan_cm ht_to_span Height_in\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1       182       142       1.28       71.7\n 2       190       192       0.990      74.8\n 3       172       167       1.03       67.7\n 4       163       160       1.02       64.2\n 5        51        52       0.981      20.1\n 6       181       187       0.968      71.3\n 7       160       159       1.01       63.0\n 8       156       142.      1.10       61.4\n 9       169       162       1.04       66.5\n10       160       160       1          63.0\n# ℹ 258 more rows\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nI decided on 1.5 and .5 as the filter values by trial and error. There are better ways to determine outliers. But I suspect it is rare indeed for someone to be 50% taller than their armspan or 50% shorter."
  },
  {
    "objectID": "2-Tidy_Data/Bonus_GroupBy_Summarise.html",
    "href": "2-Tidy_Data/Bonus_GroupBy_Summarise.html",
    "title": "group_by() + summarise()",
    "section": "",
    "text": "Summarizing Data\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nThe mosaic library is great for numerical summaries of quantitative variables using the favstats() function. We can create tables of the 5 number summary, mean, standard deviation, sample size, and number of missing values with one line of code:\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\n\nWe can add a grouping variable to get the same summary for each level of a group, using ~\n\nfavstats(survey$Height_cm ~ survey$Gender)\n\n  survey$Gender  min  Q1 median     Q3   max     mean       sd   n missing\n1        Female 5.50 160  162.5 167.15 182.8 158.6461 24.53056 152       0\n2          Male 1.68 172  177.9 182.80 999.0 179.3065 69.47610 160       0\n\n\nThis works great if you want to do one response/dependent variable at a time. But we often want specific summaries of data (often by groups) of more than one variable in the dataset.\nWe can use a combination of tidyverse functions, group_by() and summarise() to create custom summary tables.\nThe group_by() signals to R that whatever follows should be done for each level of the column(s) identified inside the parentheses. We can then “pipe” the grouped dataset into a summarize function and define what summary statistics we would like. summarise() works very much like the mutate() function in that we create a name for our summary and tell R how to make it.\nEXAMPLE: Let’s calculate the means for Height_cm, Reaction_time, and Social_Websites_Hours for Males and Females:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 2 × 4\n  Gender mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        159.           0.717                  14.4\n2 Male          179.           0.639                  14.0\n\n\nPro Tip: Recall that the mean() function returns “NA” when there are missing values in the data. Adding na.rm=TRUE to your functions will make sure that you get a mean value.\nEXAMPLE: Let’s do the same means but for handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 3 × 4\n  Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Ambidextrous        255.           0.352                  20.2\n2 Left-Handed         164.           1.45                   14.5\n3 Right-Handed        167.           0.583                  13.9\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers for reaction times (reaction times greater than 1 second), and height outliers (taller than 7 feet tall), and social media hours (more than 100 hours).\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Reaction_time &lt; 1,\n         Social_Websites_Hours &lt; 100) %&gt;%\n  select(Gender, Height_cm, Reaction_time, Social_Websites_Hours)\n\n# Pipe the new clean dataset into the group_by() and summarise() as above:\n\nclean %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    sd_ht = sd(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    sd_react_time = sd(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    sd_social_hrs = sd(Social_Websites_Hours, na.rm=TRUE)\n  )\n\n# A tibble: 2 × 7\n  Gender mean_height sd_ht mean_react_time sd_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        158.  25.1           0.444         0.134                  12.9\n2 Male          174.  23.4           0.395         0.139                  12.5\n# ℹ 1 more variable: sd_social_hrs &lt;dbl&gt;\n\n\n\n\n\n\nGrouping by Multiple Variables\nIt is simple to get summary statistics for multiple grouping factors.\nEXAMPLE: Suppose we want the same means calculated above, but for gender and handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 6 × 5\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female Ambidextrous        134.           0.361                  27  \n2 Female Left-Handed         160.           0.524                  12.8\n3 Female Right-Handed        159.           0.750                  14.3\n4 Male   Ambidextrous        315.           0.348                  16.8\n5 Male   Left-Handed         167.           2.29                   16.1\n6 Male   Right-Handed        175.           0.420                  13.5\n\n\nI can also use the n() function without any inputs to count the number of observations in each group:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    N = n()\n  )\n\nclean\n\n# A tibble: 6 × 6\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs     N\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt; &lt;int&gt;\n1 Female Ambidextrous        134.           0.361                  27       3\n2 Female Left-Handed         160.           0.524                  12.8    17\n3 Female Right-Handed        159.           0.750                  14.3   132\n4 Male   Ambidextrous        315.           0.348                  16.8     6\n5 Male   Left-Handed         167.           2.29                   16.1    19\n6 Male   Right-Handed        175.           0.420                  13.5   135\n\n\nThis shows me that there are only 3 female ambidextrous students in the sample and 6 male ambidextrous students."
  },
  {
    "objectID": "2-Tidy_Data/05-Exploring_New_Data_with_Tidyverse.html",
    "href": "2-Tidy_Data/05-Exploring_New_Data_with_Tidyverse.html",
    "title": "Into The Tidyverse",
    "section": "",
    "text": "Statistics is only as interesting as the research questions we create. However, we cannot hope to answer those questions appropriately without going through the trouble of wrangling data.\nThe more time we spend digging into the data and noticing irregularities, the more likely we are to make better research questions and more appropriate conclusions. It doesn’t matter how sophisticated our analysis becomes if it’s based on bad data.\nIn this activity, you will explore a survey about happiness and related factors. By reviewing the data at a high level and then drilling into specific variables, you will gain a better idea about what this survey contains, generate interesting research questions and, with clean data, make better-informed answers to those questions."
  },
  {
    "objectID": "2-Tidy_Data/05-Exploring_New_Data_with_Tidyverse.html#lying",
    "href": "2-Tidy_Data/05-Exploring_New_Data_with_Tidyverse.html#lying",
    "title": "Into The Tidyverse",
    "section": "Lying",
    "text": "Lying\nRepeat the above analysis comparing how peoples’ happiness scores depend on their attitudes about lying.\n\nCreate a new dataset called lying that excludes the #N/A values in Lying and the Happiness scores outliers, and includes only “Happiness_score” and “Lying”:\n\n\n\nlying_data &lt;- happiness %&gt;%\n  filter(Happiness_score &lt;= __, \n         _______________ &gt;= 0, \n         Lying __ \"#N/A\"\n  ) %&gt;%\n  ______(Happiness_score, Lying)\n\n#View(lying_data)\n\nError: &lt;text&gt;:3:30: unexpected input\n2: lying_data &lt;- happiness %&gt;%\n3:   filter(Happiness_score &lt;= __\n                                ^\n\n\n\nCreate a side-by-side boxplot and summary statistics (using favstats()) table for each attitude about Lying:\n\n\n\nboxplot(____________________ ~ lying_data$Lying)\n\nfavstats(lying_data$Happiness_score ~ _______________)\n\nError: &lt;text&gt;:2:10: unexpected input\n1: \n2: boxplot(__\n            ^"
  },
  {
    "objectID": "2-Tidy_Data/03-Select.html",
    "href": "2-Tidy_Data/03-Select.html",
    "title": "select()",
    "section": "",
    "text": "Selecting Columns\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nIt is likely that we are not interested in analyzing every column in this dataset. Many may even be useless. We can use the tidyverse function, select() to create a subset of the columns that we are primarily interested in.\nRecall that we can “pipe” the raw data into tidy functions using %&gt;%. Suppose we want to see if there are differences in reaction times (Reaction_time) for left-handed and right-handed students. We could create a more manageable dataset with only the columns of interest:\n\nsurvey %&gt;%\n  select(Handed, Reaction_time)\n\n# A tibble: 312 × 2\n   Handed       Reaction_time\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Left-Handed          0.349\n 2 Right-Handed         0.358\n 3 Right-Handed         0.447\n 4 Right-Handed         0.438\n 5 Left-Handed          0.542\n 6 Right-Handed         0.428\n 7 Ambidextrous         0.258\n 8 Right-Handed         0.427\n 9 Right-Handed         0.412\n10 Right-Handed         0.346\n# ℹ 302 more rows\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter out Ambitextrous participants and reaction time outliers (reaction times less than 1 seconds):\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  select(Handed, Reaction_time)\n\nboxplot(clean$Reaction_time ~ clean$Handed, col = c(2,3), ylab = \"Reaction Times\", xlab=\"\", main = \"Distribution of Reaction times for \\n Left and Right-hand Dominance\")\n\n\n\n\n\n\n\n\nWhich hand dominance appears to have quicker reaction times?"
  },
  {
    "objectID": "2-Tidy_Data/01-Wrangling_Basics.html",
    "href": "2-Tidy_Data/01-Wrangling_Basics.html",
    "title": "Intro to Data Wrangling",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "2-Tidy_Data/01-Wrangling_Basics.html#the-pipe-in-action",
    "href": "2-Tidy_Data/01-Wrangling_Basics.html#the-pipe-in-action",
    "title": "Intro to Data Wrangling",
    "section": "The Pipe in Action",
    "text": "The Pipe in Action\nSuppose we have a dataset, marital_status_data, with 2 columns: age and marital_status. We expect the marital_status to be one of 4 options: “Single”, “Married”, “Divorced”, “Widowed”, but some joker input, “It’s complicated”. Because we are mainly interested in making inference about the primary statuses, we may want to filter out the rows with “It’s complicated” as the status.\nWe would begin with the raw data, “pipe” it into the filter() function and tell R what I want it to do. I can either tell R what I want to keep or what I want to exclude (more here):\nmarital_Status_data %&gt;% filter(marital_status != \"It's complicated\")\nNOTE: The != is read “not equal to”, and is a common logical operator used in computer programming. So the above code returns a subset of the original data omitting the rows.\nI can use pipes sequentially to do complicated data wrangling in very few lines of code."
  },
  {
    "objectID": "1-Data_Literacy/9-Summarizing_Data_Practice.html",
    "href": "1-Data_Literacy/9-Summarizing_Data_Practice.html",
    "title": "Summarizing Data Practice",
    "section": "",
    "text": "Introduction\nIn this assignment, you will use several datasets to summarize data numerically and visually.\n\n# Load the libraries and the datasets\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import('https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx') %&gt;% tibble()\n\nwordle &lt;- read_csv(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/Wordle.csv\")\n\n\n\n1 - Whoopsies!\nOn rare occasions, a medical procedure is performed on the wrong body part of a patient’s body or on the wrong patient. These are called wrong-site and wrong-patient mistakes. Such errors occur hundreds of times each year across the United States. The medical community is trying to eliminate these errors but have had difficulty reducing their frequency. In a small percentage of these cases, the patient files a lawsuit against the hospital. Philip Stahel et al. conducted a study on these mistakes and the lawsuits that follow.\nThe data in the file WrongSiteWrongPatient.xlsx represent the amount (in US dollars) hospitals have been required to pay in wrong-site and wrong-patient lawsuits sourced from JAMA Surgery. Some of the values equal zero, indicating that the hospital won the legal battle.\nQuestion: Find the mean and median amount hospitals had to pay in Wrong-Site lawsuits. Round your answer to the nearest whole dollar.\nMean Wrong-Site:\nMedian Wrong-Site:\nCreate a histogram of the Wrong-Site data and describe its shape:\nShape of Wrong-Site:\nQuestion: Find the mean and median amount hospitals had to pay in Wrong-Patient lawsuits. Round your answer to the nearest whole dollar.\nMean Wrong-Patient:\nMedian Wrong-Patient:\nCreate a histogram of the Wrong-Patient data and describe its shape:\nShape of Wrong-Patient:\nQuestion: Which error is the most costly for the medical community?\n\n\n2 - A la Mode\nThe mode is mostly used for low frequency count data or for tallying the most frequently occurring category in categorical data analysis.\nQuestion: Give 3 examples of situations where the mode might be useful:\n\n\n\n\n\n\n\nWorldle you like to know!\nBrother Cannon’s family plays the New York Times Wordle every day. Brother Cannon and his son, Ben, are the most competitive. The dataset wordle.csv contains the number of guesses for the last few months of Wordles for Brother Cannon and his son.\nUsing favstats() function, create a table of summary statistics for each player:\nQuestion: Which player has the lowest standard deviation?\nAnswer:\nUsing the table() function, create a table of frequencies for each player and score:\nQuestion: What is the mode for each player?\nAnswer:\nQuestion: Who is the better player, and why?\nAnswer:"
  },
  {
    "objectID": "1-Data_Literacy/7-Introducing_R_Practice.html",
    "href": "1-Data_Literacy/7-Introducing_R_Practice.html",
    "title": "Introducing R",
    "section": "",
    "text": "In these document, we explore 2 datasets. The first is data collected on the duration and the time between geyser eruptions at Old Faithful in Yellowstone National park. The second is data collected about expressions of gratitude and their impact on subjective well-being.\nFor each dataset, you will create numerical and visual summaries of the data."
  },
  {
    "objectID": "1-Data_Literacy/7-Introducing_R_Practice.html#calculate-the-summary-statistics-for-duration",
    "href": "1-Data_Literacy/7-Introducing_R_Practice.html#calculate-the-summary-statistics-for-duration",
    "title": "Introducing R",
    "section": "Calculate the Summary Statistics for Duration",
    "text": "Calculate the Summary Statistics for Duration\nWhat is the mean duration time of Old Faithful eruptions?\nWhat is the standard deviation of duration?"
  },
  {
    "objectID": "1-Data_Literacy/7-Introducing_R_Practice.html#create-a-historgram-for-duration",
    "href": "1-Data_Literacy/7-Introducing_R_Practice.html#create-a-historgram-for-duration",
    "title": "Introducing R",
    "section": "Create a Historgram for Duration",
    "text": "Create a Historgram for Duration\nCreate a histogram and describe the shape of the distribution of duration:"
  },
  {
    "objectID": "1-Data_Literacy/7-Introducing_R_Practice.html#calculate-summary-statistics-for-wait-time",
    "href": "1-Data_Literacy/7-Introducing_R_Practice.html#calculate-summary-statistics-for-wait-time",
    "title": "Introducing R",
    "section": "Calculate Summary Statistics for Wait time",
    "text": "Calculate Summary Statistics for Wait time\nQuestion: What is the mean wait time between eruptions?\nAnswer:\nQuestion: What is the maximum wait time between eruptions?\nAnswer:\nQuestion: The middle 50% of wait times will be between what 2 numbers?\nAnswer:"
  },
  {
    "objectID": "1-Data_Literacy/7-Introducing_R_Practice.html#histogram",
    "href": "1-Data_Literacy/7-Introducing_R_Practice.html#histogram",
    "title": "Introducing R",
    "section": "Histogram",
    "text": "Histogram\nCreate a histogram of happiness scores:\nQuestion: What is the general shape of the distribution?\nAnswer:"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html",
    "href": "1-Data_Literacy/5-Shape_and_Center.html",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#lesson-outcomes",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#lesson-outcomes",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Review of the Five Steps of the Statistical Process",
    "text": "Review of the Five Steps of the Statistical Process\nWe will use the five steps in the Statistical Process throughout the course. Recall the five steps (and the mnemonic “Daniel Can Discern More Truth) before you begin this lesson.\n\n\n\n\n\nStep 1:\n\n\n\n\nDaniel\n\n\n\n\nDesign the study\n\n\n\n\n\n\n\n\nStep 2:\n\n\n\n\nCan\n\n\n\n\nCollect data\n\n\n\n\n\n\nStep 3:\n\n\n\n\nDiscern\n\n\n\n\nDescribe the data\n\n\n\n\n\n\nStep 4:\n\n\n\n\nMore\n\n\n\n\nMake inferences\n\n\n\n\n\n\nStep 5:\n\n\n\n\nTruth\n\n\n\n\nTake action"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#shape-of-a-distribution",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#shape-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Shape of a Distribution",
    "text": "Shape of a Distribution\nCost to Treat Tuberculosis in India\n\n  Step 1: Design the study.\nTuberculosis (TB) is the deadliest bacterial disease in the world. In 2009, nine million new cases of tuberculosis were diagnosed, leading to almost 2 million deaths worldwide. Currently, the principal vaccine used to prevent tuberculosis is Bacille Calmette Guerin (BCG). Unfortunately, BCG is only moderately effective at preventing tuberculosis. Historically, India has had a high number of tuberculosis cases. The Indian Government wants to reduce the prevalence of this disease.\nIn this activity, you will compare the average costs of treating a person who contracts tuberculosis to the costs of preventing a case of tuberculosis in India.\n  Step 2: Collect data.\nHealth Care records of tuberculosis patients in India were surveyed to estimate the cost to treat patients with tuberculosis. The following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nThese costs include health care treatment, time missed from work, and in some cases utility lost due to death.\n  Step 3: Describe the data.\n\nVisualizing Quantitative Data: Histograms\nThe following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nTo help us visualize these data, we will create a graph called a histogram. To make a histogram, we will divide the number line from 0 to 35,000 in seven equal parts. We will then count the number of data points in each of these intervals:\n\n\n\n\n\n\nInterval\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\n\n\nAt least 0 and less than 5,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 5,000 and less than 10,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 10,000 and less than 15,000\n\n\n\n\n3\n\n\n\n\n\n\nAt least 15,000 and less than 20,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 20,000 and less than 25,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 25,000 and less than 30,000\n\n\n\n\n0\n\n\n\n\n\n\nAt least 30,000 and less than 35,000\n\n\n\n\n1\n\n\n\n\n\n\nFor each of these intervals, called bins, we draw a bar on the histogram. The width of the bars is determined by the width of the bin (5000 in this example). The height of the bars is equal to the number of observations that fall in each bin. As we look at the histogram shown below, we see bars ranging from $0 to $35,000. We also see higher bars in the middle between $10,000 to $20,000 show that these values are more commonly occurring than the other values. If we computed the average of the values contained in our histogram, we would compute the number \\[\n  \\frac{15,100 + 19,000 + 4,800 + 6,500 + 14,900 + 600 + 23,500 + 11,500 + 12,900 + 32,200}{10} = 14,100\n\\] showing that the center of the histogram (or average) is at $14,100.\n\nThis is a histogram created in R: \n\n\n\nTo create this histogram in R, you can copy and paste the following code into R:\n\n# Create a dataset with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\n# Create a histogram. We add x-axis labels using `xlab = \"\"` and a title `main = \"\"`\nhist(data, xlab=\"Treatment Costs\", main = \"Tuberculosis Costs in India\")\n\n\n\n\n\n\n\n\n\n\nMaking Inference About the Population\nAfter summarizing the data from our sample of the populations both numerically and graphically, we can use this information to make inference about the full population. \n  Step 4: Make inferences.\nIn the past, the total average cost to society to treat a case of tuberculosis in India was known to be $13,800. As shown in our Step 3 calculations, the 10 randomly selected patients showed an average cost that was higher than the historic value at $14,100. This might make us believe that the actual total average cost to society is also $14,100. However, in depth statistical calculations (that you will be taught how to do later this semestr) show that there is a 46% chance that our sample had an average of $14,100 just by random chance. This isn’t too hard to believe since we only had a sample size of 10 people, and $14,100 is only $300 above $13,800, so it turns out to be fairly likely (46% chance) that because of random chance our sample had an average that was a little higher than the actual value from the population. So we will conclude that the total average cost to society is still essentially the same as it has been in the past.\n\n  Step 5: Take action.\nAfter making inferences, you take action. The motivation for conducting a study like this is usually to see if there is inflation in the costs.\n\nAnswer the following question:\n\n\n\nGiven our conclusion in Step 4 (that the results of our random sample being at an average $14,100 had a 46% probability of just being caused by random chance) do you think the Government of India needs to take any special action to stop the increase in the cost to treat tuberculosis?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary. – However, we could not say that the true mean cost has really changed from $13,800. So, there is not enough evidence of inflation. There is no need for the Government of India to take action.\n\n\n\n\nOne benefit of using a histogram is that it allows you to visualize the distribution of the data. A histogram illustrates the overall shape of the distribution of the data. The height of the bars show how many observations fall in that range.\n\nAnswer the following question:\n\n\n\nWhich bin of the histogram of tuberculosis costs contained the most data points?\n\n\n\nShow/Hide Solution\n\n\nThe bin going from $10,000 to $15,000 contained 3 observations ($11,500, $12,900, and $14,900), which was the most of any of the bins in the histogram. This can be seen visually in the histogram by looking at the height of each bar and the starting and stopping points of the bar along the x-axis of the graph.\n\n\n\n\nWe will describe the shape of the distribution of a data set using the following basic categories: symmetric, bell-shaped, skewed right, and skewed left. Additionally, we can label the shape of a distribution as uniform, unimodal, bimodal, or multimodal.\nA distribution is symmetric if both the left and right side of the distribution appear to be roughly a mirror image of each other. A special symmetric distribution is a bell-shaped distribution. When data follow a bell-shaped distribution, the histogram looks like a bell. Bell-shaped distributions play an important role in Statistics and will play a role in most of the future lessons.\nA distribution is right-skewed if a histogram of the distribution shows a long right tail. This can occur if there are some very large outliers on the right-hand side of the distribution. A distribution is left-skewed if a histogram shows that it has a long tail to the left.\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\nIf a distribution has only one peak, it is said to be unimodal. The three distributions illustrated above are all unimodal distributions. Some people might argue that there are several peaks in the GPA data, so it should not be considered unimodal. Even though there are jagged bumps in the histogram, it is important to visualize the overall shape in the data. When interpreting a histogram, it can be helpful to blur your eyes and imagine the overall shape after smoothing out the bumps. If the overall trend indicates that there is more than one bump, then we do not consider the distribution to be unimodal. We will usually only work with unimodal data sets in this course.\nSome distributions have no distinct peak, others have more than one peak. When there is no distinct peak, and the histogram shows a relatively flat shape, we might say the data follow a uniform distribution. If there are two distinct peaks, a distribution is called bimodal. If there are more than two peaks, we refer to the distribution as multimodal."
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#center-of-a-distribution",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#center-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Center of a Distribution",
    "text": "Center of a Distribution\n\nStep 3: Describe the data.\nSometimes people talk about the “typical” BYU-Idaho student or the average waiting time for a bus. But what does it mean for something or someone to be “average?” How can we quantify what it means to be typical or average? In the example below, we will explore one way to define what “average” means.\nWhen we talk about the “typical” or “average” value, we are essentially describing the center of a population. If we want to estimate the “average” costs to treat a tuberculosis patient, there are several ways we can do it.\n\n\nMeasuring the Center of a Distribution\n\nMean\nThe sample mean or sample arithmetic mean is the most common tool to estimate the center of a distribution. It is referred to simply as the mean. It is computed by adding up the observed data and dividing by the number of observations in the data set.\nIn Statistics, important ideas are given a name. Very important ideas are given a symbol. The sample mean has both a name (mean) and a symbol (\\(\\bar x\\), called “x-bar”).\n\\[\n  \\bar{x} \\text{ is used to denote the sample mean}\n\\]\nYou may have heard people refer to the sample mean as the average. Technically, the word average refers to any number that is used to estimate the center of a distribution. The mean, median and mode are all examples of “averages.” To avoid confusion, it is best to use the words mean, median, and mode instead of the word average, so that it is clear which “average” your are referencing.\n\nAnswer the following question:\n\n\n\nPractice finding the mean, \\(\\bar x\\), for the tuberculosis treatment costs of the 10 patients in India by simplifying the following: \\[\\bar x=\\frac{15100 + 19000 + 4800 + 6500 + 14900 + 600 + 23500 + 11500 + 12900 + 32200}{10}=\\]\n\n\n\nShow/Hide Solution\n\n\nThe mean cost to treat the 10 TB patients in India is: \\(\\bar x = \\$14,100\\). To see how to calculate the mean in Excel, see the “Excel Instructions” below.\n\n\n\n\n\n\nMedian\nThe median is the middle value in a sorted data set. Half of the observations in the data set are below the median and half are above the median. To find the median, you:\n\nSort the values from smallest to largest\n\nDo one of the following:\n\nIf there are an odd number of values, the median is the middle value in the sorted list.\nIf there are an even number of values, the median is the mean of the two middle values in the sorted list.\n\n\n\n\nAnswer the following questions:\n\n\n\nPractice finding the median of the tuberculosis treatment costs for the 10 patients in India. First, sort the data from smallest to largest.\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\n\n\n\nSince there are an even number of observations (n=10), the median is computed as the mean of the middle two values. Use your answer to the previous question to find the median of the data. What is the median?\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\nThe middle two numbers are 12900 and 14900. The mean of these two numbers is:\n\n\\(\\text{Median } = \\frac{12900 + 14900}{2} = 13900\\)\n\nThe median cost to treat the ten TB patients in India is $13,900.\n\n\n\n\n\nMode\nThe most frequently occurring value is called the mode. Sometimes there is more than one mode. For example, in the data set\n\\[{1,~~2, ~~2, ~~2, ~~3, ~~4, ~~4, ~~5, ~~5, ~~5, ~~6}\\]\nthe modes are 2 and 5. Both of these values occur three times, which is more times than any other value.\nIf no number occurs more than once in the data set, we say that there is no mode. For the data set representing the costs to treat tuberculosis in India, none of the values is repeated. So, there is no mode for these data.\n\nAnswer the following question:\n\n\n\nFor a particular data set, which of the following can occur?\n\n\nThere may be no mode.\nThere may be exactly one mode.\nThere may be several modes.\nOnly A and B can occur.\nA, B, and C can all occur.\n\n\n\nShow/Hide Solution\n\n\nA, B, and C can all occur.\n\n\n\n\n\n\nExcel Instructions for Mean, Median, and Mode\n\nR Instructions\n\n\n\nTo calculate most numerical summaries (such as the mean, median, and mode) in Excel, follow these general steps:\n\nOpen R.\nEnter the data using the “assignment operator”, &lt;-, and c() which establishes a “collection” of things, in this case numbers.\n\nCalculate summary statisti\nThen, highlight the data (by clicking on it) to which you want to apply the function. The cell reference range will automatically be added to your formula. Then type a closed parenthesis, “)” and hit enter.\n\nCaculate a Mean\nFor example, to calculate the mean of the sample of tuberculosis patient costs in India:\n\n# Create a dataset called `data` with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\nmean(data)\n\n[1] 14100\n\n\n\nCalculate a Median\nSimply replace the word “mean” in the formula with the word “median”. Try it with the tuberculosis patient data, you should get the same value that was calculated by hand above.\n\nmedian(data)\n\n[1] 13900\n\n\n\nCalculate a Mode\nR makes it difficult to use the mode because there are fewer situations when it is useful for quantitative data where few values repeat. In the rare occasion we are not interested in a mean or median, we can tabulate the frequency of specific values using the table() function:\n\n# Create a new dataset called data2:\ndata2 &lt;- c(3,4,9,5,2,3,5,4,2,3,1,5,3,1,2,6,2,4,6,2,2,2,9,1,2,7,8)\n\n# The `table()` function counts up all the times specific values show up.  This works for numbers or categories:\ntable(data2)\n\ndata2\n1 2 3 4 5 6 7 8 9 \n3 8 4 3 3 2 1 1 2 \n\n\nThe first row of the table() output is the value being counted. The second row is the frequency of occurrence.\nWhich value is most frequently occurring?\nIf there are lots of occuring values, we can use R to sort() the table output to make it easier to see which is the mode:\n\nsort(table(data2))\n\ndata2\n7 8 6 9 1 4 5 3 2 \n1 1 2 2 3 3 3 4 8 \n\n\nThe rows are the same as before but are sorted in ascending order. We can now easily see that 2 occurred 8 times making 2 the mode.\n\n\n\nParameters and Statistics\nWe only have data on the cost to treat ten randomly selected tuberculosis patients. This represents a random sample from the population. The sample obtained by the researchers depends on random chance. If the study was repeated and a new sample of ten patients was randomly drawn from all cases of tuberculosis in India, would we observe the same data values? Certainly not!\nHowever, if we took a second random sample from the population, we would expect the mean of the new sample to be somewhat similar to the mean for our original sample. And if we took a third sample of data, we should expect the mean of this sample to be different than the means of the other two samples. In fact, every sample will give us a different sample mean, but all of these sample means will be fairly similar in value.\nOne of the primary purposes of collecting and analyzing data is to estimate the true mean of a population. Since collecting data on the entire population is usually not feasible, we usually never know what the true mean is. So we estimate the true population mean with the sample mean from a single sample of data from the population.\nThe sample mean is an example of a statistic. A statistic is a number that describes a sample. The true (usually unknown) population mean is an example of a parameter. A parameter is any number that describes a population.\nAn easy way to distinguish between a parameter and a statistic is to note the repetition in the first letters:\n\nPopulation Parameter True (usually unknown) value describing a population\nSample Statistic Estimate of the population parameter obtained from a sample\n\nIn the example above, the sample mean \\(\\bar x\\) = $14,100 is a statistic. Over the last few years, the total mean cost to treat tuberculosis in India has been $13,800. This $13,800 is considered a parameter because it is the “known” value for the full population.\nDifferent symbols are used to distinguish between the sample mean (a statistic) and the population mean (a parameter). The symbol for the sample mean is \\(\\bar x\\). The symbol for the population mean is \\(\\mu\\).\nPerspective\nThe mean cost to treat the ten tuberculosis patients in the sample was \\(\\bar x\\) = $14,100. This number gives us some useful information. However, if this was all we were given, we would not be able to distinguish the data above from a situation where the cost for each of the ten patients was exactly $14,100. Notice that if the cost for each patient was $14,100, the mean would be:\n\\[\\bar x=\\frac{14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100}{10} =14,100\\]\nEven though measures of center are important, we need to consider the shape, center and spread of a distribution of data. When evaluating data, it is sometimes tempting to compute a mean but to avoid creating a histogram. This can lead to errant decisions based on a misunderstanding or incorrect transcription of data. If there is a transcription error in the data, it is sometimes easiest to detect it as an outlier in a histogram."
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#summary",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#summary",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\n\nHistograms are created by dividing the number line into several equal parts, starting at or below the minimum value occurring in the data and ending at or above the maximum value in the data. The number of data points occurring in each interval (called a bin) are counted. A bar is then drawn for each bin so that the height of the bar shows the number of data points contained in that bin.\nA histogram allows us to visually interpret data to quickly recognize which values are most common and which values are least common in the data.\nHistograms can be left-skewed (the majority of the data is on the right of the histogram, less common values stretch to the left side), right-skewed (majority of the data is on the left side with less common values stretching to the right), or symmetrical and bell-shaped (most data is in the middle with less common values stretching out to either side).\nThe mean, median, and mode are measures of the center of a distribution. The mean is the most common measure of center and is computed by adding up the observed data and dividing by the number of observations in the data set. The median represents the 50th percentile in the data. The mean can be calculated in R using mean(...), the median by using median(...), and the mode by table(...) where the ... in each case consists of the data.\nWhen comparing the centers of distributions using graphical and numerical summaries, the direction of the skew showing in the histogram will generally correspond with the mean being pulled in that direction.\n\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\n\nIn a symmetrical and bell-shaped distribution of data, the mean, median, and mode are all roughly the same in value. However, in a skewed distribution, the mean is strongly influenced by outliers and tends to be pulled in the direction of the skew. In a left-skewed distribution, the mean will tend to be to the left of the median. In a right-skewed distribution, the mean will tend to be to the right of the median.\nA parameter is a true (but usually unknown) number that describes a population. A statistic is an estimate of a parameter obtained from a sample of the population."
  },
  {
    "objectID": "1-Data_Literacy/3-Statistical_Process_Practice.html",
    "href": "1-Data_Literacy/3-Statistical_Process_Practice.html",
    "title": "The Statistical Process-Practice",
    "section": "",
    "text": "The Forgetting Curve relates to how quickly we forget new information and suggests strategies for maximize retention. Studies have shown that retention of classroom materials can drop as low as 58% within 15 minutes of class and within 44% after the first hour after class if there is no review of the material. This forgetting curve continues, so that within six days only 25% is remembered.\nSuppose your teacher wanted to test strategies for retention and invited you to participate in a study. You would be randomly assigned to one of two groups.\nGroup 1 would be asked to review for ten minutes at three specific times: fifteen minutes after class, one hour after class, and two days after class.\nGroup 2 would not be asked to review the material at all. Use the above information for all parts.\nQuestion: Which treatment group would be considered the control and why?\nAnswer:\nQuestion: Before participating, your teacher gives you a short quiz and records your score out of 10. What type of data (quantitative or categorical) is being recorded?\nAnswer:"
  },
  {
    "objectID": "1-Data_Literacy/3-Statistical_Process_Practice.html#the-sitch",
    "href": "1-Data_Literacy/3-Statistical_Process_Practice.html#the-sitch",
    "title": "The Statistical Process-Practice",
    "section": "",
    "text": "The Forgetting Curve relates to how quickly we forget new information and suggests strategies for maximize retention. Studies have shown that retention of classroom materials can drop as low as 58% within 15 minutes of class and within 44% after the first hour after class if there is no review of the material. This forgetting curve continues, so that within six days only 25% is remembered.\nSuppose your teacher wanted to test strategies for retention and invited you to participate in a study. You would be randomly assigned to one of two groups.\nGroup 1 would be asked to review for ten minutes at three specific times: fifteen minutes after class, one hour after class, and two days after class.\nGroup 2 would not be asked to review the material at all. Use the above information for all parts.\nQuestion: Which treatment group would be considered the control and why?\nAnswer:\nQuestion: Before participating, your teacher gives you a short quiz and records your score out of 10. What type of data (quantitative or categorical) is being recorded?\nAnswer:"
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html",
    "title": "Summarizing Data Activity",
    "section": "",
    "text": "This example is taken from an experiment listed in the R help files under ?CO2.\n“An experiment on the cold tolerance of the grass species Echinochloa crus-galli was conducted. The CO2 uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient CO2 concentration. Half the plants of each type were chilled overnight before the experiment was conducted.” Plants were considered tolerant to the cold if they were still able to achieve high CO2 uptake values after being chilled.\nIgnoring the Plant ID for a moment, there are three factors that possibly effect the uptake of a plant. These include Type, Treatment, and conc. The factor Type has two levels, Quebec and Mississippi. The factor Treatment has two levels chilled and nonchilled and the factor conc has seven levels 95, 175, 250, 350, 500, 675, and 1000.\nConc is related to the ambient concentration of CO2."
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#univariate-analysis",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#univariate-analysis",
    "title": "Summarizing Data Activity",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nCreate a table with the overall summary statistics for uptake:\n\n# This is NEW!  if you tell R to knit the favstats output using the \"kable\" function, it will look a lot nicer when the document is rendered.\n\nknitr::kable(favstats(CO2$uptake))\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n7.7\n17.9\n28.3\n37.125\n45.5\n27.2131\n10.81441\n84\n0\n\n\n\n\n\nCreate a histogram of the CO2 uptake across all groups:"
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-plant-type-on-uptake",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-plant-type-on-uptake",
    "title": "Summarizing Data Activity",
    "section": "Impact of Plant Type on Uptake",
    "text": "Impact of Plant Type on Uptake\nCreate a table of summary statistics of uptake for each Type of plants used in the experiment.\nCreate a side-by-side boxplot of uptake for each Type of plants used in the experiment."
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-treatment-on-uptake",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-treatment-on-uptake",
    "title": "Summarizing Data Activity",
    "section": "Impact of Treatment on Uptake",
    "text": "Impact of Treatment on Uptake\nCreate a table of summary statistics of uptake for each Treatment used in the experiment.\nCreate a side-by-side boxplot of uptake for each Treatment used in the experiment."
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-ambient-co2-concentration-on-uptake",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#impact-of-ambient-co2-concentration-on-uptake",
    "title": "Summarizing Data Activity",
    "section": "Impact of Ambient CO2 Concentration on Uptake",
    "text": "Impact of Ambient CO2 Concentration on Uptake\nCreate a table of summary statistics of uptake for each level of ambient CO2, conc, found in the experiment.\nCreate a side-by-side boxplot of uptake for each level of ambient CO2, conc, found used in the experiment."
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#what-makes-this-an-experiment",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#what-makes-this-an-experiment",
    "title": "Summarizing Data Activity",
    "section": "What makes this an experiment?",
    "text": "What makes this an experiment?\nThe background suggests this study was an experiment. In your own words, describe the difference between an experiment and an observational study.\nWhat might this study have looked like if it had been an observational study?"
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#what-is-the-population-of-this-study",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#what-is-the-population-of-this-study",
    "title": "Summarizing Data Activity",
    "section": "What is the Population of this study?",
    "text": "What is the Population of this study?\nDescribe in your own words what the population of this study could be. How broad can we make our conclusions?"
  },
  {
    "objectID": "1-Data_Literacy/10-AA_Data_Summaries.html#what-are-your-recommendations",
    "href": "1-Data_Literacy/10-AA_Data_Summaries.html#what-are-your-recommendations",
    "title": "Summarizing Data Activity",
    "section": "What are your recommendations?",
    "text": "What are your recommendations?\nIf you were to recommend a planting scenario to maximize CO2 uptake, what would you recommend based on the summaries provided?"
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html",
    "href": "1-Data_Literacy/1-Course_Introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#lesson-outcomes",
    "href": "1-Data_Literacy/1-Course_Introduction.html#lesson-outcomes",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#welcome-to-the-course",
    "href": "1-Data_Literacy/1-Course_Introduction.html#welcome-to-the-course",
    "title": "Course Introduction",
    "section": "Welcome to the Course!",
    "text": "Welcome to the Course!\nIn this course, you will explore important connections between the academic discipline of Statistics and the world around us. By pondering these ideas, your understanding of statistics will increase, as will your knowledge and testimony of the restored Gospel of Jesus Christ.\nThis course has been designed to help you slowly build up a knowledge base of ideas and skills. Not all of these ideas and skills will come easily. It takes a lot of work and practice before some things will even start to make sense, so you should not be surprised to find that it may take you a little time to comprehend these ideas. Just be patient. Once you’re far enough into the course, the ideas will start to come together, and you will see how much progress you have really made. You will understand what this course is all about, and you will be glad you persisted in your efforts to learn."
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#course-description",
    "href": "1-Data_Literacy/1-Course_Introduction.html#course-description",
    "title": "Course Introduction",
    "section": "Course Description",
    "text": "Course Description\nThis course covers the following topics as they are applied to Statistics: graphical representations of data, measures of center and spread; elementary probability; sampling distributions; correlation and regression; statistical inference involving means, proportions, and contingency tables.\n\n\nCourse Learning Outcomes\nIn this course, we will:\n\nSummarize data numerically and graphically using spreadsheets\nMake decisions regarding situations with inherent randomness\nApply probability distributions to investigate questions\nEmploy confidence intervals in various situations\nImplement tests of diverse hypotheses\nCommunicate the results of statistical analyses to relevant audiences\n\n\n\n\nHow the Outcomes will Be Assessed\nWhile you may not be tested on everything you learn in this course, the instructor will be assessing your mastery of the Course Learning Outcomes. You may also be asked to create reports with quality visualizations and analyze new datasets. At times, the instructor may assess your performance of a skill, or the instructor may assess products you create using particular skills. In addition, the instructor may engage in personal communication with you to determine how well you understand the course content.\n\n\n\nKeys to Success\n\nFive Principles of the Learning Model\nYou will experience much deeper learning if you follow the Five Principles of the BYU-Idaho Learning Model\n\nExercise Faith: Exercise faith in the Lord Jesus Christ as a principle of action and power.\nLearn by the Holy Ghost: Understand that true teaching is done by and with the Holy Ghost.\nLay Hold on the Word of God: Lay hold of the word of God.\nAct for Themselves: Act for yourself and accept responsibility for learning and teaching.\nLove, Serve, and Teach One Another: Love, serve, and teach other students in your classes.\n\n\n\nThree Process Steps of the Learning Model\nYou will learn more in less time if you follow the Three Process Steps of the BYU-Idaho Learning Model\n\nPrepare: This involves (a) spiritual preparation, (b) individual preparation, and (c) group preparation.\nTeach One Another: You should (a) be on time, (b) pray together, and (c) actively engage with other students.\nPonder/Prove: You should (a) ponder what you have learned, (b) record your learning, and (c) pursue unanswered questions and discuss what you learn with others.\n\nIf you feel confused or have questions about anything in the lesson, take immediate action (Exercise Faith; Act for Themselves) and talk with your classmates, the teaching assistant, or the instructor (Love, Serve, and Teach One Another).\nTeach One Another\nAt BYU-Idaho, an “A” student will demonstrate “diligent application of Learning Model principles, including initiative in serving other students” (BYU-Idaho Catalog). In this class, you will have the opportunity to work with other students.\nDoctrine and Covenants 84:106 states, “And if any man among you be strong in the Spirit, let him take with him him that is weak, that he may be edified in all meekness, that he may become strong also.” In the spirit of this revelation, you will have the opportunity to help others in the class when you have developed an understanding of a principle. Likewise, you will be able to receive help from others (peers, tutors, TA, and your instructor) when you are still working to understand concepts.\nIn a spirit of love and service, please reach out to others. You are not graded on a curve. If someone else does well, it does not adversely affect you. Research has shown that students who help other students to understand the material gain a much deeper grasp on the concepts of the course. Please take opportunities to help your peers succeed."
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#course-structure",
    "href": "1-Data_Literacy/1-Course_Introduction.html#course-structure",
    "title": "Course Introduction",
    "section": "Course Structure",
    "text": "Course Structure\nThis course consists of lessons presented in a topical order in which concepts and skills learned in the earlier lessons provide the requisite knowledge to succeed in later lessons. If the general order of the lessons doesn’t make sense at first, don’t worry. It will all come together in the end, and you’ll see the reasoning behind why the lessons have been presented in this particular order.\nYour main goal as a student will be to complete all of the learning activities within each lesson by their due dates every week. These activities follow a consistent weekly schedule, and it will be up to you to make sure that you keep on pace with all your assignments. These weekly activities may include the following:\n\nReading assigned texts or viewing presentations\nCompleting Class Notes\nParticipating in group discussions and assignments with other class members\nWriting papers and/or developing presentations\nParticipating in meetings with the instructor, teaching assistants, and other students\n\nA typical lesson will contain a preparation reading, class notes and practice applications. Because not all material is equally challenging, some lessons will span multiple days. But expect to cover 2-3 lessons a week.\nWhile a general flow of reading, classwork and practice will be the typical lesson flow, there may be adjustments to the schedule from time to time. Changes are typically due to adjusting the pace of the material to support student learning. Please be flexible as we adapt the pace to suit the needs of the class.\nYou should create a study schedule that will keep you on pace throughout the semester. This is a rigorous course with a lot of subject matter to cover, and it can be extremely difficult to recover if you fall too far behind in your work. So, please make every effort to study on a regular basis and get your work turned in on time."
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#course-materials",
    "href": "1-Data_Literacy/1-Course_Introduction.html#course-materials",
    "title": "Course Introduction",
    "section": "Course Materials",
    "text": "Course Materials\nThis course has been designed with the student in mind. Every effort has been made to provide a high quality experience at the lowest possible cost.\nTextbook\nTo keep costs as low as possible for students and their families, no physical textbook is required for this class. The readings for this course are provided on this website and will continue to be available to you after the course is completed. Please report any problems with the textbook (links not working, loading slowly, inability to view images, etc.) to your instructor.\nYou may also download all coursework at the main page. This includes all class notes, practice problems and application activities.\nComputer Equipment\nYou will need a laptop with the ability to install software locally.\nNOTE: It is possible to complete this course with a Chromebook or other cloud-based device, but it adds significant complications and stress. For those who are new to programming and statistics, this can be the difference between success and failure in the class."
  },
  {
    "objectID": "1-Data_Literacy/1-Course_Introduction.html#course-resources",
    "href": "1-Data_Literacy/1-Course_Introduction.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nPeer Support\nYour experience in this course will be enhanced as you work with other students to learn and grow together.\nHelp Desk\nThe BYU-Idaho Help Desk has been established to help students with technological problems related to approved course software. You can access the Help Desk at any time in three ways: - Walk-in: The Help Desk is located in room 322 of the McKay Library - Call in: 208-496-1411 (toll free) - Email: helpdesk@byui.edu Additional information is available at the Help Desk web page: http://www.byui.edu/helpdesk/\nWhen you have technical problems with I-Learn, you should first try contacting the Help Desk before you contact your instructor. They are connected with the IT support staff who can resolve problems with I-Learn. Please take a moment now to look at the Help Desk web page. That way, if a problem does arise later on in the course, you will know where to go for help.\nTutoring Center\nThe BYU-Idaho Study Skills/Tutoring Center is a powerful resource for students who would like a little extra help with a course. The Tutoring Center is located in the McKay Library in room 272. This is in the east wing of the second floor.\nThe Tutoring Center provides many services to help students succeed: - Individual tutors - Walk-in tutoring in the Math Study Center (McKay 266 & 270) - Virtual tutoring\nPlease take 5 minutes to explore the Study Skills/Tutoring Center web site.\nFaculty Support\nYour instructor is committed to your success. If you have any needs or concerns, please contact your instructor for help. If you feel yourself getting behind or struggling, talk to your teacher right away. If caught in time, a small problem can be addressed quickly before it grows."
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html",
    "href": "1-Data_Literacy/2-Stat_Process.html",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#lesson-outcomes",
    "href": "1-Data_Literacy/2-Stat_Process.html#lesson-outcomes",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#introduction",
    "href": "1-Data_Literacy/2-Stat_Process.html#introduction",
    "title": "The Statistical Process",
    "section": "Introduction",
    "text": "Introduction\nStatistics are used in every aspect of society. Every statistical analysis follows a pattern we will call the Statistical Process. This process will be introduced in this lesson and will be used throughout the course.\n\n\nThe Statistical Process and Daniel’s Experiment\n\n\n\nStained-glass depiction of Daniel’s deliverance from the lions’ den. Found in the old Dominican priory church at Hawkesyard in Staffordshire, England. (Photo credit: Fr Lawrence Lew, O.P. Used by permission.)\n\n\n\nThe Old Testament prophet Daniel planned one of the earliest recorded scientific research studies. We will use his example to illustrate the following five steps of The Statistical Process.\nThe following icons can help you remember these steps. Notice that each icon has a letter and an image to help you remember the five steps of the Statistical Process.\n\n\n\n\n\n\n\n\n\n\n\nThe Statistical Process\n\n\n\n\n\n\nDesign the Study\n\n\n\nCollect the Data\n\n\n\nDescribe the Data\n\n\n\nMake Inference\n\n\n\nTake Action\n\n\n\n\n\n\n\nStep 1: Design the Study\n\nAn important step in scientific inquiry or problem solving can be to state a research question such as:\n\nWill internet advertising increase a company’s revenue?\nDoes expressing gratitude increase a person’s satisfaction with life in general?\nDoes a newly developed vaccine prevent the spread of disease?\n\nResearchers also investigate the background of the situation. What have other people discovered about this situation? How can we find the answer to the research question? What do we need to do? What is the population (or total collection of all individuals) under consideration? What kind of data need to be collected?\nBefore collecting data, researchers make a hypothesis, or an educated guess about the outcome of their research. A hypothesis is a statement such as the following:\n\nUsing internet advertising will increase the company’s sales revenue.\nPeople who express gratitude will be more satisfied with life than those who do not.\nA newly-developed vaccine is effective at preventing tuberculosis.\n\n\n\nDaniel’s Experiment\nAfter taking Israel captive, Babylon’s King Nebuchadnezzar asked his chief officer to bring Israelite children who were well favoured, and skillful in all wisdom, and cunning in knowledge, and understanding science to stand in the king’s palaces (Daniel 1:4). To aid their preparation, Nebuchadnezzar planned to feed them his meat and wine for three years (Daniel 1:5).\nDaniel did not want to defile himself by partaking of the king’s meat and wine. He asked permission to eat pulse[^1] and drink water instead. His supervisor, Melzar, was afraid to displease the king. He thought that after eating pulse and water, the selected Israelites would look worse than their peers, and he would be punished (Daniel 1:8-10).\nWith an understanding of the background of the situation, Daniel proposed an experiment. He said, Prove thy servants, I beseech thee, ten days; and let them give us pulse to eat, and water to drink. Then let our countenances be looked upon before thee, and the countenance of the children that eat of the portion of the king’s meat: and as thou seest, deal with thy servants (Daniel 1:12-13.). In short, Daniel’s implied research question can be stated as: Will those who eat pulse and drink water appear healthier than those who eat the king’s meat and drink his wine? Melzar agreed to the experiment.\n\nAnswer the following question:\n\n\n\nWhat is Daniel’s hypothesis?\n\n\n\nSolution\n\nDaniel’s hypothesis is that the Israelite children who eat pulse and drink water will appear healthier in just ten days, compared to those who eat the king’s meat and drink his wine.\n\n\n\n\n\n\nStep 2: Collect Data\n\nWhen designing a study, much attention is given to the process by which data are observed. When examining data, it is also important to understand the data collection procedures. A sample is a subset (a portion) of a population. How is this sample obtained? How are the observations made?\nDaniel’s study design required that data be collected at the end of 10 days. Melzar would compare the appearances of two groups of people: (1) Israelites who ate pulse and drank water versus (2) Israelites who ate the king’s meat and drank his wine.\n\n\n\nStep 3: Describe the Data\n\nWhen we describe data, we use any tools appropriate to the situation. This can include creating graphs or calculating summary statistics to help understand or visualize the data.\nFor Daniel’s experiment, the data are described in Daniel 1:15: And at the end of ten days [the] countenances [of those who ate pulse] appeared fairer and fatter in flesh than all the children which did eat the portion of the king’s meat.\n\n\n\nStep 4: Make Inferences\n\nInference is the process of using the information contained in a sample from a population to make a general statement (i.e. to infer something) about the entire population. Later in the course we will learn techniques that make this type of analysis possible.\nMelzar made an inference. Based on the results of the sample, he determined that (in general) those who eat pulse and drink water will be healthier than those who eat the king’s meat and drink his wine Daniel 1:15-16.\n\n\n\nStep 5: Take Action\n\nThe goal of a statistical analysis is to determine which action to take in a particular situation. Actions can include many things: launching an internet ad campaign (or not), expressing gratitude (or not), getting vaccinated (or not), etc.\nMelzar took action as described in Daniel 1:16: Thus Melzar took away the portion of their meat, and the wine that they should drink; and gave [all the Israelite children] pulse.\nWas the experiment a success?\n“Now at the end of the days that the king had said he should bring them in… the king communed with them; and among them all was found none like Daniel, Hananiah, Mishael, and Azariah And in all matters of wisdom and understanding, that the king enquired of them, he found them ten times better than all the magicians and astrologers that were in all his realm” Daniel 1:18-20.\n\n\n\nSummary of the Statistical Process\n\nDaniel’s experience can also help you learn the Statistical Process. Look at the first letter of each of the steps in the Statistical Process. You can use the phrase “Daniel Can Discern More Truth” to help you to help you remember the five steps in the Statistical Process.\n\nThe Statistical Process\n\n\n\n\n \nPneumonic\nActual Process Step\n\n\n\n\nStep 1:\nDaniel\nDesign the study\n\n\nStep 2:\nCan\nCollect data\n\n\nStep 3:\nDiscern\nDescribe the data\n\n\nStep 4:\nMore\nMake inferences\n\n\nStep 5:\nTruth\nTake action\n\n\n\n\nThe Statistical Process will be used throughout the course. Take time to memorize the five steps.\n\n\nThe study designed by the Old Testament prophet Daniel provides an ancient example of a designed experiment. Daniel’s experiment included two groups of people: those who had the experimental treatment eating pulse and drinking water (called the treatment group) and those who ate the standard food the king’s meat (called the control group.) The treatment group receives the experimental procedure. The control group is used for comparison.\n\nAnswer the following question:\n\n\n\nWhy was it important that Daniel’s experiment included a control group?\n\n\n\nSolution\n\nIf there was no control group, then there would be no way to compare the effect of the diets (the treatments). Having a control group allows a researcher to see the effect of not taking any action. For Daniel, the control group (who ate the king’s meat and drank his wine) provided a basis for comparing the effect of the new treatment (i.e. eating pulse and drinking water.)"
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#design-of-studies",
    "href": "1-Data_Literacy/2-Stat_Process.html#design-of-studies",
    "title": "The Statistical Process",
    "section": "Design of Studies",
    "text": "Design of Studies\nMost research projects can be classified into one of two basic categories: observational studies or designed experiments. In an experiment, researchers control (to some extent) the conditions under which measurements are made. In an observational study, researchers simply observe what happens, without controlling the conditions under which measurements are made. Both types of study follow the five steps of the Statistical Process.\n\n\nDesigned Experiments\nIn a designed experiment, researchers manipulate the conditions that the participants experience. They often do this by randomly assigning subjects to one of two groups, a “treatment” group (sometimes called the experimental group) and a “control” group (though this could be second treatment group instead of a control group). The experiment is typically conducted by applying some kind of treatment to the subjects in the treatment group and observing the effect of the treatment. Those in the control group do not receive the treatment and are also observed. In this way researchers can determine the effects of the treatment by comparing the treatment group results to the control group results. The following example illustrates the use of these two groups.\nJonas Salk’s First Polio Vaccine Trial\nBeginning around 1916 and through the 1950s, a mysterious plague attacked infants and children. Symptoms included excruciating muscle pain and a stiff neck. This illness, which became known as poliomyelitis or simply “polio,” left children disfigured, paralyzed, and sometimes even dead.\nWhile working as a researcher at the University of Pittsburgh School of Medicine, Dr. Jonas E. Salk developed a vaccine that might help prevent the spread of this disease. He conducted what has become one of the most famous designed experiments in history.\nThis short video below provides a compelling summary of the famous Jonas Salk vaccine experiment. As you watch, notice each of the 5 steps of a statistical study in this study.  \nAs explained in the video, in the first Salk trial almost 1.1 million children participated in the study. Even though the sample size was large, flaws in the study design rendered the results useless.\nUndaunted, Dr. Salk fixed the problems with the design and enrolled hundreds of thousands of additional children for the second phase of his study. In all, over 1.8 million infants and children participated in this experiment, making it the largest drug trial to date.\n\nStep 1: Design the study.\nThe participants in a study are commonly called subjects. Sometimes subjects are called experimental units or simply units. In the Salk trials, the children who participated were the subjects.\nSubjects (the children) were randomly assigned to one of two groups. The first group was given the experimental vaccine, the treatment. The treatment is the new or experimental condition that is imposed on the subjects. The subjects who receive the treatment make up the treatment group.\nThe second group was given a control or placebo. In this study, the control was an injection that looked just like the vaccine, but contained a harmless saline solution. The control group or placebo group is made up of the subjects assigned to receive the control.\nThis study was double blind. Neither the children’s parents nor their doctors knew whether a particular child received the treatment or the control. Both parties were blinded to this information.\nBecause the children were assigned to the groups randomly, the two groups should be similar. If the vaccine is not effective, the number of future cases of polio should be about the same in each group. However, if Salk’s vaccine helped to prevent the spread of polio, then fewer cases should occur in the vaccinated group.\n\nAnswer the following questions:\n\n\n\nSome children can be identified as having a higher risk of developing polio. Would it have been better if they were assigned to the treatment group so they could get the vaccine?\n\n\n\nSolution\n\n\nNo. The two groups need to be as similar as possible. Specifically, the people in the treatment group need to have the same potential (on average) of contracting polio as the people in the control group. If we put the people who are at a higher risk of developing polio in the treatment group, we run the risk of having more people in the treatment group getting polio simply because they are more likely to get it, whether they are vaccinated or not. Likewise, we might have fewer people in the control group getting polio just because they are less likely to get it, whether they are vaccinated or not.\nThese two effects would create a bias against the vaccine, by making the vaccine look like it doesn’t work, or doesn’t work as well as it does. It might also make it appear that people who aren’t vaccinated stay healthy and the vaccine is not needed. There is even a chance that people will conclude that the vaccine actually gives people polio.\nRandomly assigning subjects to the two groups tends to yield groups with similar characteristics—in this example, similar potential for contracting polio. Randomly assigning subjects to groups therefore defends us against problems like those mentioned in the previous paragraph.\n\n\n\nWhy is it important for the subject and those who assess the health of the subject to be unaware of whether or not that child received the vaccine?\n\n\n\nSolution\n\n\nSubjects: Suppose a subject in the study thinks they’re being treated. It has been documented that subjects with such knowledge tend to show improvement whether they are receiving the treatment or not. To see why, consider how you might feel and act if you were told you had been vaccinated. You might have a more hopeful outlook, leading to healthier living habits such as better hygiene and nutrition. Such changes would tend to reduce your chance of contracting polio whether you’ve received the vaccine or not. This might make the vaccine look like it works better than it does. It also might make the vaccine look like it works, even if it doesn’t.\nNow suppose subjects in the control group know they are not being treated. This can also change the way they feel and act, in ways that can make them more likely to contract polio than they would be if they weren’t in the study. This could make it look like the incidence of polio among unvaccinated persons is higher than it is, again making the vaccine look like it works better than it does.\nTo reduce bias caused by such errors, subjects should not know to which group they are assigned.\nResearchers: Suppose a researcher assessing the health of a subject is told that the subject is in the control group. It has been documented that in such a case, the researcher is more likely to record that the subject has symptoms even if the subject is not actually in the control group. This makes it look like unvaccinated persons are more likely to get polio than they really are, which makes it look like the vaccine works better than it does.\nThere are other effects of knowing to which group the subject belongs, such as doctors treating or advising the patient differently than they would without such knowledge. Such differences can make it harder to tell whether the vaccine works, and how well.\nTo reduce bias caused by such effects, those assessing the health of the subjects should not be told to which group the subject belongs.\n\n\n\n\nStep 2: Collect data.\nThe researchers followed up with each child to determine if they contracted polio. They recorded the number of children in each group that developed polio during the study period. Not all of Salk’s experiments were double-blind. Here is a summary of the results from the regions where a double-blind study was conducted (Francis et al., 1955; Brownlee, 1955):\n\n\nChildren Who Developed Polio\n\n\n\n\n\n\n\nYes\n\n\n\n\nNo\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\nTreatment Group\n\n\n\n\n57\n\n\n\n\n200,688\n\n\n\n\n200,745\n\n\n\n\n\n\nPlacebo Group\n\n\n\n\n142\n\n\n\n\n201,087\n\n\n\n\n201,229\n\n\n\n\n\n\nStep 3: Describe the data.\nOne way to summarize the data is to compute the proportion of children in each group that developed polio. The proportion of children in the treatment group that developed polio during the study period is:\n\\[ \\frac{57}{200745} = 0.000~283~9 \\]\n\nAnswer the following questions:\n\n\n\nCalculate the proportion of children in the placebo group that developed polio during the study period.\n\n\n\nSolution\n\n\n\\[ \\displaystyle{\\frac{142}{201229} = 0.000~705~7} \\]\n\n\n\nCompare the two proportions. What do you observe?\n\n\n\nSolution\n\n\nThe proportion of children in the placebo group that develop polio during the study period was more than double the proportion of children in the treatment group that developed polio during the study period. That suggests that the treatment is effective in reducing the proportion of children that will develop polio.\n\n\n\n\nStep 4: Make inferences\nCareful statistical analysis of the records suggested that this difference was so great that it was attributable to the vaccine and not to chance. Assuming that the vaccine had no effect, the probability that the difference in the proportions between the two groups would be at least as extreme as the difference Dr. Salk observed was very low: 0.00000000093. Because this probability is so small, it is highly unlikely that these results are due to chance.\n\n\nStep 5: Take action\nOnce it was clear that the vaccine was effective, children who were unvaccinated or had received the placebo were given Salk’s vaccine. Since 1954, there has been a marked decrease in the number of polio cases worldwide (Offit, 2005). Public health researchers are striving to eradicate this disease entirely.\n\n\n\nObservational Studies\nIn an observational study researchers observe the responses of the individuals, without controlling the conditions experienced by the individuals. Therefore, they do not assign the participants to treatment or control groups.\nObservational studies commonly occur in business settings. One example is a financial audit. The purpose of a financial audit is to assess the accuracy of a company’s financial business practices. ImmunAvance Ltd., a non-government health care organization, hired the Accounting Office at Global Optimization Unlimited to perform an independent audit of their financial practices. ImmunAvance provides inoculation and other preventative health care services in rural African communities.\n\n\n\nStep 1: Design the study\nThe volume of financial transactions conducted by ImmunAvance makes it impossible to conduct a census or an examination of the entire collection of ImmunAvance’s financial documents. Instead, you will collect a manageable group of items (called the sample) from the entire collection of financial documents (called the population.) A sample is a subset or a portion of a population. The information gained from the sample is used to make an inference (or generalization) about the population.\nAuditors typically cannot consider every item in a population, because there are too many. When it is not possible to conduct a census, auditors face sampling risk. Sampling risk is the risk affiliated with not auditing every item in the population. It is the risk that the sample may not adequately reflect the population. The only way to eliminate sampling risk is to conduct a census, which is usually not practical. Auditors can reduce sampling risk by obtaining a sample randomly. This is called random selection. Another way to reduce sampling risk is to increase the sample size, the number of items sampled.\n\n\n\n\nSampling Methods\nStep 2: Collect data\nThere are several procedures that can be used to select a random sample from a population, including: simple random sampling (SRS), stratified sampling, systematic sampling, cluster sampling, , and convenience sampling (or, haphazard sampling). These are examples of sampling methods.\n\nRandom Sampling Methods\nA simple random sample (SRS) is the best method for obtaining a sample from a population. This method allows each possible sample of a certain size an equal chance at being selected as the chosen sample. A difficulty of this method is that a list of all of the items in the population must be accessible before the sample is taken. Often, we obtain a SRS by allowing a computer to randomly select a certain number of items from the full list of the population. It is akin to the idea of putting all of the names into a hat, shaking them up, and randomly drawing out a few.\n\nFor example, suppose there are 18,000 students in the population of a certain university. School officials can use a computer to randomly choose values between 1 and 18,000 to identify which students are to be selected to complete a survey. In Excel, the command to obtain a random number between 1 and 18,000 is sample(1:18000, 1). A simple random sample can be obtained any time there is a complete list of the items to be sampled and they are all accessible. All the statistical procedures in this course assume that simple random sampling has been used. But in practice, the SRS is often difficult (or impossible) to implement.\n\nA stratified sample is when the items to be sampled are organized in groups of homogeneous (similar) items called strata, then a simple random sample is drawn from each of these strata. Stratified sampling works well when the items are similar within each stratum and tend to differ from one stratum to another. We often use stratified sampling in order to obtain a sample in such a way that we can make comparisons between each of the groups (or strata).\n\nFor example, in obtaining a sample of students from a university, school officials could define the strata as: (1) freshman, (2) sophomores, (3) juniors, and (4) seniors. A simple random sample could then be obtained from each of these strata. This would ensure that each class rank of students was represented in the sample. It would also allow the school officials to see how freshman, sophomore, junior, and senior level students compared in their answers to a survey.\n\nA systematic sample is where every \\(k^{\\text{th}}\\) item in the population is selected to be part of the sample, beginning at a random starting point. Systematic sampling works well when the items are in a random, but sequential ordering. If the items are not arranged randomly, a systematic sample can miss important parts of the population.\n\nFor example, consider a fast food company where every 10th customer is given the opportunity to compete a satisfaction survey in exchange for a small discount coupon towards their next purchase. An airport security line also often implements a procedure where every 100th (or so) person is selected for a more “in depth” security examination. Similarly, factories that use assembly lines will pull say every 500th item from the assembly line to perform a quality control check on the item.\n\nA cluster sample (sometimes called a block sample) consists of taking all items in one or more randomly selected clusters, or blocks. When the variation from one block to another is relatively low, compared to the variation within the block, cluster sampling is a reasonable way to get a sample.\n\nFor example, ecologists could draw grids on a map of a forest to create small sampling regions, or sampling clusters. Then, by randomly selecting one or two of these clusters from the map, the ecologists could go to the areas marked on the map and document information on the health of every tree they find in those clusters. This is a practical way to get a sample in this case because the ecologists only have to go to a few areas of the forest, but are still able to obtain a random sample of all of the trees in the forest. It is also worth noting that the ecologists would not be interesting in comparing the health of the trees from the selected clusters to each other like they would in a stratified sample. Instead, they are just looking for a feasible way to obtain a single random sample of all of the trees in the forest, but want to keep their traveling time to a minimum while collecting their sample. In contrast, to obtain a simple random sample of trees from the same forest, the ecologists would first have to go out and number every tree in the entire forest. Then they would need to use a computer to randomly pick which trees to collect data on. Finally, they would then have to go back to the forest and collect data on the selected trees from across the entire forest. Such an approach just isn’t feasible in practice, so we are willing to settle instead for the cluster sample.\n\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\nAs an example of a convenience sample, an auditor could haphazardly select items from a filing cabinet. This is frequently done when a quick and simple sample is needed, but may not yield a sample that represents the population well. When possible, convenience samples should be avoided.\n\n\n\n\n\nTypes of Data\nWhenever we collect data, we record information about the things we are studying. There are two basic types of data that can be recorded: quantitative measurements and categorical labels. We will call these types of data simply “quantitative” or “categorical” variables. We use the word “variable” to denote the idea that the quantitative measurements or categorical labels can vary from person to person, or item to item, in our study.\nQuantitative variables provide measurement information on each individual (or item) in our study. They represent things that are numeric in nature; things that are measured. They often include units of measurement along with the quantitative value of the measurement. For example, the heights of children measured in inches (or centimeters), or their weight measured in pounds (or kilograms). For a quantitative variable, it makes sense to apply arithmetic operations to the data (such as adding values together, computing the average of the values, or comparing two values). If one child weighs 30 pounds (13.61 kg) and a second child weights 60 pounds (27.22) then the second child is twice as heavy as the first.\nCategorical variables allow us to place each individual (or item) into to a specific category. Categorical variables are labels, and it does not make sense to do arithmetic with them. For example the gender of a newborn child, the ethnicity of an individual, a person’s job title, the brand of phone they own, or the area code of a telephone number, etc are all categorical variables. Notice that although a telephone number consists of numbers, it is not a quantitative measurement. It does not make sense to double someone’s phone number, to average phone numbers together, or to say one phone number is half the size of another. But the area code of the phone number gives information about the region where the phone number was first initiated, which is categorical information.\nIn Unit 3 of this course we will learn more about categorical variables and proportions. Units 1 and 2 of this course focus on studying quantitative variables.\nReturning to the sample accounts receivable record, we find this data to have information on both types of variables.\n\nAnswer the following question:\n\n\n\nFor each of the following variables taken from this accounts receivable record, indicate whether the variable is quantitative or categorical.\n\n\nTerms\n\n\n\nSolution\n\n\nThe variable “Terms” is categorical. It classifies the invoice by the terms of payment for that invoice.\n\n\n\nAccount number\n\n\n\nSolution\n\n\nThe variable “Account number” is categorical. Even though the account number is given a number, it is actually functioning as a label. It is not something that is counted or measured. It does not make sense to do arithmetic operations (like adding 1 or multiplying by 2) to the account number.\n\n\n\nInvoice amount\n\n\n\nSolution\n\n\nThe variable “Invoice amount” is quantitative. It makes sense to do arithmetic operations to this value. For example, the amount of Invoice 5745 (which is $990.00) is somewhat more than twice as much as that of Invoice 2378 (which is $478.00).\n\n\n \n\n\nStep 3: Describe the data\nAfter auditors collect a sample and compile the data, they review the evidence. Auditors may use graphs or compute numbers (such as the average) to summarize the evidence they found."
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#making-inferences-hypothesis-testing",
    "href": "1-Data_Literacy/2-Stat_Process.html#making-inferences-hypothesis-testing",
    "title": "The Statistical Process",
    "section": "Making Inferences: Hypothesis Testing",
    "text": "Making Inferences: Hypothesis Testing\nStep 4: Make inferences\nAuditors use the information drawn from the sample to form an opinion about the population. Whenever sample data is used to infer a characteristic of a population, it is called making an inference. Inferential statistics represents a collection of methods that can be used to make inference about a population. Based on the documents reviewed, the auditors assess if the company is conducting its business in a proper manner.\nWhen conducting an audit, the implicit assumption is that transactions have been posted properly. As auditors sample the company’s records, they are looking to see if everything is consistent with the original assumption that all transactions have been posted properly. It would only be in the case of discovering suspicious activity or evidence of fraudulent reporting that the auditors would change their belief about the company and accuse the company ImmunAvance of falsely reporting on their financial statements.\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nThere is a formal procedure for determining when enough evidence has been found to make accusations of fraud. Later this semester, after we establish some foundational principles of statistics, we will study these statistical methods in depth. Of course, these methods can be used for much more than just determining if a company has reported their financial statements fraudulently. So we will look at many different ways these statistical procedures can be applied to research and industry.\nFor ImmunAvance’s audit, based on the samples of financial statements that had been selected, while there were a few errors in the documents, there was not evidence dramatic enough to claim that the company had been fraudulent. So the company passed their audit.\n\nStep 5: Take Action\nThe auditors prepare a report in which they give their opinion on the status of the company’s current operations.\nSince there was not enough evidence to suggest that ImmunAvance’s financial statements were fraudulent, the auditor’s conclusion is that no adjustment is necessary. The few observed discrepancies were apparently just the result of random chance errors, not the deliberate falsefying of information."
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#summary",
    "href": "1-Data_Literacy/2-Stat_Process.html#summary",
    "title": "The Statistical Process",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe Statistical Process has five steps: Design the study, Collect the data, Describe the data, Make inference, Take action. These can be remembered by the pneumonic “Daniel Can Discern More Truth.”\nIn a designed experiment, researchers control the conditions of the study, typically with a treatment group and a control group, and then observe how the treatments impact the subjects. In a purely observational study, researchers don’t control the conditions but only observe what happens.\nThe population is the entire group of all possible subjects that could be included in the study. The sample is the subset of the population that is actually selected to participate in the study. Statistics use information from the sample to make claims about what is true about the entire population.\nThere are many sampling methods used to obtain a sample from a population. The best methods use some sort of randomness (like pulling names out of a hat, rolling dice, flipping coins, or using a computer generated list of random numbers) to avoid bias.\n\n\nA simple random sample (SRS) is a random sample taken from the full list of the population. This is the least biased (best) sampling method, but can only be implemented when a full list of the population is accessible.\nA stratified sample divides the population into similar groups and then takes an SRS from each group. The main reason to use this sampling method is when a study wants to compare and contrast certain groups within the population, say to compare freshman, sophomores, juniors, and seniors at a university.\nA systematic sample samples every kth item in the population, beginning at a random starting point. This is best applied when subjects are lined up in some way, like at a fast food restaurant, an airport security line, or an assembly line in a factory.\nA cluster sample consists of taking all items in one or more randomly selected clusters, or blocks. For example, ecologists could draw grids on a map of a forest to create small sampling regions and then sample all trees they find in a few randomly selected regions. Note that this differs from a stratified sample in that only a few sub-groups (clusters) are selected and that all subjects within the selected clusters are included in the study.\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\n\nThe best way to avoid bias when trying to make conclusions about a population from a single sample of that population is to use a random sampling method to obtain the sample.\nQuantitative variables represent things that are numeric in nature, such as the value of a car or the number of students in a classroom. Categorical variables represent non-numerical data that can only be considered as labels, such as colors or brands of shoes."
  },
  {
    "objectID": "1-Data_Literacy/2-Stat_Process.html#references",
    "href": "1-Data_Literacy/2-Stat_Process.html#references",
    "title": "The Statistical Process",
    "section": "References",
    "text": "References\nBible Dictionary, “Pulse” at http://churchofjesuschrist.org/scriptures/bd/pulse.\nBrownlee, K. A. (1955). Statistics of the 1954 polio vaccine trials. Journal of the American Statistical Association, 50(272), pp. 1005-1013.\nFrancis, T., et. al. (1955). An evaluation of the 1954 poliomyletis vaccine trials. American Journal of Public Health and the Nation’s Health, 45(5)\nOffit, P. A. (2005). Why are pharmaceutical companies gradually abandoning vaccines? Health Affairs, 24(3), 622-630. doi:10.1377/hlthaff.24.3.622"
  },
  {
    "objectID": "1-Data_Literacy/4-Installing_R.html",
    "href": "1-Data_Literacy/4-Installing_R.html",
    "title": "Installing R",
    "section": "",
    "text": "In this course, we will use R and RStudio to perform necessary visualizations, analyses and to create web-based reports.\nIf you have not already installed R and RStudio on your computer, please follow these instructions to do so.\n\nInstall R: Install the latest version of R link\nInstall RStudio: (Mac OS X | PC)\n\nIf you are using a Chromebook or other “web browsing only” computer that will not allow you to install software locally, then set up an account at RStudio Cloud instead of installing R and RStudio as shown here. Use your BYU-I email and user ID.\nJust accept all of the default options when installing.\n\n\nTo install the statistical analysis program RStudio you will first need to install a piece of software called R. Funny name, right? (There was originally a software called “S” for statistics, and then “R” was invented later on. Part of the reason they used “R” was to claim that “R” was a “leap ahead” of “S.”)\nInstall the R Software by clicking:\n\nMac OS X M1-3 Chip (Most Common)\nMac OS X Intel Chip\nPC\n\nOnce that download finishes, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nNow that R is properly installed on your computer, we need to install RStudio. RStudio is an app that runs R inside of it and provides you with many other tools that go way beyond what R can do. This is why R must be installed first, so that RStudio can use it. You will never need to open R yourself. Just use RStudio. But without R, RStudio won’t work properly.\nInstall the RStudio app by clicking here: (Mac OS X | Windows).\nOnce the RStudio installer downloads, open the resulting file.\nAgain, work through the installation process, agreeing with all the defaults and terms of conditions.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps."
  },
  {
    "objectID": "1-Data_Literacy/4-Installing_R.html#detailed-instructions",
    "href": "1-Data_Literacy/4-Installing_R.html#detailed-instructions",
    "title": "Installing R",
    "section": "",
    "text": "To install the statistical analysis program RStudio you will first need to install a piece of software called R. Funny name, right? (There was originally a software called “S” for statistics, and then “R” was invented later on. Part of the reason they used “R” was to claim that “R” was a “leap ahead” of “S.”)\nInstall the R Software by clicking:\n\nMac OS X M1-3 Chip (Most Common)\nMac OS X Intel Chip\nPC\n\nOnce that download finishes, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nNow that R is properly installed on your computer, we need to install RStudio. RStudio is an app that runs R inside of it and provides you with many other tools that go way beyond what R can do. This is why R must be installed first, so that RStudio can use it. You will never need to open R yourself. Just use RStudio. But without R, RStudio won’t work properly.\nInstall the RStudio app by clicking here: (Mac OS X | Windows).\nOnce the RStudio installer downloads, open the resulting file.\nAgain, work through the installation process, agreeing with all the defaults and terms of conditions.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps."
  },
  {
    "objectID": "1-Data_Literacy/4-Installing_R.html#mac-processing-chip",
    "href": "1-Data_Literacy/4-Installing_R.html#mac-processing-chip",
    "title": "Installing R",
    "section": "Mac Processing Chip",
    "text": "Mac Processing Chip\nFor Macs, Which version of R-Studio you download depends on which processing chip you have. If you followed the instructions above and R-Studio opens but gives you a big error, you need to download the other version of R linked above."
  },
  {
    "objectID": "1-Data_Literacy/6-Spread.html",
    "href": "1-Data_Literacy/6-Spread.html",
    "title": "Describing Quantitative Data (Spread)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCalculate a percentile from data\nInterpret a percentile\nCalculate the standard deviation from data\nInterpret the standard deviation\nCalculate the five-number summary using software\nInterpret the five-number summary\nCreate a box plot using software\nDetermine the five-number summary visually from a box plot"
  },
  {
    "objectID": "1-Data_Literacy/6-Spread.html#lesson-outcomes",
    "href": "1-Data_Literacy/6-Spread.html#lesson-outcomes",
    "title": "Describing Quantitative Data (Spread)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCalculate a percentile from data\nInterpret a percentile\nCalculate the standard deviation from data\nInterpret the standard deviation\nCalculate the five-number summary using software\nInterpret the five-number summary\nCreate a box plot using software\nDetermine the five-number summary visually from a box plot"
  },
  {
    "objectID": "1-Data_Literacy/6-Spread.html#spread-of-a-distribution",
    "href": "1-Data_Literacy/6-Spread.html#spread-of-a-distribution",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Spread of a Distribution",
    "text": "Spread of a Distribution\nIn the previous lesson, we introduced two important characteristics of a distribution: the shape and the center. In this section, you will discover ways to summarize the spread of a distribution of data. The spread of a distribution of data describes how far the observations tend to be from each other. There are many ways to describe the spread of a distribution, but one of the most popular measurements of spread is called the “standard deviation.” \n\nStandard Deviation and Variance\nThis activity introduces two measures of spread: the standard deviation and the variance.\n\nBird Flu Fever \n\nAvian Influenza A H5N1, commonly called the bird flu, is a deadly illness that is currently only passed to humans from infected birds. This illness is particularly dangerous because at some point it is likely to mutate to allow human-to-human transmission. Health officials worldwide are preparing for the possibility of a bird flu pandemic.\n\nDr. K. Y. Yuen led a team of researchers who reported the body temperatures of people admitted to Chinese hospitals with confirmed cases of Avian Influenza. Their research team collected data on the body temperature at the time that people with the bird flu were admitted to the hospital. In the article, they reported on two groups of people, those with relatively uncomplicated cases of the bird flu and those with severe cases.\n\nThe table below presents the data representative of the body temperatures for the two groups of bird flu patients:\n\n\n\nBody Temperature\nCase Type\n\n\n\n\n38.1\nSimple\n\n\n38.3\nSimple\n\n\n38.4\nSimple\n\n\n39.5\nSimple\n\n\n39.7\nSimple\n\n\n39.1\nSevere\n\n\n39.5\nSevere\n\n\n38.9\nSevere\n\n\n39.2\nSevere\n\n\n39.9\nSevere\n\n\n39.7\nSevere\n\n\n39.0\nSevere\n\n\n\n\nLet us focus on the relatively uncomplicated cases. Creating a histogram of such a small dataset does not provide much benefit. With only a handful of values, there is not much shape to the distribution.\nWe can, however, use numerical summaries to give an indication of the center of the distribution.\n\nAnswer the following questions:\n\n\n\nWhat is the median of the body temperatures for the simple cases? \n\n\n\nSolution\n\n\nThe median body temperature for the simple cases is 38.4 degrees Centigrade.\n\n\n\nWhat is the mean of the body temperatures for the simple cases? \n\n\n\nSolution\n\n\nThe mean body temperature for the simple cases is 38.8 degrees Centigrade.\n\n\n\n\nWe will use these data to investigate some measures of the spread in a data set.\nThere is relatively little difference in the temperatures of the uncomplicated patients. The lowest is \\(38.1 ^\\circ \\text{C}\\), while the highest temperature is \\(39.7 ^\\circ \\text{C}\\).\nThe standard deviation is a measure of the spread in the distribution. If the data tend to be close together, then the standard deviation is relatively small. If the data tend to be more spread out, then the standard deviation is relatively large.\nThe standard deviation of the body temperatures is \\(0.742 ^\\circ \\text{C}\\). This number contains information from all the patients. If the patients’ temperatures had been more diverse, the standard deviation would be larger. If the patients’ temperatures were more uniform (i.e. closer together), then the standard deviation would have been smaller. If all the patients somehow had the same temperature, then the standard deviation would be zero.\nWe are working with a sample. To be explicit, we call \\(0.742 ^\\circ \\text{C}\\) the sample standard deviation. The symbol for the sample standard deviation is \\(s\\). This is a statistic. The parameter representing the population standard deviation is \\(\\sigma\\) (pronounced /SIG-ma/). In practice, we rarely know the value of the population standard deviation, so we use the sample standard deviation \\(s\\) as an approximation for the unknown population standard deviation \\(\\sigma\\).\nAt this point, you probably do not have much intuition regarding the standard deviation. We will use this statistic frequently. By the end of the semester you can expect to become very comfortable with this idea. For now, all you need to know is that if two variables are measured on the same scale, the variable with values that are further apart will have the larger standard deviation.\n\nR Instructions\n\n\n  To calculate the sample standard deviation in R, follow these steps:   \n\ndata &lt;- c(38.1,38.3,38.4,39.5,39.7,39.1,39.5,38.9,39.2,39.9,39.7,39.0)\n\nsd(data)\n\n[1] 0.5930788\n\n\n\n\n\n\nRounding: As a general rule, when reporting your answers in this class, round to three decimal places unless otherwise specified.\n\n \n\nCalculating the Standard Deviation by Hand\nHow is the standard deviation computed? Where does this “magic” number come from? How does one number include the information about the spread of all the points?\nIt is a little tedious to compute the standard deviation by hand. You will usually compute standard deviation with a computer. However, the process is very instructive and will help you understand conceptually what the statistic represents. As you work through the following steps, please remember the goal is to find a measure of the spread in a data set. We want one number that describes how spread out the data are.\nFirst, observe the number line below, where each x represents the temperature of a patient with a relatively uncomplicated case of bird flu. As mentioned earlier, there is not a huge spread in the temperatures.\n\nOn your sketch of the number line, we draw a vertical line at 38.8 degrees, the sample mean. Now, draw horizontal lines from the mean to each of your \\(\\times\\)’s. These horizontal line segments represent the spread of the data about the mean. Your plot should look something like this:\n\nThe length of each of the line segments represents how far each observation is from the mean. If the data are close together, these lines will be fairly short. If the distribution has a large spread, the line segments will be longer. The standard deviation is a measure of how long these lines are, as a whole.\nThe distance between the mean and an observation is referred to as a deviation. In other words, deviations are the lengths of the line segments drawn in the image above.\n\\[\n\\begin{array}{1cl}\n\\text{Deviation} & = & \\text{Value} - \\text{Mean} \\\\\n\\text{Deviation} & = & x - \\bar x\n\\end{array}\n\\]\nIf the observed value is greater than the mean, the deviation is positive. If the value is less than the mean, the deviation is negative.\nThe standard deviation is a complicated sort of average of the deviations. Making a table like the one below will help you keep track of your calculations. Please participate fully in this exercise. Writing your answers at each step and developing a table as instructed will greatly enhance the learning experience. By following these steps, you will be able to compute the standard deviation by hand, and more importantly, understand what it is telling you.\nStep 01: The first step in computing the standard deviation by hand is to create a table, like the following. Enter the observed data in the first column.\n\n\n\n\n\n\nObservation (\\(x\\))\n\n\n\n\n\n\nDeviation from the Mean (\\(x-\\bar x\\))\n\n\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\n\n\n\n\nStep 02: The second column of the table contains the deviations from the mean. Complete column 2 of the table above.\nCheck Results for Step 2\n\n\n\n\n\n\nObservation (\\(x\\))\n\n\n\n\nDeviation from the Mean (\\(x-\\bar x\\))\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nHow could we use this table to find the “typical” distance from each point to the mean? Think carefully about this, and then write down your answer before continuing.\n\n\n\nSolution\n\n\nYou may have suggested that we compute the mean of these values. This seems like a good idea. If we compute the mean, it will tell us the average deviation from the mean. \n\n\n9b. Compute the mean of Column 2. What do you get?\n\n\nSolution\n\n\nYou should have found that the mean of the deviations is zero. This is true for every data set. If you add up the deviations from the mean, the positive values will cancel with the negative values. The sum of the deviations from the mean will be zero, so the mean also must equal zero.\nThe good news is that you can use this fact to check if you are on the right track. If the deviations from the mean do not add up to zero, then you have made a mistake in the calculations. The bad news is that the deviations always add up to 0, making it look like the distance from the data to the mean is 0. Nonsense!\nThe mean of the deviations from the mean cannot be used to find a measure of the spread in a data set, but it does provide a guidepost that shows we are on the right track. We must find another way to estimate the spread of a data set.\n\n\n\nWe need a way to work with the negative deviations from the mean, so they do not cancel with the positive ones. What could we do? (Choose one of the four options below.)\n\n\n\nOption 1: Take the absolute value of the deviations\n\n\nThis is an excellent suggestion. This is probably one of the first things statisticians used to estimate the spread in the data.\nIf we take the absolute value of the deviations, then all the values are positive. By taking the mean of these numbers, we do get a measure of spread. This quantity is called the mean absolute deviation (MAD).\nThere is good news and bad news. The good news is, you discovered a way to estimate the spread in a data set. (In fact, the MAD is used as one estimate of the volatility of stocks.) The bad news is that the MAD does not have good theoretical properties. A proof of this claim requires calculus, and so will not be discussed here. For most applications, there is a better choice. Please select another option.\n\n\n\n\nOption 2: Square the deviations\n\n\nIf we square the deviations from the mean, the values that were negative will become positive. This leads to an estimator of the spread that has excellent theoretical properties. This is the best of the four options. You will apply this idea in Step 03.\n\n\n\n\nOption 3: Delete the negative deviations\n\n\nSorry, you can’t make your troubles go away by deleting things you don’t like. Please try again.\n\n\n\n\nOption 4: Do something entirely different\n\n\nYou probably have an ingenious idea. Surprisingly enough, there is a right answer to the question. Please choose a different option.\n\n\n\nPlease do not go on to Step 03 until you have finished this exploration.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nStep 03: Add a third column to your table. To get the values in this column, square the deviations from the mean that you found in Column 2.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(9.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\n\n\n\n\nStep 04: Now, add up the squared deviations from the mean.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\nThe sum of the squared deviations is 2.20. \n\nAnswer the following questions:\n\n\n\nSuppose that the researchers had collected body temperature data on 500 bird flu patients instead of 5. What would happen to the sum of the squared deviations, if the distribution of the data is the same for the 500 patients as the 5 patients?\n\n\n\nSolution\n\n\nWe would expect the sum of the squared deviations to be a lot larger than it is now. We would be adding squared deviations for 500 observations instead of 5. So, the sum of the squared deviations would be about 100 times larger.\n\nRemember, we are trying to find a measure of the spread of a data set. Our final measure should not be dependent on the sample size. We need to do something else.\n\n\n\nPlease do not go on until you have finished this exercise.\n\n\n\nStep 05: Recall that an average is adding a bunch of things up and dividing by the number of things. Consider taking the average of the squared deviations by adding them up and dividing by the number of deviations.\nUnfortunately, this is what is technically known as a “biased” estimate. We don’t get into what that means in this class, but to correct for the bias, we divide by \\(n-1\\) instead.\nThe number you computed in Step 05 is called the sample variance. It is a measure of the spread in a data set. It has very nice theoretical properties. The variance plays an important role in Statistics. We denote the sample variance by the symbol \\(s^2\\).\nIt can be shown that the sample variance is an unbiased estimator of the true population variance (which is denoted \\(\\sigma^2\\).) This means that the sample variance can be considered a reasonable estimator of the population variance. If the sample size is large, this estimator tends to be very good.\n\n\nResults from Step 5\n\nThe sum of the squared deviations is the sum of the values in Column 3. This sum equals 2.20. We divide the sum of Column 3 (\\(2.20\\)) by \\(n-1=5-1=4\\) to get the sample variance, \\(s^2\\):\n\\[s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55\\]\nThis is the sample variance.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\n\nVariance:\n\n\n\n\n\\(\\displaystyle{s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55}\\)\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nThe temperature data for the bird flu patients are in degrees Centigrade. What are the units of the variance?\n\n\n\nSolution\n\n\nThe data in Column 1 of the table is in degrees Centigrade. The mean also is in degrees Centigrade.\nTo get the numbers in Column 2, we subtracted the mean from each of the values in Column 1.\nWe squared the values in Column 2 to get Column 3. The units for this column are degrees Centigrade squared.\n\nThe sum of the numbers in Column 3 will also be in units of degrees Centigrade squared.\nWhen we divided that sum by \\(n-1\\), we obtained the sample variance. The sample variance has units of degrees Centigrade squared. This is not easily interpretable. It would be much easier to think about it if our measure of spread was in the same units as the data.\n\n\n\nWhat operation can we do to the variance to get a quantity with units degrees Centigrade?\n\n\n\nSolution\n\n\nIf we take the square root of the variance, we get a quantity that has units of degrees Centigrade. This quantity is the standard deviation.\n\n\n\n\nStep 06: Take the square root of the sample variance to get the sample standard deviation.\nThe sample standard deviation is defined as the square root of the sample variance.\n\\[\\text{Sample Standard Deviation} = s = \\sqrt{ s^2 } = \\sqrt{\\strut\\text{Sample Variance}}\\]\nThe standard deviation has the same units as the original observations. We use the standard deviation heavily in statistics.\nThe sample standard deviation (\\(s\\)) is an estimate of the true population standard deviation (\\(\\sigma\\)).\n\nAnswer the following questions:\n\n\n\nWhat is the sample standard deviation, \\(s\\), of the temperatures of the five patients with relatively uncomplicated cases of the bird flu?\n\n\n\nSolution\n\n\nThe sum of the squared deviations is the sum of the values in Column 3. This sum equals 2.20. We divide the sum of Column 3 (\\(2.20\\)) by \\(n-1=5-1=4\\) to get the sample variance, \\(s^2\\):\n\n\n\\(s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55\\)\n\n\nThis is the sample variance.\n\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\n\nVariance:\n\n\n\n\n\\(\\displaystyle{s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55}\\)\n\n\n\n\n\n\n\n\nStandard Deviation:\n\n\n\n\n\\(\\displaystyle{s = \\sqrt{s^2}=\\sqrt{0.55} \\approx 0.742}\\)\n\n\n\n\n\n\n\n\nThe sample standard deviation is \\(s = 0.742\\) degrees Centigrade.\nTake a few minutes to verify that you can recreate this table on your own.\n\n\n\n\n\n\nSummary\n\n  Standard Deviation\n  The standard deviation is one number that describes the spread in a set of data. If the data points are close together the standard deviation will be smaller than if they are spread out.\n  At this point, it may be difficult to understand the meaning and usefulness of the standard deviation. For now, it is enough for you to recognize the following points:\n\nThe standard deviation is a measure of how spread out the data are.\nIf the standard deviation is large, then the data are very spread out.\nIf the standard deviation is zero, then all the values are the identical–there is no spread in the data.\nThe standard deviation cannot be negative. \n\n\nVariance\nThe variance is the square of the standard deviation. The sample variance is denoted by the symbol \\(s^2\\). You found the sample standard deviation for patient temperatures of uncomplicated cases of bird in the bird above is \\(s = 0.74162\\). So, the sample variance for this data set is \\(s^2 = 0.74162^2 = 0.550\\). Be aware, if you had squared the rounded value of \\(s^2 = 0.742\\) in the calculation, you would have gotten an answer of 0.551 instead. This would be considered incorrect! \n\nRounding: Use unrounded values in interim calculations. Rounding too early in the process can lead to wrong answers.\n\n \n\nR Instructions\n\n\n  To calculate the sample variance in R:   \n\ndata &lt;- c(38.1,38.3,38.4,39.5,39.7,39.1,39.5,38.9,39.2,39.9,39.7,39.0)\n\nvar(data)\n\n[1] 0.3517424\n\n\n\n\n\nThe standard deviation and variance are two commonly used measures of the spread in a data set. Why is there more than one measure of the spread? The standard deviation and the variance each have their own pros and cons.\nThe variance has excellent theoretical properties. It is an unbiased estimator of the true population variance. That means that if many, many samples of \\(n\\) observations were drawn, the variances computed for all the samples would be centered nicely around the true population variance, \\(\\sigma^2\\). Because of these benefits, the variance is regularly used in higher-level statistics applications. One drawback of the variance is that the units for the variance are the square of the units for the original data. In the bird flu example, the body temperatures were measured in degrees Centigrade. So, the variance will have units of degrees Centigrade squared \\((^\\circ \\text{C})^2\\). What does degrees Centigrade squared mean? How do you interpret this? It doesn’t make any sense. This is one of the major drawbacks of the sample variance.\nBecause we take the square root of the variance to get the standard deviation, the standard deviation is in the same units as the original data. This is a great advantage, and is one of the reasons that the standard deviation is commonly used to describe the spread of data.\n\nAnswer the following questions:\n\n\nEnter the patient temperature data for the severe cases of bird flu into R Then use R to calculate the numerical summaries you have learned so far. As a reminder, the temperatures of patients with a severe case of bird flu are:\n\n39.1, 39.5, 38.9, 39.2, 39.9, 39.7, 39\n\n\nWhat is the mean, median, standard deviation and variance of the sample?\n\n\n\nSolution\n\n\nbird_flu &lt;- c(39.1, 39.5, 38.9, 39.2, 39.9, 39.7, 39)\n\nmean(bird_flu)\n\n[1] 39.32857\n\nmedian(bird_flu)\n\n[1] 39.2\n\nsd(bird_flu)\n\n[1] 0.377334\n\nvar(bird_flu)\n\n[1] 0.142381\n\n\n\n\nFor the next two questions, consider the histograms below comparing weight (in kilograms) of men (top histogram) to elephant seals (bottom histogram).\n\n\nWeight of Men Compared to Weight of Seals \n\n\nBased on the histograms, who has a greater sample mean weight, men or elephant seals?\n\n\n\nSolution\n\n\nThe mean is a measure of the center of a distribution. The mean weight of the men is less than the mean weight of the seals. We can see this because the bulk of the data in the histogram for the men’s weight is to the left of the seals’. The center of the distribution of elephant seals is about 195 kg. The center of the distribution of men’s weight is located below 100 kg on the number line.\n\n\n\n\nBased on the histograms, do the weights of men or elephant seals have a larger sample standard deviation?\n\n\n\nSolution\n\n\nStandard deviation is a measure of spread. You will note that the weights of the seals are more spread out than the weights of the men. Therefore, we conclude that the sample standard deviation of elephant seal weights is larger than the sample standard deviation of men’s weights.\n\n\n \n\n\nReview of Parameters and Statistics\nWe have now learned some statistics that can be used to estimate population parameters. For example, we use \\(\\bar x\\) to estimate the population mean \\(\\mu\\). The sample statistics \\(s\\) estimates the true population standard deviation \\(\\sigma\\). The following table summarizes what we have done so far:\n\n\n\n\n\n\n\n\nSample Statistic\n\n\n\n\nPopulation Parameter\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\\(\\bar x\\)\n\n\n\n\n\\(\\mu\\)\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\\(s\\)\n\n\n\n\n\\(\\sigma\\)\n\n\n\n\n\n\nVariance\n\n\n\n\n\\(s^2\\)\n\n\n\n\n\\(\\sigma^2\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\nUnless otherwise specified, we will always use R to find the sample variance and sample mean. In each case, the sample statistic estimates the population parameter. The ellipses \\(\\vdots\\) in this table hint that we will add rows in the future.\n\nOptional Reading: Formulas for \\(s\\) and \\(s^2\\) (Hidden)\n\n\nClick here if you love math\n\nFormulas\nFor those who like formulas, the equation for the sample variance and sample standard deviation are given here.\nSample variance:\n\\[\\displaystyle{ s^2=\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar x)^2}{n-1} }\\]\nSample standard deviation:\n\\[\\displaystyle{ s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar x)^2}{n-1}} }\\]\nwhere \\(x_i\\) is the \\(i^{th}\\) observed data value, and \\(i=1, 2, \\ldots, n\\).\nUnless otherwise specified, we will always use Excel to find the sample variance and sample mean.\nWhy do we divide by \\(n-1\\)?\nWhen computing the standard deviation or the variance, we are finding a value that describes the spread of data values. It is a measure of how far the data are from the mean. Since we do not know the true mean (\\(\\mu\\),) we use the sample mean (\\(\\bar x\\),) to estimate it. Typically, the data will be closer to \\(\\bar x\\) than to \\(\\mu\\), since \\(\\bar x\\) was computed using the data. To compensate for this, we divide by \\(n-1\\) rather than \\(n\\) when we find the “average” of the squared deviations from the mean. It turns out, that subtracting 1 from \\(n\\) inflates this average by the precise amount needed to compensate for the use of \\(\\bar x\\) as an estimate for \\(\\mu\\). As a result, the sample variance (\\(s^2\\)) is a good estimator of the population variance (\\(\\sigma^2\\).)\n\n\nNeither the standard deviation nor the variance is resistant to outliers. This means that when there are outliers in the data set, the standard deviation and the variance become artificially large. It is worth noting that the mean is also not resistant. When there are outliers, the mean will be “pulled” in the direction of the outliers.\nThe mean and standard deviation are used to describe the center and spread when the distribution of the data is symmetric and bell-shaped. If data are not symmetric and bell-shaped, we typically use the five-number summary (discussed below) to describe the spread, because this summary is resistant."
  },
  {
    "objectID": "1-Data_Literacy/6-Spread.html#additional-tools-to-describe-the-data",
    "href": "1-Data_Literacy/6-Spread.html#additional-tools-to-describe-the-data",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Additional Tools to Describe the Data",
    "text": "Additional Tools to Describe the Data\nRecall the five steps of the Statistical Process (and the mnemonic “Daniel Can Discern More Truth).\n\n\n\n\n\n\nStep 1:\n\n\n\n\nDaniel\n\n\n\n\nDesign the study\n\n\n\n\n\n\nStep 2:\n\n\n\n\nCan\n\n\n\n\nCollect data\n\n\n\n\n\n\nStep 3:\n\n\n\n\nDiscern\n\n\n\n\nDescribe the data\n\n\n\n\n\n\nStep 4:\n\n\n\n\nMore\n\n\n\n\nMake inferences\n\n\n\n\n\n\nStep 5:\n\n\n\n\nTruth\n\n\n\n\nTake action\n\n\n\n\n\n\n\nStep 3 of this process is “Describe the data.” You have already learned about the mean, median, mode, standard deviation, variance and histograms. These can be good ways to describe the data. The following information on percentiles, quartiles, 5-number summaries, and boxplots will help you learn other common ways to describe data, especially if the data are skewed or contain outliers.\n\nFor symmetric, bell-shaped data, the mean and standard deviation provide a good description of the center and shape of the distribution. The mean and standard deviation are not sufficient to describe a distribution that is skewed or has outliers. An outlier is any observation that is very far from the others. The mean is pulled in the direction of the outlier. Also, the standard deviation is inflated by points that are very far from the mean.\nNow, you have probably had some experience with percentiles in the past especially when you received a score on a standardized test such as the ACT. Even though percentiles are commonly used, they are generally misunderstood. Before examining the wrong site/wrong patient data, let’s review percentiles. Even if you think you understand percentiles, please study this section carefully.\n\n\nPercentiles and Quartiles\nImagine a very long street with houses on one side. The houses increase in value from left to right. At the left end of the street is a small cardboard box with a leaky roof. Next door is a slightly larger cardboard box that does not leak. The houses eventually get larger and more valuable. The rightmost house on the street is a huge mansion.\n\nAnswer the following question:\n\n\n\nThere are 100 homes with increasing property values. How many fences are needed to separate the 100 properties?\n\n\n\nSolution\n\n\nIn order to separate the 100 homes, 99 fences are required.\n\n\n\n\nThe home values are representative of data. If we have a list of data, sorted in increasing order, and we want to divide it into 100 equal groups, we only need 99 dividers (like fences) to divide up the data. The first divider is as large or larger than 1% of the data. The second divider is as large or larger than 2% of the data, and so on. The last divider, the 99th, is the value that is as large or larger than 99% of the data. These dividers are called percentiles. A percentile is a number such that a specified percentage of the data are at or below this number. For example, the 99th percentile is a number such that 99% of the data are at or below this value. As another example, half (50%) of the data lie at or below the 50th percentile. The word percent means \\(\\div 100\\). This can help you remember that the percentiles divide the data into 100 equal groups.\nQuartiles are special percentiles. The word quartile is from the Latin quartus, which means “fourth.” The quartiles divide the data into four equal groups. The quartiles correspond to specific percentiles. The first quartile, Q1, is the 25th percentile. The second quartile, Q2, is the same as the 50th percentile or the median. The third quartile, Q3, is equivalent to the 75th percentile.\n\nAnswer the following questions:\n\n\n\nHow many quartiles are there?\n\n\n\nSolution\n\n\nThere are 3 quartiles! To divide the data into 100 equal groups, we needed 99 percentiles. To divide the data into 4 equal groups, we need 3 quartiles.\n\n \n\n\n\nWrong Site/Wrong Patient Lawsuits\nPercentiles can be used to describe the center and spread of any distribution and are particularly useful when the distribution is skewed or has outliers. To explore this issue, you will use software to calculate percentiles of data on costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nBut first, we need to learn how to load data into R.\nR has many built-in toolboxes. R also has a vast array of toolboxes beyond the built-in ones that we must first install. This is like going to the Home Depot to buy a specialized toolbox and then storing it in your garage. We only have to “buy” it once.\nTo install a library, we use the install.packages(\"\") command, where we specify the library we want in the quotes inside the parentheses.\nrio is a toolbox that is very useful for loading data into R. If you haven’t already done so, install the rio library.\ninstall.packages('rio')\nWhile you only have to install libraries once, you have to load them every time you want to use one. It’s like going to the garage to get the toolbox you need for the job.\nNow let’s load the data and calculate some percentiles!\n\nR Instructions\n\n\n\nOpen R and load the rio library:\n\n\nlibrary(rio)\n\n\nUse the import() function to load the dataset:\n\n\nwrong_site &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")\n\n  To calculate percentiles and quartiles in R, do the following\n\nDatasets loaded into R may have many columns of information. To specify which column in the dataset should be used for analysis we use the $. For example, if we wanted only to look at the Wrong_Patient clumn in the wrong_site dataset:\n\n\n\n  [1]  250000  106900   62307  192800   20769    2680    4300   30819   23214\n [10]   26099       0   50000   66600  175000   10384   42900   52928       0\n [19]    8200    2500    6900  126300     900    7700  140000   76000   50000\n [28]  354530    5359    4300   12000   16749   35600    9045   21900    2010\n [37]   22444   50000   85000   40370   39863       0   36100   49000   48908\n [46]   19800   32200    3400       0   75000   21774    2600   30000    7300\n [55]  176940   55000    9500   55272    4690   75000   34168   83700    1005\n [64]   17419   34800   14739       0       0    1000     325   41538  108200\n [73]   63224   15000       0    3900   65657   50000  109205    3900   10000\n [82]    9900   87096   12090       0    1000       0   74701    3900   18000\n [91]       0   33499    1250       0   29813   11724  141363    3685   35508\n[100]    2500   12060    5695   50582   82071   55400       0  104400     500\n[109]       0   25000   10000   85000   25000       0   24100    3900 1250000\n[118]   15074     550    7195  101800   11600    1000    4020   19764   25794\n[127]     900   10000   35200   94100       0   16909  128400   60967   50000\n[136]   50000   84751   46800  130308   43800   49242   22800   15500   11054\n[145]     400   10000  104790   13064    6400  100000   17084   16300   11000\n[154]   12500       0    1200       0  200000    3900    3015  172200   25000\n[163]   27468  250000   21104   12500   30000   59000   46227     500  131000\n[172]    2345    6000       0     670    9714      NA      NA      NA      NA\n[181]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[190]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[199]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[208]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[217]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[226]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[235]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[244]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[253]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[262]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[271]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[280]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[289]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[298]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[307]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[316]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[325]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[334]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[343]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[352]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[361]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[370]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[379]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[388]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[397]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[406]      NA      NA      NA      NA      NA      NA\n\n\n\nUse R’s quantile() function. This functions requires two inputs separated by a comma: the data and the desired percentile input as a decimal.\nTo calculate the 25th percentile for the costs of surgery done on the Wrong Site:\n\n\nquantile(wrong_site$Wrong_Site, .25, na.rm=TRUE)\n\n  25% \n29496 \n\n# Note: the na.rm=TRUE removes the missing values from the dataset\n\n\n\n\nThe first quartile (\\(Q_1\\)) or 25th percentile (calculated in R) of the wrong-site data is: $29,496. (This result is illustrated in the figure below.) This means that 25 percent of the time hospitals lost a wrong-site lawsuit, they had to pay $29,496 or less. The 25th percentile can be written symbolically as: P25 = $29,496. Other percentiles can be written the same way. The 99th percentile can be written as P99.\n\nPercentiles (Calculated in Excel)\n\n\n\n\n \n \n\n\n\n\n1st percentile\n0\n\n\n2nd percentile\n0\n\n\n3rd percentile\n0\n\n\n…\n…\n\n\n24th percentile\n28633.4\n\n\n25th percentile\n29496\n\n\n26th percentile\n31067\n\n\n\n\n\nAnswer the following questions:\n\n\n\nWhat is the 13th percentile of the wrong site data?\n\n\n\nSolution\n\n\n$6343.40\n\n\nquantile(wrong_site$Wrong_Site, .13, na.rm=TRUE)\n\n   13% \n6343.4 \n\n\n\n\nHow would you interpret the 13th percentile (assuming the 13th percentile is $6343.40)?\n\n\n\n100 of the lawsuits cost more than 13%.\n\n\n13% of the lawsuits cost the hospital over $6343.40.\n\n\nIn 13% of the wrong-site lawsuits, hospitals had to pay $6343.40 or less.\n\n\nFor 13% of the wrong-site lawsuits, the hospitals had to pay $6343.40 to the patient.\n\n\n\n\nSolution\n\n\nCorrect Answer: C\n\n\n\n\nFind P90.\n\n\n\nSolution\n\n\n$149,963.00\n\n\nquantile(wrong_site$Wrong_Site, .9, na.rm=TRUE)\n\n   90% \n149963 \n\n\n\n\n\nThe quartiles divide a sorted list of data into four equal groups. So, each group contains 25% of the data. The first quartile is the value that is greater than or equal to 25% of the data. What is another name for this number?\n\n\n\nSolution\n\n\nThe 25th percentile.\n\n\n\n\nWhat is the value of the third quartile?\n\n\n\nSolution\n\n\n$124,280.00\n\n\nquantile(wrong_site$Wrong_Site, .75, na.rm=TRUE)\n\n   75% \n124280 \n\n\n\n\n\nHalf of the wrong-site lawsuits judgments were less than or equal to what value?\n\n\n\nSolution\n\n\n$68,552.00\n\n\nquantile(wrong_site$Wrong_Site, .5, na.rm=TRUE)\n\n  50% \n68552 \n\n#Or\nmedian(wrong_site$Wrong_Site)\n\n[1] 68552\n\n\n\n\n\nThe median is the middle observation in a sorted list of data. What percentile is always equal to the median?\n\n\n\nSolution\n\n\nThe 50th percentile\n\n\n \n\n\n\nThe Five-Number Summary\nAnother way to summarize data is with the five-number summary. The five-number summary is comprised of the minimum, the first quartile, the second quartile (or median), the third quartile, and the maximum.\nThere is a very easy way to get the Five-Number Summary along with the mean and standard deviation. The favstats() function in the mosaic library gives us all of our favorite statistics.\nAs before, we will have to install the mosaic library once, then load it when we want to use it.\n\nR Instructions\n\n\n  To find the values for a five-number summary in R, do the following\n\nInstall the mosaic library (Only Once):\n\ninstall.packages(\"mosaic\")\n\nLoad the Library:\n\n\nlibrary(mosaic)\n\n\nInput the data into the favstats() function:\n\n\nfavstats(wrong_site$Wrong_Site)\n\n min    Q1 median     Q3    max     mean       sd   n missing\n   0 29496  68552 124280 780575 80041.24 71403.83 411       0\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nGive the five-number summary for the Wrong Site data.\n\n\nSolution\n\n\n\\[\\displaystyle{\\$0,~~\\$29,496;~~\\$68,552;~~\\$124,280;~~\\$780,575}\\]\n\n\n \n\n\n\n\n\nSome students mistakenly include the mean in the five-number summary. The third value in the five-number summary is the median.\n\n \n\n\nBoxplots\nA boxplot is a graphical representation of the five-number summary. Unlike the mean or standard deviation, a boxplot is resistant to outliers. That means that it won’t be “pulled” one way or the other by extraordinarily large or small values in the data as will a mean, for instance. We will illustrate the process of making a boxplot using the wrong-site data.\nFollow the steps below to learn how a boxplot relates to the five-number summary. Learning what each part of the boxplot represents will enable you to interpret the plot correctly.\nStep 01: To draw a boxplot, start with a number line.\n\nStep 02: Draw a vertical line segment above each of the quartiles.\n\nStep 03: Connect the tops and bottoms of the line segments, making a box.\n\nStep 04: Make a smaller mark above the values corresponding to the minimum and the maximum.\n\nStep 05: Draw a line from the left side of the box to the minimum, and draw another line from the right side of the box the maximum.\n\nStep 06: These last two lines look like whiskers, so this is sometimes called a box-and-whisker plot.\n\n\n\nR Instructions\n\n\nTo create a boxplot in Excel, do the following\n\nLoad the data file. For this example, open the file WrongSiteWrongPatient.xlsx.\n\n\nwrong_site &lt;-  import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")\n\n\nUse the boxplot() function to get a boxplot:\n\n\nboxplot(wrong_site$Wrong_Site)\n\n\n\n\n\n\n\n# We can make it a little nicer by adding labels to the x and y axes and adding a title as follows:\n\nboxplot(wrong_site$Wrong_Site, xlab=\"Wrong Site\", ylab=\"Cost in $\", main=\"Boxplot of Costs of Operating on the Wrong Site\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nCreate a histogram of the wrong-patient lawsuit data, located in column B of the file WrongSiteWrongPatient.xlsx. What is the shape of the wrong-patient data?\n\n\nSkewed left\nSymmetric\nSkewed right\nMulti-modal\nUniform\n\n\n\nSolution\n\nTo create the histogram, use the histogram() function on the data:\n\nhistogram(wrong_site$Wrong_Patient)\n\n\n\n\n\n\n\n\nFrom the histogram we clearly see most values bunched near the left and gradually fewer values as we move to the right along the number line, so the correct answer is ‘c. Skewed right’."
  },
  {
    "objectID": "1-Data_Literacy/6-Spread.html#summary-1",
    "href": "1-Data_Literacy/6-Spread.html#summary-1",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA percentile is calculated in R using quantile(data, 0.#) where the 0.# is the percentile written as a decimal number. So the 20th percentile would be written as 0.2.\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, if say 80% of college students were shorter than (or equal to) 70 inches tall in height, then the 80th percentile of heights of college students would be 70 inches.\nStandard deviation is calculated in R for a sample of data using sd(data).\nThe standard deviation is a number that describes how spread out the data typically are from the mean of that data. A larger standard deviation means the data are more spread out from their mean than data with a smaller standard deviation. The standard deviation is never negative. A standard deviation of zero implies all values in the data set are exactly the same.\nTo compute any of the five-number summary values in R, use the R function favstats(data) which also includes the mean and standard deviation.\nThe five-number summary consists of (1) the minimum value in the data, (2) the first quartile (25th percentile) of the data, (3) the median of the data (50th percentile), (4) the third quartile (75th percentile) of the data, and (5) the maximum value occurring in the data.\nTo create a boxplot in R, use the boxplot(data) or for multiple columns boxplot(data1, data2, names=c(\"Name of Column 1\", \"Name of Column 2)).\nBoxplots are a visualization of the five-number summary of a data set."
  },
  {
    "objectID": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html",
    "href": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "In this document, we will demonstrate how to summarize quantitative data for multiple groups in a dataset.\n\n\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#load-the-data-and-libraries",
    "href": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#load-the-data-and-libraries",
    "title": "Summarizing Data",
    "section": "",
    "text": "library(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#summary-statistics",
    "href": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#summary-statistics",
    "title": "Summarizing Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nWe can easily extend favstats() to output our favorite statistics for multiple groups. We first must identify the quantitative factor we want to compare. For example, we could compare agreeableness between the sexes.\n\n# This gives us the summary statistics for Agreeableness across all groups\nfavstats(big5$Agreeableness)\n\n min Q1 median Q3 max     mean       sd   n missing\n  21 67     75 81 100 73.43457 13.24909 405       0\n\n# Adding the '~' tells R to break the data into groups (determined by the right side of the '~') and calculate the means of the variable on the left\nfavstats(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n  big5$`Sex(M/F)` min Q1 median Q3 max     mean       sd   n missing\n1               F  21 69     77 85 100 75.92035 12.94640 226       0\n2               M  25 63     73 79  94 70.29609 12.99218 179       0"
  },
  {
    "objectID": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#visual-summaries-by-group",
    "href": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#visual-summaries-by-group",
    "title": "Summarizing Data",
    "section": "Visual Summaries by Group",
    "text": "Visual Summaries by Group\nWe can use the exact same format as we used for favstats() for boxplot():\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`)"
  },
  {
    "objectID": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#improving-graphs",
    "href": "1-Data_Literacy/8-Summarizing_Data_Multiple_Groups.html#improving-graphs",
    "title": "Summarizing Data",
    "section": "Improving Graphs",
    "text": "Improving Graphs\nThroughout this course, we will ease into making better visualizations. For now, here are some basic techniques that will usually apply to all graphing functions in R:\n\n# Changing color by sepecifying the `col = c()`\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n# R also assigns a numerical value to `col = `.  Try different numbers\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(2,3))\n\n\n\n\n\n\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(4,6))\n\n\n\n\n\n\n\n# Adding better axis labels using `xlab = ` and `ylab = `:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, xlab = \"Biosex\", ylab = \"Trait Agreeableness\")\n\n\n\n\n\n\n\n# Adding a title:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, main = \"Comparing Agreeableness by Biosex\")\n\n\n\n\n\n\n\n# Putting it all together:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`,main = \"Comparing Agreeableness by Biosex\", xlab = \"Biosex\", ylab = \"Trait Agreeableness\", col = c(3, 4))"
  },
  {
    "objectID": "1-Data_Literacy/TestingTesting.html",
    "href": "1-Data_Literacy/TestingTesting.html",
    "title": "Testing…Testing…1…2…3",
    "section": "",
    "text": "Introduction\nThis type of file is called a “markdown” file. Markdown is like Microsoft Word but much more powerful. This file is made specifically for creating fancy reports and has a file type .qmd meaning “Quarto Markdown”.\nYou will become very familiar with these files throughout the semester. For now, it’s only necessary to download this file, save it in a sensible folder on your computer or OneDrive, and “run” it.\nClicking on the “Render” button above will create an .html document that should open up in your default browser. This .html document will be created and saved in the same location as this TestingTesting.qmd document.\nNOTE: If this document is in your Downloads folder, that is also where the html file will appear.\nAs you see, we can make section headers and type regular text. But the power of .qmd files is that we can code inside these documents and present our output directly within the document.\nClick “Render” to test to see if your software is set up.\nWhen coding, we have to tell the computer when we’re writing text and when we expect it to compile code. Below is an example of a “code chunk” that creates a made up graph.\nYou do not have to understand this right now. We’re only testing that R and RStudio are set up correctly.\n\n\nCode\nx &lt;- seq(0,10, length = 100)\ny &lt;- 2+exp(x)\n\nplot(x,y, type = \"l\", lwd=2, col=\"darkblue\", main = \"Exponential Function\")"
  },
  {
    "objectID": "2-Tidy_Data/02-Filter.html",
    "href": "2-Tidy_Data/02-Filter.html",
    "title": "filter()",
    "section": "",
    "text": "Good scientists NEVER delete data from original records. The tidyverse allows us to create a new, clean dataset with a transparent set of steps from which we can create graphs, visualizations and analyses without losing any of the original data.\nWe here demonstrate how to define criteria for choosing which rows to include from the data. Consider the High School survey data which consists of responses to 60 questions from 312 high school students.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nLogical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "2-Tidy_Data/02-Filter.html#logical-operators",
    "href": "2-Tidy_Data/02-Filter.html#logical-operators",
    "title": "filter()",
    "section": "",
    "text": "Logical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "2-Tidy_Data/02-Filter.html#filtering-on-categorical-data",
    "href": "2-Tidy_Data/02-Filter.html#filtering-on-categorical-data",
    "title": "filter()",
    "section": "Filtering on Categorical Data",
    "text": "Filtering on Categorical Data\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\n# See what the distinct values are in the Handed column\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# We could also try using %in% instead of  \"!=\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"))\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nNOTE: The %in% and the != will not always give you the same results. If there are misspellings or other options, using %in% will limit the data to only those with the exact spelling in the list provided. For example, someone responding “left handed” (all lower case), would not be included in the clean data. Misspellings would, however, be included if I use != \"Ambitextrous\" because that only removes rows written exactly that way. Things like, ambidextrious or RightHanded would still be included.\nI could further limit my data to students from Florida and Missouri:\n\nnew_data &lt;- survey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\ndim(new_data)\n\n[1] 27 60\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#additional-resources",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#additional-resources",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Additional Resources",
    "text": "Additional Resources\nBelow are 2 great resources for digging a little deeper into data manipulation in R.\nTidyverse Cheat Sheet\nR for Data Science\nNext, we will explain a few programming fundamentals that will help make"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#logical-operators",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#logical-operators",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Logical Operators",
    "text": "Logical Operators\nLogical operators are used extensively in computer programming to determine if a certain condition is met. They always return a “True” or a “False”, but we can treat them like a 0 for false and 1 for true.\nWe can tell a computer to determine a conditional statement (typically “less than”, “greater than” or “not equal to”) for specific variables, and it will return a TRUE if the statement is true and FALSE if not.\n\nQuantitative Variables\nLet’s examine the height_cm column in the survey data.\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\nhist(survey$Height_cm)\n\n\n\n\n\n\n\n\nThe maximum is 999 cm, which is around 33 Feet! We know this is not a possible value.\nIt is very unlikely that a high school student is taller than 7 feet. We can use a logical operator to see which students are taller than 7 feet (213.36 cm):\n\nsurvey$Height_cm &gt; 213.36\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# To illustrate, the below code puts the Survey column, Height_cm, along with the TRUE/FALSE logical\ndata.frame(Height_cm =survey$Height_cm, logical = survey$Height_cm &gt; 213.36)[73:84,]\n\n   Height_cm logical\n73     162.0   FALSE\n74     172.0   FALSE\n75     160.0   FALSE\n76     175.0   FALSE\n77     182.8   FALSE\n78     153.0   FALSE\n79     184.0   FALSE\n80     170.0   FALSE\n81     150.0   FALSE\n82     177.8   FALSE\n83     999.0    TRUE\n84     172.7   FALSE\n\n\nWhat does the above code return?\nA list of TRUE and FALSE for every line of the data. It is as long as the number of rows in the dataset.\n\n\nCategorical Variables\nWe can also use logical operators for categorical data. For example, if we wanted to see how many people are ambidextrous, we can run the following:\n\n# What are unique values in of the respondents?\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\n# Use a logical operator to get TRUE and FALSE for students who responded \"Ambidextrous\" on the survey question about handedness\n\nsum(survey$Handed == \"Ambidextrous\")\n\n[1] 9\n\ndata.frame(Handed = survey$Handed, logical = survey$Handed == 'Ambidextrous')\n\n          Handed logical\n1    Left-Handed   FALSE\n2   Right-Handed   FALSE\n3   Right-Handed   FALSE\n4   Right-Handed   FALSE\n5    Left-Handed   FALSE\n6   Right-Handed   FALSE\n7   Ambidextrous    TRUE\n8   Right-Handed   FALSE\n9   Right-Handed   FALSE\n10  Right-Handed   FALSE\n11  Right-Handed   FALSE\n12   Left-Handed   FALSE\n13  Right-Handed   FALSE\n14  Right-Handed   FALSE\n15  Right-Handed   FALSE\n16  Right-Handed   FALSE\n17  Right-Handed   FALSE\n18  Right-Handed   FALSE\n19  Right-Handed   FALSE\n20   Left-Handed   FALSE\n21  Right-Handed   FALSE\n22   Left-Handed   FALSE\n23  Right-Handed   FALSE\n24  Right-Handed   FALSE\n25  Right-Handed   FALSE\n26  Right-Handed   FALSE\n27  Right-Handed   FALSE\n28  Ambidextrous    TRUE\n29  Right-Handed   FALSE\n30  Right-Handed   FALSE\n31  Right-Handed   FALSE\n32  Right-Handed   FALSE\n33  Right-Handed   FALSE\n34  Right-Handed   FALSE\n35   Left-Handed   FALSE\n36  Right-Handed   FALSE\n37  Right-Handed   FALSE\n38   Left-Handed   FALSE\n39  Right-Handed   FALSE\n40  Right-Handed   FALSE\n41  Right-Handed   FALSE\n42  Right-Handed   FALSE\n43  Right-Handed   FALSE\n44  Right-Handed   FALSE\n45  Right-Handed   FALSE\n46  Right-Handed   FALSE\n47  Right-Handed   FALSE\n48  Ambidextrous    TRUE\n49  Right-Handed   FALSE\n50  Right-Handed   FALSE\n51  Right-Handed   FALSE\n52  Right-Handed   FALSE\n53  Right-Handed   FALSE\n54  Right-Handed   FALSE\n55  Right-Handed   FALSE\n56   Left-Handed   FALSE\n57  Right-Handed   FALSE\n58  Right-Handed   FALSE\n59  Right-Handed   FALSE\n60  Right-Handed   FALSE\n61  Right-Handed   FALSE\n62  Right-Handed   FALSE\n63  Right-Handed   FALSE\n64  Right-Handed   FALSE\n65  Right-Handed   FALSE\n66  Right-Handed   FALSE\n67  Right-Handed   FALSE\n68  Right-Handed   FALSE\n69  Right-Handed   FALSE\n70  Right-Handed   FALSE\n71  Right-Handed   FALSE\n72  Right-Handed   FALSE\n73  Right-Handed   FALSE\n74   Left-Handed   FALSE\n75   Left-Handed   FALSE\n76  Right-Handed   FALSE\n77  Right-Handed   FALSE\n78  Right-Handed   FALSE\n79  Right-Handed   FALSE\n80   Left-Handed   FALSE\n81  Right-Handed   FALSE\n82   Left-Handed   FALSE\n83  Ambidextrous    TRUE\n84  Right-Handed   FALSE\n85  Right-Handed   FALSE\n86   Left-Handed   FALSE\n87  Right-Handed   FALSE\n88  Right-Handed   FALSE\n89   Left-Handed   FALSE\n90  Right-Handed   FALSE\n91   Left-Handed   FALSE\n92  Right-Handed   FALSE\n93  Right-Handed   FALSE\n94  Right-Handed   FALSE\n95  Right-Handed   FALSE\n96  Right-Handed   FALSE\n97  Right-Handed   FALSE\n98  Right-Handed   FALSE\n99  Right-Handed   FALSE\n100 Right-Handed   FALSE\n101 Right-Handed   FALSE\n102 Right-Handed   FALSE\n103 Right-Handed   FALSE\n104 Right-Handed   FALSE\n105 Right-Handed   FALSE\n106 Right-Handed   FALSE\n107 Right-Handed   FALSE\n108 Right-Handed   FALSE\n109 Right-Handed   FALSE\n110 Right-Handed   FALSE\n111 Right-Handed   FALSE\n112 Right-Handed   FALSE\n113 Right-Handed   FALSE\n114 Right-Handed   FALSE\n115 Right-Handed   FALSE\n116 Right-Handed   FALSE\n117 Right-Handed   FALSE\n118 Right-Handed   FALSE\n119 Right-Handed   FALSE\n120 Right-Handed   FALSE\n121 Right-Handed   FALSE\n122 Right-Handed   FALSE\n123 Right-Handed   FALSE\n124 Right-Handed   FALSE\n125 Right-Handed   FALSE\n126 Right-Handed   FALSE\n127 Right-Handed   FALSE\n128 Right-Handed   FALSE\n129 Right-Handed   FALSE\n130 Right-Handed   FALSE\n131  Left-Handed   FALSE\n132 Right-Handed   FALSE\n133 Right-Handed   FALSE\n134 Right-Handed   FALSE\n135 Right-Handed   FALSE\n136 Right-Handed   FALSE\n137 Right-Handed   FALSE\n138 Right-Handed   FALSE\n139 Right-Handed   FALSE\n140 Right-Handed   FALSE\n141 Right-Handed   FALSE\n142 Right-Handed   FALSE\n143 Ambidextrous    TRUE\n144 Right-Handed   FALSE\n145 Right-Handed   FALSE\n146 Right-Handed   FALSE\n147 Right-Handed   FALSE\n148 Right-Handed   FALSE\n149  Left-Handed   FALSE\n150 Right-Handed   FALSE\n151 Right-Handed   FALSE\n152 Right-Handed   FALSE\n153 Right-Handed   FALSE\n154 Right-Handed   FALSE\n155 Right-Handed   FALSE\n156 Right-Handed   FALSE\n157 Right-Handed   FALSE\n158 Right-Handed   FALSE\n159 Right-Handed   FALSE\n160 Right-Handed   FALSE\n161 Right-Handed   FALSE\n162 Right-Handed   FALSE\n163  Left-Handed   FALSE\n164 Right-Handed   FALSE\n165 Right-Handed   FALSE\n166 Right-Handed   FALSE\n167  Left-Handed   FALSE\n168 Right-Handed   FALSE\n169  Left-Handed   FALSE\n170  Left-Handed   FALSE\n171 Right-Handed   FALSE\n172 Right-Handed   FALSE\n173 Right-Handed   FALSE\n174 Right-Handed   FALSE\n175 Right-Handed   FALSE\n176 Right-Handed   FALSE\n177 Right-Handed   FALSE\n178 Right-Handed   FALSE\n179 Right-Handed   FALSE\n180 Right-Handed   FALSE\n181 Right-Handed   FALSE\n182 Right-Handed   FALSE\n183 Right-Handed   FALSE\n184 Right-Handed   FALSE\n185 Ambidextrous    TRUE\n186 Right-Handed   FALSE\n187 Right-Handed   FALSE\n188 Right-Handed   FALSE\n189 Right-Handed   FALSE\n190 Right-Handed   FALSE\n191 Right-Handed   FALSE\n192 Right-Handed   FALSE\n193 Right-Handed   FALSE\n194 Right-Handed   FALSE\n195 Right-Handed   FALSE\n196 Right-Handed   FALSE\n197 Right-Handed   FALSE\n198 Right-Handed   FALSE\n199 Right-Handed   FALSE\n200 Right-Handed   FALSE\n201 Right-Handed   FALSE\n202 Right-Handed   FALSE\n203 Right-Handed   FALSE\n204 Right-Handed   FALSE\n205 Right-Handed   FALSE\n206  Left-Handed   FALSE\n207 Ambidextrous    TRUE\n208 Right-Handed   FALSE\n209 Ambidextrous    TRUE\n210 Right-Handed   FALSE\n211 Right-Handed   FALSE\n212 Right-Handed   FALSE\n213 Right-Handed   FALSE\n214 Right-Handed   FALSE\n215 Right-Handed   FALSE\n216 Right-Handed   FALSE\n217 Right-Handed   FALSE\n218 Right-Handed   FALSE\n219 Right-Handed   FALSE\n220 Right-Handed   FALSE\n221 Right-Handed   FALSE\n222 Right-Handed   FALSE\n223 Right-Handed   FALSE\n224  Left-Handed   FALSE\n225  Left-Handed   FALSE\n226 Right-Handed   FALSE\n227 Right-Handed   FALSE\n228 Right-Handed   FALSE\n229 Right-Handed   FALSE\n230 Right-Handed   FALSE\n231 Right-Handed   FALSE\n232 Right-Handed   FALSE\n233 Right-Handed   FALSE\n234 Right-Handed   FALSE\n235 Right-Handed   FALSE\n236  Left-Handed   FALSE\n237 Right-Handed   FALSE\n238  Left-Handed   FALSE\n239 Right-Handed   FALSE\n240  Left-Handed   FALSE\n241 Right-Handed   FALSE\n242 Right-Handed   FALSE\n243 Right-Handed   FALSE\n244  Left-Handed   FALSE\n245 Right-Handed   FALSE\n246 Right-Handed   FALSE\n247 Right-Handed   FALSE\n248 Right-Handed   FALSE\n249 Right-Handed   FALSE\n250 Right-Handed   FALSE\n251  Left-Handed   FALSE\n252 Right-Handed   FALSE\n253 Right-Handed   FALSE\n254 Right-Handed   FALSE\n255 Right-Handed   FALSE\n256 Right-Handed   FALSE\n257 Right-Handed   FALSE\n258 Right-Handed   FALSE\n259 Right-Handed   FALSE\n260 Right-Handed   FALSE\n261 Right-Handed   FALSE\n262 Right-Handed   FALSE\n263 Right-Handed   FALSE\n264 Right-Handed   FALSE\n265 Right-Handed   FALSE\n266 Right-Handed   FALSE\n267 Right-Handed   FALSE\n268 Right-Handed   FALSE\n269 Right-Handed   FALSE\n270 Right-Handed   FALSE\n271 Right-Handed   FALSE\n272 Right-Handed   FALSE\n273 Right-Handed   FALSE\n274 Right-Handed   FALSE\n275 Right-Handed   FALSE\n276  Left-Handed   FALSE\n277 Right-Handed   FALSE\n278 Right-Handed   FALSE\n279 Right-Handed   FALSE\n280  Left-Handed   FALSE\n281 Right-Handed   FALSE\n282 Right-Handed   FALSE\n283 Right-Handed   FALSE\n284 Right-Handed   FALSE\n285  Left-Handed   FALSE\n286 Right-Handed   FALSE\n287 Right-Handed   FALSE\n288 Ambidextrous    TRUE\n289  Left-Handed   FALSE\n290 Right-Handed   FALSE\n291  Left-Handed   FALSE\n292 Right-Handed   FALSE\n293 Right-Handed   FALSE\n294 Right-Handed   FALSE\n295 Right-Handed   FALSE\n296 Right-Handed   FALSE\n297 Right-Handed   FALSE\n298 Right-Handed   FALSE\n299  Left-Handed   FALSE\n300 Right-Handed   FALSE\n301 Right-Handed   FALSE\n302 Right-Handed   FALSE\n303 Right-Handed   FALSE\n304 Right-Handed   FALSE\n305 Right-Handed   FALSE\n306 Right-Handed   FALSE\n307  Left-Handed   FALSE\n308 Right-Handed   FALSE\n309 Right-Handed   FALSE\n310 Right-Handed   FALSE\n311 Right-Handed   FALSE\n312 Right-Handed   FALSE\n\n\nWe will show you why this is useful when we introduce tidyverse functions."
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Ceci n’est pas une pipe",
    "text": "Ceci n’est pas une pipe\nThe tidyverse organizes actions to data sequentially. We separate steps by what is called a “pipe” which is programmed %&gt;%.\nHINT: The shortkey for adding a “pipe” is ctrl+shift+m for Windows, and cmd+shift+m on Mac. Learn this because we use them a lot!"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Removing Rows - filter()",
    "text": "Removing Rows - filter()\nLogical operators are useful when removing rows from a dataset. The most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% is useful for defining a list of levels that you want to include\n\nWe typically begin with the raw dataset, then “pipe” that dataset into a sequence of functions using the “pipe” operator, %&gt;%.\nLet’s begin by filtering out rows we think have legitimate heights:\n\nsurvey %&gt;% \n  filter(Height_cm &lt; 214)\n\n# A tibble: 310 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     NC         2022         11 Male         17 Ambidextrous       175\n 8 USA     SC         2022         11 Female       18 Right-Handed       160\n 9 USA     WA         2022         11 Female       16 Right-Handed       156\n10 USA     WA         2022         12 Female       17 Right-Handed       169\n# ℹ 300 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nThe above code will return a new dataset without the outliers.\nHow many rows does the original dataset have?\nHow many rows does the filtered dataset have?\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# Equivalently we can use %in% instead of the !=\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\n# A tibble: 27 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed       182 \n 2 USA     MO         2022         11 Male         17 Right-Handed      181 \n 3 USA     MO         2022         11 Female       17 Right-Handed      151 \n 4 USA     FL         2022         12 Female       18 Right-Handed      170 \n 5 USA     FL         2022         12 Male         18 Right-Handed      180 \n 6 USA     FL         2022         12 Female       18 Right-Handed      173.\n 7 USA     FL         2022         12 Male         18 Right-Handed      175 \n 8 USA     FL         2022         12 Male         17 Right-Handed      180 \n 9 USA     MO         2022         11 Male         17 Right-Handed      164 \n10 USA     FL         2022         12 Female       17 Right-Handed      166 \n# ℹ 17 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\nsurvey$Handed %in% c(\"Left-Handed\", \"Right-Handed\")\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Adding Columns - mutate()",
    "text": "Adding Columns - mutate()\nThe mutate() statement is used to add new columns to a dataset.\nTo create a new column, “pipe” the previous steps into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: Create a column of the ratio of Height to armspan called, ht_to_span, by using a mutate() statement:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         ht_in = Height_cm / 2.54) %&gt;%\n  select(Handed, ht_to_span, ht_in)\n\nView(clean)\n\nNotice we no longer have to use $ to access specific columns! The tidyverse lives up to its name!"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Selecting Columns - select()",
    "text": "Selecting Columns - select()\nThere are now over 60 columns in this dataset. Suppose we are only interested in reaction times and height-to-armspan ratio as they related to handedness. To tidy up the data even further, select only the columns we are interested in (Handed, Reaction_time, and ht_to_span):\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n# A tibble: 302 × 3\n   Handed       Reaction_time ht_to_span\n   &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;\n 1 Left-Handed          0.349      1.28 \n 2 Right-Handed         0.358      0.990\n 3 Right-Handed         0.447      1.03 \n 4 Right-Handed         0.438      1.02 \n 5 Left-Handed          0.542      0.981\n 6 Right-Handed         0.428      0.968\n 7 Right-Handed         0.427      1.01 \n 8 Right-Handed         0.412      1.10 \n 9 Right-Handed         0.346      1.04 \n10 Right-Handed         0.391      1    \n# ℹ 292 more rows\n\n\nSee how much we can do in just a few short, sequential lines of code? Let’s name out clean dataset, clean, and create a boxplot of reaction times comparing left and right handed students:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n# Modify the code to remove outliers in Reaction_time and remake the boxplot\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Summarising Data - group_by() + summarise()",
    "text": "Summarising Data - group_by() + summarise()\nThe above data might be adequate for a visualization or analysis, but we can calculate summary statistics tables like we did with favstats() using the tidyverse.\nThe summarise() (or equivalently, summarize()) function is like the mutate statement. We create a name for the new column and set it equal to what we want.\nLet’s name the new dataset, clean, and see how to make summaries using tidyverse.\n\nboxplot(survey$Reaction_time)\n\n\n\n\n\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214, \n         Handed != \"Ambidextrous\", \n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0,\n         ClassGrade == 12) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n\n\n\n\n\n\nclean %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE)\n  )\n\n# A tibble: 1 × 3\n  mn_react_time med_react_time mn_ratio\n          &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1         0.397          0.366     1.27\n\n\nNotice that the mn_ratio is Inf.\nWhy might that be the case?\nModify the code chunk to exclude rows where arm span is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  summarise(\n    `Mean Reaction Time` = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    min_react_time = min(Reaction_time, na.rm=TRUE)\n  ) %&gt;% knitr::kable()\n\n\n\n\nMean Reaction Time\nmed_react_time\nmn_ratio\nmin_react_time\n\n\n\n\n0.4205068\n0.388\nInf\n0.067\n\n\n\n\nfavstats(clean$Reaction_time ~ clean$Handed)\n\n  clean$Handed   min      Q1 median      Q3   max      mean        sd   n\n1  Left-Handed 0.274 0.34825 0.4415 0.53850 0.895 0.4741471 0.1691585  34\n2 Right-Handed 0.067 0.33825 0.3845 0.44775 0.995 0.4134380 0.1302318 258\n  missing\n1       0\n2       0\n\nfavstats(clean$ht_to_span)\n\n    min        Q1   median       Q3 max mean  sd   n missing\n 0.0168 0.9841579 1.006519 1.052681 Inf  Inf NaN 292       0\n\n\nIf we want to get means for separate groups, we can add a group_by() statement to tell which variable(s) we want to group by:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  ) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nHanded\nmn_react_time\nmed_react_time\nmn_ratio\nmax_react\ncount\n\n\n\n\nLeft-Handed\n0.4741471\n0.4415\n1.266468\n0.895\n34\n\n\nRight-Handed\n0.4133891\n0.3840\n1.260190\n0.995\n257\n\n\n\n\n\nThe n() is very useful for counting up the number of observations in each group.\nIf we were only interested in the summary statistics table, we can do everything in one series of steps:\n\nsummary_stats_table &lt;-  survey %&gt;%\n  filter(\n    Height_cm &lt; 214,\n    Handed != \"Ambidextrous\",\n    Reaction_time &lt; 1,\n    Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span) %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  )\n\nsummary_stats_table\n\n# A tibble: 2 × 6\n  Handed       mn_react_time med_react_time mn_ratio max_react count\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Left-Handed          0.474          0.442     1.27     0.895    34\n2 Right-Handed         0.413          0.384     1.26     0.995   257"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html",
    "title": "Introducing GGPlot!",
    "section": "",
    "text": "GGPlot is a data visualization library that follows Leland Wilkinson’s Grammar of Graphics. The Grammar of Graphics is a systematic approach to how we think about connecting raw data to visual elements.\nThink about a basic sentence in English: The boy threw the ball. This sentence has a subject (the boy), a verb (threw), and a direct object (the ball). While not all sentences include every part of speech, virtually all sentences have at least a subject and a verb.\nThe grammar of graphics has 3 essential components of distinct graphical elements that are needed to make basic “sentences.” They are like the subjects and verbs of English sentences. These elements are:\n\nData layer\nAesthetic mappings\nGeometry layers\n\nThe data layer identifies the data we wish to express visually.\nThe Aesthetic Mapping is a description of how we map specific data elements to specific chart elements. For example, what variable in the data do we want expressed on the X axis or Y axis. We can also map a data variable to the color element.\nLastly, the Geometry Layer tells the computer how to express those Aesthetic Mappings, such as a scatter plot, boxplot, bar chart, etc.\nAs in English, we can make more complex sentences with other graphical elements, but the three mentioned above will be common to all.\nThis sounds more complicated than it is in practice. So let’s look at a familiar example: the Personality data.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#adding-color",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#adding-color",
    "title": "Introducing GGPlot!",
    "section": "Adding Color",
    "text": "Adding Color\nTo see the relationship between sepal length and width for each species separately, map the column Species onto the color element in the aesthetic mapping.\nBecause Species is a variable inside the dataset, we put it INSIDE the aes(). This maps Species onto the chart element, color.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo change ALL the points to a single color, include a “color” statement in the geometry layer:\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\")"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#more-additions",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#more-additions",
    "title": "Introducing GGPlot!",
    "section": "More Additions",
    "text": "More Additions\nIt is easy to make more interesting graphs that combine multiple geometries or even multiple data layers. For example, if I want to include a trend line on top of the points, simply add a new geometry. The geom_smooth() geometry can add different types of trend lines. We can specify method = 'lm' meaning “linear model” to get a simple line.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#further-customizations",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#further-customizations",
    "title": "Introducing GGPlot!",
    "section": "Further Customizations",
    "text": "Further Customizations\nWithout changing the underlying “grammar” we can change the “font,” so to speak. To modify the axis labels or add a title and a subtitle, use a labs() layer.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  )"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#themes",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#themes",
    "title": "Introducing GGPlot!",
    "section": "Themes",
    "text": "Themes\nAn easy way to change many visual elements all at once, ggplot() has several pre-packaged themes() you can add to a plot.\nWe typically want high contrast between data points and the background. This makes it easier to perceive differences. Changing the theme of the chart can make lots of changes all at once. theme_bw() is a useful theme which drops the gray default background.\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are more themes to try. If you begin typing theme_ you will see a drop down with several other themes.\nExplore some of the themes. Who can come up with the wildest visualization?"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#facets",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#facets",
    "title": "Introducing GGPlot!",
    "section": "Facets",
    "text": "Facets\nSometimes adding more things to a graph makes it too cluttered. When dealing with multiple groups, you may want to split the graph into several panels, one for each group.\nFacets allow us to split a graph up based on a variable in the data. For example, if we wanted a separate regression plot for each species, we could “add” a facet:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNotice that the x-axes are the same for each group by default. That is often how we want to visualize data. Sometimes, though, we want to have each graph only cover the range of the data. We can allow the x and y axes to accommodate different ranges of data by setting the “scales” parameter inside the facet_grid to “free”:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species, scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE: When we want to “add” something to a graph, we simply include a + and tell it what we want to add. If we want to learn more about any of the graphing elements and their customization, we can always use the question mark help prompts (eg. ?facet_grid)."
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#your-turn",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#your-turn",
    "title": "Introducing GGPlot!",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a side-by-side boxplot using the iris dataset that looks at the distribution of Sepal.Length for each species type.\nBe sure to:\n1. Color the boxes by Species 2. Add theme_bw() to make the chart more high contrast 3. Add a title Sepal Length by Species\nNOTE: Using color = Species with boxplots doesn’t look great. Try fill = Species instead. Using BOTH is not a good idea, but different combinations and see.\n\n\nggplot() + \n\nError: &lt;text&gt;:5:0: unexpected end of input\n3: \n4: \n  ^"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#conclusion",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#conclusion",
    "title": "Introducing GGPlot!",
    "section": "Conclusion",
    "text": "Conclusion\nGGplot provides many options for easily making complex visualizations. While there is far too much to cover in one lesson, the basic framework is fairly intuitive once you get the hang of it."
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#histograms-and-density-plots",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#histograms-and-density-plots",
    "title": "Introducing GGPlot!",
    "section": "Histograms and Density Plots",
    "text": "Histograms and Density Plots\nWhen we want to look at the distribution of a single variable, we typically use histograms. Because this is a single variable, we only define an x without a y.\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram() +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n# We can modify the number of bins in a histogram:  Play around with the \"bin\" Parameter\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nThe above histogram includes data from all species. We can distinguish species in several ways. One is to color the bars by species. Compare the difference between “color=Species” and “fill=Species” inside the aesthetic.\nWARNING: Would not recommend:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill=Species)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nIt’s not usually a good idea to layer histograms like this because it can obscure what is happening behind the covered layers. This is a situation where faceting can be useful.\nRecall that by default the x-axis will be fixed to the same values for each facet. We can let the x axis scale be different for each group by including scales = \"free\" into the facet_grid argument as above.\nTry both and see which tells a more compelling story:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 10) +\n  facet_grid(~Species) +\n  #facet_grid(~Species, scales = \"free\") +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\n\nA Better Histogram\nWhile histograms are a fine way to express the distribution of quantitative variables, it is not the only way. A Density plot is a smooth version of a histogram. Density plots use data to calculate a smooth line that expresses the quantitative variable as a continuous value rather than using crude bins.\nBy making the smooth line, it is much easier to compare between groups on the same plot:\n\nggplot(iris, mapping = aes(x = Sepal.Length, color = Species)) +\n  geom_density(linewidth = 1.2) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )"
  },
  {
    "objectID": "2-Tidy_Data/Bonus_More_Tidy_Practice.html",
    "href": "2-Tidy_Data/Bonus_More_Tidy_Practice.html",
    "title": "Level up your Tidy-ness",
    "section": "",
    "text": "Putting it All Together\nHere are two more data wrangling questions to test your skills. Try as best you can to work each step on your own before checking solutions.\nThe questions relate to the High School survey used in other examples.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nTelepaths, Gender and Sleep\nSuppose we want to see who gets more sleep on non-school nights, males or females whose chosen superpower would be telepathy. Also, create a column that is the ratio of sleep hours on non-school nights to 8. This calculates the percent of recommended sleep on non-school nights.\n\nCreate a dataset that includes columns Gender, Superpower, Sleep_Hours_Non-Schoolnight, and the ratio of non-schoolnight sleep hours divided by 8, for Males and Females who choose Telepathy as their superpower.\n\n\n# Your Code:\n\n\n\nSolution\n\n\nunique(survey$Superpower)\n\n[1] \"Telepathy\"      \"Invisibility\"   \"Fly\"            \"Freeze time\"   \n[5] \"Super strength\"\n\ntelepaths &lt;- survey %&gt;%\n  select(Gender, Superpower, Sleep_Hours_Non_Schoolnight) %&gt;%\n  filter(Superpower==\"Telepathy\") %&gt;%\n  mutate(\n    percent_of_recommended = Sleep_Hours_Non_Schoolnight / 8\n  )\n\ntelepaths\n\n# A tibble: 81 × 4\n   Gender Superpower Sleep_Hours_Non_Schoolnight percent_of_recommended\n   &lt;chr&gt;  &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Male   Telepathy                            7                  0.875\n 2 Female Telepathy                            9                  1.12 \n 3 Male   Telepathy                            9                  1.12 \n 4 Female Telepathy                            8                  1    \n 5 Female Telepathy                            9                  1.12 \n 6 Male   Telepathy                           11                  1.38 \n 7 Female Telepathy                           11                  1.38 \n 8 Female Telepathy                            9                  1.12 \n 9 Female Telepathy                            9                  1.12 \n10 Female Telepathy                           10                  1.25 \n# ℹ 71 more rows\n\n\n\n\nCreate a summary table comparing males and females whose preferred super power is telepathy that includes:\n\na. Mean, standard deviation, and sample size of Sleep Hours on non-school nights \nb. Mean, standard deviation, and sample size of the percent of recommended sleep\nHINT: Use the unique() function to see what the options are for a given categorical variable.\n\n# Your Code\n\n\n\nSolution\n\n\ntelepaths %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mn_hrs = mean(Sleep_Hours_Non_Schoolnight),\n    mn_percent_recommended = mean(percent_of_recommended),\n    count = n()\n  )\n\n# A tibble: 2 × 4\n  Gender mn_hrs mn_percent_recommended count\n  &lt;chr&gt;   &lt;dbl&gt;                  &lt;dbl&gt; &lt;int&gt;\n1 Female   8.73                   1.09    60\n2 Male     8.19                   1.02    21\n\n\n\n\n\nVegetarians and Height\n\nHow many vegetarians say meat is their favorite food?\n\nHINT: This can be done with a single filter statement\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  filter(Favorite_Food == \"Meat\",\n         Vegetarian == \"Yes\")\n\n# A tibble: 1 × 60\n  Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n  &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 USA     NC         2022         11 Male         16 Right-Handed       178\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;,\n#   Imprtance_wning_cmputer &lt;int&gt;, Imprtance_Internet_access &lt;int&gt;, …\n\n\n\n\nCompare mean, and standard deviation of heights between those who are vegetarian and those who aren’t. Include the number of respondents in your analysis.\n\nBe sure to filter out any major outliers in heights first.\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100) %&gt;%\n  group_by(Vegetarian) %&gt;%\n  summarise(\n    med_ht = median(Height_cm),\n    mean_ht = mean(Height_cm),\n    sd_ht = sd(Height_cm),\n    count = n()\n  )\n\n# A tibble: 2 × 5\n  Vegetarian med_ht mean_ht sd_ht count\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 No           170.    171.  10.8   285\n2 Yes          163     162.  17.1    15\n\n\nAfter removing outliers, it looks like vegetarians are shorter, on average.\n\n# Bonus Boxplot:\nveg &lt;- survey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100)\n\n\nboxplot(veg$Height_cm ~ veg$Vegetarian, col = c(5,6), main = \"Heights (cm) of Vegetarians and Non-Vegetarians\", xlab=\"vegetarian\", ylab = \"Height (cm)\")\n\n\n\n\n\n\n\n\n\n\nCreate a dataset that:\n\nIncludes a column that is percent of recommended sleep (Sleep_Hours_Schoolnight divided by 8 using a mutate statement)\nIncludes only columns for Favourite_physical_activity, Reaction_time, percent_recommended_sleep (part a)\nIncludes only students whose favorite physical activity is Walking/Hiking, Basketball, Swimming, Soccer\nFilters Reaction Times to be less than 1 second\n\n\n\n# Your Code:\n\n\n\nSolution\n\n\nphys_act &lt;- survey %&gt;%\n  mutate(\n    pct_recommended_sleep = Sleep_Hours_Schoolnight / 8\n  ) %&gt;%\n  filter(Favourite_physical_activity %in% c('Walking/Hiking', \"Basketball\", \"Swimming\", \"Soccer\"),\n         Reaction_time &lt; 1) %&gt;%\n  select(Favourite_physical_activity, Reaction_time, pct_recommended_sleep)\n\n\nUse the clean dataset to:\n\nCreate a side-by-side boxplot for the percent of recommended sleep comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$pct_recommended_sleep ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"% Recommended Sleep on School Nights\", main = \"School Night Sleep by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\n\nCreate a side-by-side boxplot for the reaction times comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$Reaction_time ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"Reaction Time\", main = \"Reaction Time Results by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\nWhich physical activity group has the quickest reaction time?"
  },
  {
    "objectID": "3-Stat_Fundamentals/01-Intro_to_Probability.html",
    "href": "3-Stat_Fundamentals/01-Intro_to_Probability.html",
    "title": "Intro to Probability (Reading)",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material.\nTo put some perspective on this, a high school athlete in the United States is over 26 times more likely to play professional sports than Columbus was to draw the three marked peas! (Fields, Mike 2008) Consider how you might explain the occurrence of this extremely unlikely event. (While no response is required of you right now, this kind of question will be very important later, so take a little time to ponder it.) In fact, it is worth restating the question, “How might you explain the occurrence of this extremely unlikely event?”\nNow, take a moment to practice what you have read by answering the following questions.\n\nAnswer the following questions:\n\n\n\nIf a fair, six-sided die* is rolled, what is the probability of rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a 6 on a die is \\(\\displaystyle{ \\frac{1}{6}} = 0.1667\\). This is because a six-sided die has 6 sides total and only 1 side that has a “6” on it. Thus, 1/6 represents the chance of getting a “six” divided by the “total number of possibilities on the die” in the denominator (6).\n\n\n\nIf a fair, six-sided die is rolled, what is the probability of not rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of not rolling a 6 on a die is \\(\\displaystyle{\\frac{5}{6}} = 0.8333.\\) This is because there are five sides on the die that are “not a 6” (the sides representing 1, 2, 3, 4, and 5) and six total sides, so 5 out of 6 sides will yield something other than a “six.”\n\n\n\nWhen a die is rolled, what is the sum of the probability of rolling a 6 and the probability of not rolling a six?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a six is \\(\\displaystyle{\\frac{1}{6}}\\) and the probability of not rolling a six is \\(\\displaystyle{\\frac{5}{6}}\\). These things cannot happen at the same time, so the probability of either rolling a six or not rolling a six is \\[ \\frac{1}{6} + \\frac{5}{6} = \\frac{6}{6} = 1 \\]\n\n*The only possible outcomes in this case are that you either roll a six or that you do not roll a six. The probability that one of these will happen is 1. If we list all the possible outcomes, the probability that at least one of them will occur is 1.\n\n\nIn general, if we know the probability that a particular outcome will occur, how could we calculate the probability that it will not occur?\n\n\n\nShow/Hide Solution\n\n\nIf we know the probability that an event will occur, we can subtract this probability from 1 to find the probability that the event will not occur. This is because the sum of the probabilities that the event will occur and that the event will not occur must be 1.\n\n\n\\(*\\)Note: The word “die” is the singular form of the word “dice.” When we refer to a die, we are talking about a fair, six-sided die.\n\n\n\n\n\nYou may already have a good understanding of the basics of probability. It is worth noting that there is a special notation used to denote probabilities. The probability that an event, \\(x\\), will occur is written \\(P(x)\\) and pronounced as “probability-of-event-x.” As an example, the probability that you will roll a 6 on a fair six-sided die can be written as\n\\[\nP\\text{(Roll a \"six\" on a fair six-sided die)}= \\frac{1}{6} = \\frac{\\text{number of sides that show a \"six\"}}{\\text{total number of sides on the die}}\n\\]\n\n\n\nProbabilities follow patterns, called probability distributions, or just distributions, for short. There are three rules that a probability distribution must follow. Answer the following questions to explore what these three rules might be.\n\nAnswer the following questions:\n\n\n\nAs an answer to a homework problem, a student reported, The probability of finding life on Mars is -0.35. What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be negative because they represent the “number of ways something specific can happen” (like how many planets that are “just like” Mars and do have life on them) divided by “the total number of possibilities” (in this case, the total number of planets that exist that are “just like” Mars). The fewest planets there could be that are “just like” Mars and have life on them is zero. So the lowest a probability can go is zero.\n\n\n\nA student in an introductory statistics class wrote the following statement on an exam: The probability that the event will occur is 1.25 (i.e. 125%). What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be larger than 1 (or 100%). This is because probabilities represent frequencies of occurrence, and the most something can happen is “all the time” or 100% of the time.\n\n\n\nA student estimated that the probability that he would finish his homework is 0.45 (i.e., 45%), and the probability that he would not finish his homework is 0.35 (i.e., 35%). What is wrong with this student’s statement?\n\n\n\nShow/Hide Solution\n\nThis can be viewed as one of two problems:\n\nThe probabilities for all the events do not add up to 1 (or 100%.)\nThe probability that he does not finish his homework is actually 1 minus the probability that he will finish his homework or 0.55 (i.e., 55%).\n\n\n\n\nIn this course we are interested in experiments where the outcomes of the experiment are uncertain, yet they follow a pattern or probabilitiy distribution. As you read in the above questions and answers, these probability distributions follow three rules.\n\n  The three rules of probability are:\n\nRule 1: The probability of an event \\(X\\) is a number between 0 and 1.\n\n\\[0 \\leq P(X) \\leq 1\\]\n\nRule 2: If you list all the outcomes of an experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities of all the possible outcomes of any experiment is 1.\n\n\\[\\sum P(X) = 1\\]\n\nRule 3: (Complement Rule) The probability that an event \\(X\\) will not occur is 1 minus the probability that it will occur.\n\n\\[P(\\text{not}~X) = 1 - P(X)\\]\n  You may have noticed that the Complement Rule is just a combination of the first two rules.\n\n\n\nAnswer the following questions:\n\n\n\nWhich of the probability rules was violated by the statement in Question 10?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 11?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 12?\n\n\n\nShow/Hide Solution\n\n\nRule 2 or Rule 3\n\n\n\n\nInformally, a distribution can be thought of as being “all the possible outcomes of an experiment and how often they occur.”\n\n\n\nA BYU-Idaho student was overhead saying, “I went shopping and bought some random items.” Did the person actually take a random sample of the items at the store? Did they write all the items down and randomly select the items for purchase? Of course not!\nWhat did the student mean? That the items they bought seemed unrelated. When we consciously or subconsciously choose a sample, it is not random.\nSo, what does it mean to be random? When something is random, it is not just haphazard, with no pattern. A random process follows a very distinct pattern over time—the distribution of its outcomes. For example, if you roll a die thousands of times, about one-sixth of the time you will roll a four. This is a very clear pattern, or part of a pattern. The entire pattern (or, the entire distribution) is that each number on the die is rolled about one-sixth of the time.\nBut there’s something different about the patterns followed by random processes than other kinds of patterns. Other kinds of patterns can be very predictable, such as a color pattern of the red, yellow, blue, red, yellow, blue, and so on. If you’re following this pattern and happen to see yellow, you know the next color will be blue. By contrast, you never know what you will get on the next roll of a six-sided die. You do know that in the long run you will roll fours about one-sixth of the time.\nWhen something is random, we can be sure that it follows a long-term pattern. This long-term pattern is called its probability distribution. However, what makes “randomness” interesting is that despite knowing the long-term pattern, or how often something will occur over time, we still never know what the outcome of the next experiment will be."
  },
  {
    "objectID": "3-Stat_Fundamentals/01-Intro_to_Probability.html#probability",
    "href": "3-Stat_Fundamentals/01-Intro_to_Probability.html#probability",
    "title": "Intro to Probability (Reading)",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material.\nTo put some perspective on this, a high school athlete in the United States is over 26 times more likely to play professional sports than Columbus was to draw the three marked peas! (Fields, Mike 2008) Consider how you might explain the occurrence of this extremely unlikely event. (While no response is required of you right now, this kind of question will be very important later, so take a little time to ponder it.) In fact, it is worth restating the question, “How might you explain the occurrence of this extremely unlikely event?”\nNow, take a moment to practice what you have read by answering the following questions.\n\nAnswer the following questions:\n\n\n\nIf a fair, six-sided die* is rolled, what is the probability of rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a 6 on a die is \\(\\displaystyle{ \\frac{1}{6}} = 0.1667\\). This is because a six-sided die has 6 sides total and only 1 side that has a “6” on it. Thus, 1/6 represents the chance of getting a “six” divided by the “total number of possibilities on the die” in the denominator (6).\n\n\n\nIf a fair, six-sided die is rolled, what is the probability of not rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of not rolling a 6 on a die is \\(\\displaystyle{\\frac{5}{6}} = 0.8333.\\) This is because there are five sides on the die that are “not a 6” (the sides representing 1, 2, 3, 4, and 5) and six total sides, so 5 out of 6 sides will yield something other than a “six.”\n\n\n\nWhen a die is rolled, what is the sum of the probability of rolling a 6 and the probability of not rolling a six?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a six is \\(\\displaystyle{\\frac{1}{6}}\\) and the probability of not rolling a six is \\(\\displaystyle{\\frac{5}{6}}\\). These things cannot happen at the same time, so the probability of either rolling a six or not rolling a six is \\[ \\frac{1}{6} + \\frac{5}{6} = \\frac{6}{6} = 1 \\]\n\n*The only possible outcomes in this case are that you either roll a six or that you do not roll a six. The probability that one of these will happen is 1. If we list all the possible outcomes, the probability that at least one of them will occur is 1.\n\n\nIn general, if we know the probability that a particular outcome will occur, how could we calculate the probability that it will not occur?\n\n\n\nShow/Hide Solution\n\n\nIf we know the probability that an event will occur, we can subtract this probability from 1 to find the probability that the event will not occur. This is because the sum of the probabilities that the event will occur and that the event will not occur must be 1.\n\n\n\\(*\\)Note: The word “die” is the singular form of the word “dice.” When we refer to a die, we are talking about a fair, six-sided die.\n\n\n\n\n\nYou may already have a good understanding of the basics of probability. It is worth noting that there is a special notation used to denote probabilities. The probability that an event, \\(x\\), will occur is written \\(P(x)\\) and pronounced as “probability-of-event-x.” As an example, the probability that you will roll a 6 on a fair six-sided die can be written as\n\\[\nP\\text{(Roll a \"six\" on a fair six-sided die)}= \\frac{1}{6} = \\frac{\\text{number of sides that show a \"six\"}}{\\text{total number of sides on the die}}\n\\]\n\n\n\nProbabilities follow patterns, called probability distributions, or just distributions, for short. There are three rules that a probability distribution must follow. Answer the following questions to explore what these three rules might be.\n\nAnswer the following questions:\n\n\n\nAs an answer to a homework problem, a student reported, The probability of finding life on Mars is -0.35. What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be negative because they represent the “number of ways something specific can happen” (like how many planets that are “just like” Mars and do have life on them) divided by “the total number of possibilities” (in this case, the total number of planets that exist that are “just like” Mars). The fewest planets there could be that are “just like” Mars and have life on them is zero. So the lowest a probability can go is zero.\n\n\n\nA student in an introductory statistics class wrote the following statement on an exam: The probability that the event will occur is 1.25 (i.e. 125%). What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be larger than 1 (or 100%). This is because probabilities represent frequencies of occurrence, and the most something can happen is “all the time” or 100% of the time.\n\n\n\nA student estimated that the probability that he would finish his homework is 0.45 (i.e., 45%), and the probability that he would not finish his homework is 0.35 (i.e., 35%). What is wrong with this student’s statement?\n\n\n\nShow/Hide Solution\n\nThis can be viewed as one of two problems:\n\nThe probabilities for all the events do not add up to 1 (or 100%.)\nThe probability that he does not finish his homework is actually 1 minus the probability that he will finish his homework or 0.55 (i.e., 55%).\n\n\n\n\nIn this course we are interested in experiments where the outcomes of the experiment are uncertain, yet they follow a pattern or probabilitiy distribution. As you read in the above questions and answers, these probability distributions follow three rules.\n\n  The three rules of probability are:\n\nRule 1: The probability of an event \\(X\\) is a number between 0 and 1.\n\n\\[0 \\leq P(X) \\leq 1\\]\n\nRule 2: If you list all the outcomes of an experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities of all the possible outcomes of any experiment is 1.\n\n\\[\\sum P(X) = 1\\]\n\nRule 3: (Complement Rule) The probability that an event \\(X\\) will not occur is 1 minus the probability that it will occur.\n\n\\[P(\\text{not}~X) = 1 - P(X)\\]\n  You may have noticed that the Complement Rule is just a combination of the first two rules.\n\n\n\nAnswer the following questions:\n\n\n\nWhich of the probability rules was violated by the statement in Question 10?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 11?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 12?\n\n\n\nShow/Hide Solution\n\n\nRule 2 or Rule 3\n\n\n\n\nInformally, a distribution can be thought of as being “all the possible outcomes of an experiment and how often they occur.”\n\n\n\nA BYU-Idaho student was overhead saying, “I went shopping and bought some random items.” Did the person actually take a random sample of the items at the store? Did they write all the items down and randomly select the items for purchase? Of course not!\nWhat did the student mean? That the items they bought seemed unrelated. When we consciously or subconsciously choose a sample, it is not random.\nSo, what does it mean to be random? When something is random, it is not just haphazard, with no pattern. A random process follows a very distinct pattern over time—the distribution of its outcomes. For example, if you roll a die thousands of times, about one-sixth of the time you will roll a four. This is a very clear pattern, or part of a pattern. The entire pattern (or, the entire distribution) is that each number on the die is rolled about one-sixth of the time.\nBut there’s something different about the patterns followed by random processes than other kinds of patterns. Other kinds of patterns can be very predictable, such as a color pattern of the red, yellow, blue, red, yellow, blue, and so on. If you’re following this pattern and happen to see yellow, you know the next color will be blue. By contrast, you never know what you will get on the next roll of a six-sided die. You do know that in the long run you will roll fours about one-sixth of the time.\nWhen something is random, we can be sure that it follows a long-term pattern. This long-term pattern is called its probability distribution. However, what makes “randomness” interesting is that despite knowing the long-term pattern, or how often something will occur over time, we still never know what the outcome of the next experiment will be."
  },
  {
    "objectID": "3-Stat_Fundamentals/01-Intro_to_Probability.html#conclusion",
    "href": "3-Stat_Fundamentals/01-Intro_to_Probability.html#conclusion",
    "title": "Intro to Probability (Reading)",
    "section": "Conclusion",
    "text": "Conclusion\nAs with all the classes you take at BYU-Idaho, it is up to you to decide what you want to get out of this class. If you choose to approach the things you study in class with an open mind, if you prepare diligently and work hard to complete all the learning activities, and if you humbly seek the Lord’s help to understand the intellectual and spiritual truths discussed in this course and in other courses, you will have an outstanding educational experience that will be a blessing to you throughout your life. May you enjoy the journey this semester into statistics!"
  },
  {
    "objectID": "3-Stat_Fundamentals/01-Intro_to_Probability.html#summary",
    "href": "3-Stat_Fundamentals/01-Intro_to_Probability.html#summary",
    "title": "Intro to Probability (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nIn this class you will use the online textbook that has been written for you by your statistics teachers. All of the assignments and quizzes, available in I-Learn, will be based on the readings, so study it well. Most weeks will cover two lessons.\nYou have successfully located the online textbook. Ensure you have also located the course in I-Learn and can access the quizzes and assignments that are there.\nEnsure you have located the contact information for your instructor in the I-Learn course. Recording the contact information of peers from class would also be a wise idea.\nThis course uses R for all statistical analysis. Check that you have access to the software on your computer. If not, see I-Learn for details on how to obtain it through the University for free.\nBy doing the work, staying on schedule, and living the Honor Code you will succeed in this class.\nThe three rules of probability are:\n\nA probability is a number between 0 and 1. \\[0 \\leq P(X) \\leq 1\\]\nIf you list all the outcomes of a probability experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities in any probability is 1. \\[\\sum P(X) = 1\\]\nThe probability that an outcome will not occur is 1 minus the probability that it will occur. \\[P(\\text{not}~X) = 1 - P(X)\\]"
  },
  {
    "objectID": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html",
    "href": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html",
    "title": "The Normal Distribution (Class)",
    "section": "",
    "text": "In your reading, you learned about the normal distribution which is a probability model that can calculate probabilities for certain types of events that follow a normal distribution."
  },
  {
    "objectID": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#the-math",
    "href": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#the-math",
    "title": "The Normal Distribution (Class)",
    "section": "The Math",
    "text": "The Math\nWe can calculate a Z-score using the formula\n\\[Z = \\frac{x - \\mu}{\\sigma} \\]\nwhere \\(\\mu\\) is the mean of the normal distribution and \\(\\sigma\\) is the standard deviation.\nNOTE: Z-scores follow what is called a standard normal distribution which means it is centered at \\(\\mu = 0\\) with a standard deviation \\(\\sigma=1\\).\nRecall that to get an area under the curve, we need the calculus. Fortunately, we have computers to do the heavy lifting for us."
  },
  {
    "objectID": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#pnorm",
    "href": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#pnorm",
    "title": "The Normal Distribution (Class)",
    "section": "pnorm()",
    "text": "pnorm()\nWe will use the function pnorm() to calculate the areas under the curve for specified values. The p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nThe pnorm(x, mu, sigma) function takes the value, x, we wish to evaluate, the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). We can use pnorm() in the original units of the data and put in \\(\\mu\\) and \\(\\sigma\\). Sticking with our example of Stanford admissions, we can calculate the probability of getting a value greater than 33.\nBy default, pnorm() gives the area to the LEFT of the given value. If we want the area to the right, we can use the lower.tail = FALSE or equivalently 1-pnorm(33,21,4).\n\n1-pnorm(33,21,4)\n\n[1] 0.001349898\n\n# Equivalently:\npnorm(33, 21,4, lower.tail = FALSE)\n\n[1] 0.001349898\n\n\nEquivalently, we can put the z-score into pnorm()\nIt’s easy to create a calculator in R that will calculate Z for us and probabilities automatically.\nNOTE: Z is what we call the Standard Normal Distribution. It has a mean of 0 and a SD of 1. If X is normally distributed, subtracting the mean and dividing by the standard deviation gives the standard normal distribution with a mean of 0 and an SD = 1.\n\n# To calculate a z-score, you can use R like a calculator:\n\nx &lt;- 33\nmu &lt;- 21\nstdev &lt;- 4\nz &lt;- (x-mu) / stdev\n\nz\n\n[1] 3\n\n# By default, pnorm() gives us the area to the LEFT of our observation\n\npnorm(x, mean = mu, sd = stdev)\n\n[1] 0.9986501\n\n1 - pnorm(x, mean = mu, sd = stdev)\n\n[1] 0.001349898\n\n# Equivalently:  \n\npnorm(z) ## Left Tail\n\n[1] 0.9986501\n\n1-pnorm(z) ## Right Tail\n\n[1] 0.001349898"
  },
  {
    "objectID": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#iq-scores",
    "href": "3-Stat_Fundamentals/03-Normal_Distribution_Workbook.html#iq-scores",
    "title": "The Normal Distribution (Class)",
    "section": "IQ Scores",
    "text": "IQ Scores\nIf IQ is normally distributed with a mean of 100 and a Standard Deviation of 11, what’s the probability of a randomly selected person having an IQ GREATER than 127?\nWhat about the probability of a randomly selected person having an IQ LESS than 85?\n\nx &lt;- \nmu &lt;- \nstdev &lt;- \n\nz &lt;- (x-mu) / stdev\n  \npnorm(z)  # LESS THAN\n\n[1] 0.9986501\n\n1-pnorm(z) # GREATER THAN\n\n[1] 0.001349898"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution of the sample mean for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#lesson-outcomes",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#lesson-outcomes",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution of the sample mean for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#introduction",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#introduction",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Introduction",
    "text": "Introduction\nThis lesson introduces three important concepts of statistical theory:\n\nThe Sampling Distribution of the Sample Mean\nThe Central Limit Theorem\nThe Law of Large Numbers\n\nIn practice, a single sample of data is obtained from a population of interest in order to make inference about the entire population. However, even though only one sample is obtained, there are many, many random samples that are possible to obtain from a population. Understanding what could happen from a theoretical perspective is important in knowing how to use the single sample of data appropriately when making inference about a population. Let’s look at an example."
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#case-study-ddts-negative-impact-on-pregnant-women",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#case-study-ddts-negative-impact-on-pregnant-women",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Case Study: DDT’s Negative Impact on Pregnant Women",
    "text": "Case Study: DDT’s Negative Impact on Pregnant Women\n\n\nStep 1: Design the Study\nMosquitoes and other biting insects carry malaria. In an attempt to curb the spread of malaria and save lives, the pesticide DDT was used for many years to control the insect population, even indoors. Unfortunately, this pesticide does not break down quickly in nature and is very harmful to humans.\nA metabolite of DDT (called DDE) is also very dangerous. A metabolite is the byproduct that occurs when our body breaks down a substance. When DDT is broken down in humans, DDE is one of the metabolites that remains from the original DDT.\nScientists have shown that DDT and its metabolites cause reproductive problems in humans and other animals. When a pregnant woman has a contamination level as low as 10 mg/kg of DDE in her body, she is much more likely to give birth to an underweight baby or to deliver prematurely (Wells & Leonard, 2006).\nA study of 45 pregnant women who have been exposed to DDT / DDE was conducted using a convenience sample (Bornman, MS. 2005). Global Solutions Unlimited (GSU) will analyze the data from this study. These data will be used to assess the prevalence of DDT in drinking water.\n\nStep 2: Collect Data\nAfter selecting specific women to be sampled, researchers measured the level of DDE contamination for each of these randomly selected women.\n\nStep 3: Describe the Data\nThe researchers computed the mean and standard deviation of the observed levels of exposure for the 45 women. They found the mean observed contamination level was 24.75 mg/kg, and the highest level of contamination was 419.91 mg/kg! (Bornman, MS. 2005)\n\nStep 4: Make Inferences\nThe researchers only obtained one sample mean, \\(\\bar x\\). They do not get to see any other data. They only had enough time, funding, and other resources to survey the 45 women in their sample. An important question to ask at this point is whether or not the results of this sample show evidence that the mean DDE contamination in the full population of South American women was greater than 10 mg/kg.\nIf the researchers had selected a different random sample of pregnant women, the mean contamination of the women in the sample would undoubtedly be different. However, this did not happen. The researchers do not get to see the contamination levels for any other women. They only collected this one sample.\nThis is how research is often conducted…repeated samples are not typically drawn, so we do not get to know what might have happened had we gathered data from a different sample. The good news is that statistical theory provides us with the tools we need to be able to use our single sample in a meaningful way to make inference about the full population.\n\nStep 5: Take Action\nIn this study, there was a lot of evidence that the mean DDE contamination in the pregnant women was greater than 10 mg/kg. The probability of the DDE contamination in the women’s bodies exceeding 10 mg/kg (if there were no contamination) is approximately 0.02. This probability is low enough that there is reason for concern. Statistically speaking, there is sufficient evidence to suggest that the levels of DDE contamination are too high among South African women."
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#introduction-to-sampling-distributions",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#introduction-to-sampling-distributions",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Introduction to Sampling Distributions",
    "text": "Introduction to Sampling Distributions\nImagine that each student in the classroom replicated the study described above. Each student sampled 45 women and measured the level of contamination. Each student calculates a sample mean for their own samples. Now imagine making a histogram with everyone’s sample means. The distribution of sample means is an example of a sampling distribution.\n\nNOTE: Any sample statistic we calculate has a sampling distribution because of the inherent randomness from sample to sample, even standard deviations! In this chapter, we focus on the distribution of sample means, though later on in the semester we will consider the distribution of sample proportions as well.\n\nRecall that the sample mean is denoted by the symbol \\(\\bar{x}\\). The mean is computed by adding up all of the values in the sample and dividing by the number of things in the sample.\nRecall also that the true mean of the full population is denoted by the symbol \\(\\mu\\) and that the standard deviation of the population is denoted by \\(\\sigma\\).\nNow, with those reminders in place, you will need to work through the following tutorial to come to understand the idea of a “sampling distribution of a sample mean.”\n\nA Tutorial on Sampling Distributions\n\nStep 1: Open this applet and click on “Begin”\n\nStep 2: Ensure your internet browser window is wide enough to show all of the applet controls.\n\n\n\n\nInternet browser window is too small. (Bad)\n\n\nInternet browser window large enough to see all controls. (Good)\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Press the “Animated” button on the right of the applet. It will drop down a single random sample of N=5 data points (i.e., black boxes) from the parent population. (Because the sample is random, your sample will possibly land in a different place than what is shown here.)\n\n\n\nStep 4: Notice that a little blue box dropped down from your “Sample Data” of N=5 black boxes (data points). This little blue box represents the mean, \\(\\bar{x}\\), of your single sample of N=5 boxes.\n\n\nNow is where things get interesting. In real life, all we would use is the single sample of N=5 values and the sample mean, \\(\\bar{x}\\), to make inference (make our guess) about the mean, \\(\\mu\\), of the full population. (The full population in this applet is shown in the top black graph.) However, the purpose of this applet is to allow us to explore how our answer would change if we used a different sample of N=5 data points. A different sample will result in a different sample mean \\(\\bar{x}\\). So the question is, what values are possible for the sample mean? Are some values of the sample mean more likely than others?\n\nStep 5: Click the Animated button several more times. Each time, watch how a new random sample of N=5 data points is generated for the “Sample Data” graph, and then the mean of that sample is plotted in the bottom “Distribution of Means, N=5” graph. The following image shows the results after clicking the “Animated” button 10 times.\n\n\n\nStep 6: Now, let’s get the number of sample means (blue boxes) up to a million! To save time, there are buttons below the “Animated” button labeled “5”, “10,000”, and “100,000”. Clicking the “100,000” button 10 times will quickly get us up to one million sample means. Keep in mind that by doing this, you are essentially clicking the “Animated” button 1,000,000 times! (To actually click the “Animated” button one million times it would take you roughly 26 days of constantly clicking the “Animated” button over and over again!)\n\nNow study the arrows in the image below. As you do, you should notice that to the left of the “Distribution of Means, N=5” graph, the mean of the distribution of sample means (which is equal to 16) and standard deviation of the distribution of sample means (which is equal to 2.24) are given. Compare these values to the mean and standard deviation shown for the Parent Population graph.\n\nHopefully you noticed that the “mean of the distribution of means” is equal to the “mean of the population.” The mean of the means is the mean! Let’s say that again. The mean of all possible sample means is equal to the mean of the population. In short, the mean of the means is the mean. As for the standard deviation though, notice that the “standard deviation of the sample means” is smaller than the “standard deviation of the population.” It turns out that these two results hold true for any parent population and any sample size. There are two mathematical formulas that summarize these truths.\n\nThe Mean of the Sampling Distribution of the Sample Mean\n\\[\n\\underbrace{\\mu_{\\bar{x}}}_\\text{The mean of the sample means} = \\underbrace{\\mu}_\\text{The mean of the population}\n\\]\nThe Standard Deviation of the Sampling Distribution of the Sample Mean\n\\[\n\\underbrace{\\sigma_{\\bar{x}}}_\\text{The st. dev. of the sample means} = \\frac{\\overbrace{\\sigma}^\\text{The st. dev. of the population}}{\\underbrace{\\sqrt{n}}_\\text{sample size of each sample}}\n\\]\n\nThe Shape of the Sampling Distribution of the Sample Mean\nNotice that the shape of the sampling distribution of the sample mean is normal in the experiment we just performed in the above applet. This is an important result because we will be able to use the Normal Probability Applet to make probability calculations about sample means whenever the distribution of sample means is normal. Unfortunately, there are some situations where the distribution of sample means is not normal. This ruins our ability to make inference about the parent population. Let’s look at one such situation.\n\nStep 7: Press the [Clear Lower 3] button. This will erase your data and reset the applet. Then, use the drop-down menu below the [Clear Lower 3] button to change the distribution from a normal distribution to a skewed distribution. The applet should now show a right-skewed distribution for the “Parent Population” data graph.\n\n\n\nStep 8: Change the sample size to a very small sample size by selecting “N=2”. Then press the “Animated” button a few times to get a feel for what a sample of N=2 data points looks like from this right-skewed population.\n\n\n\nStep 9: Then, get a million such samples by clicking the “100,000” button ten times. You should get something similar to what is shown below. (Notice that the mean of the “Distribution of Means” is still equal to the mean of the “Parent population.” Also note that the standard deviation of the “Distribution of Means” is equal to \\(6.22/\\sqrt(2) = 4.40\\), which is \\(\\sigma/\\sqrt(n)\\), as shown by our formula stated previously for the “Standard deviation of the Sample Means.”)\n\n\nHowever, when it comes to the shape of the distribution of means, you should notice that the blue graph showing the “Distribution of Means, N=2” is not normally distributed. Like the distribution of the “Parent population” it is also right-skewed. This is problematic when it comes to trying to perform statistical analysis on a small sample (like N=2) from a population of skewed data because we can’t use the normal probability applet for things that are skewed. But there is a beautiful promise in statistical theory called the Central Limit Theorem that solves this problem. All we need is a larger sample size and the skewed distribution of means will magically “go away.” We will state this result formally in a moment. For now, see it happen yourself by doing the following.\n\nStep 10: Change the sample size from “N=2” (a small sample) to “N=25” (a much larger sample). Then, click the “Animated” button a few times to get a feel for what is happening. Finally, click the “100,000” button ten times to get a million sample means from a million different samples of “N=25” data points in each sample. Look at how beautifully normal the distribution of sample means (the blue graph) is becoming! The “Parent population” data is still skewed. Even the “Sample Data” is still skewed. But the “Distribution of Means” is becoming normal! This is the Central Limit Theorem in action.\n\n\n\n\n\nWhat is the Sampling Distribution of Sample Mean?\nThe sampling distribution of the sample mean is the set of all possible values of \\(\\bar x\\) that could occur. You have seen several examples of sampling distributions as you have plotted many means in the simulations and observed the approximately normal distribution that occurs. In the real world, you only observe your sample mean. You do not get to view the distribution. However, the fact that you sample randomly means that you could easily have drawn a different sample and had a different sample mean, \\(\\bar x\\). There are many possible sample means!"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#the-central-limit-theorem",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#the-central-limit-theorem",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large.\n\nIn the study of DDE levels in the South African women, we never saw the distribution of sample means. We only observed one sample mean, \\(\\bar x\\). Even though we do not get to see the distribution of all possible sample means, since the sample size (\\(n = 45\\)) was large, we can be assured that the sample mean \\(\\bar x\\) can be considered as one drawn from a normally distributed population of possible sample means. This allows us to make powerful statistical inference about the population.\nYou have observed in the simulations that if the sample size is large, the random variable \\(\\bar x\\) will be approximately normally distributed. In other words, the sampling distribution of the sample mean will be approximately normal if the sample size is sufficiently large. This important result is called the Central Limit Theorem. This is arguably the most important concept in all of Statistics.\nThe Central Limit Theorem works for every population where the standard deviation is defined (is not infinite). In other words, it will work for any distribution you find in the real world.\n\n\nWhen is \\(\\bar x\\) Normal?\nHow many observations are required so that the Central Limit Theorem will assure that the distribution of sample means will be approximately normal? The answer is, “it depends.”\n\nIf the parent population is normal, then the sampling distribution of \\(\\bar x\\) will always be normally distributed, no matter how many observations are selected.\nIf the parent population is not normal, then the sampling distribution \\(\\bar x\\) will be approximately normally distributed, if the sample size is large enough.\n\nIf the parent population is almost normal (e.g. mound-shaped and nearly symmetrical), then a sample of size n = 5 will probably be sufficient to assure that \\(\\bar x\\) will be approximately normally distributed.\nIf the parent population is heavily skewed, then it will require a larger sample size to be assured that \\(\\bar x\\) will be normally distributed. For most moderately skewed distributions, a sample size of around 30 is traditionally thought to be sufficiently large to assure that \\(\\bar x\\) will be approximately normally distributed. This is not a definitive number but is a rule of thumb.\nFor tremendously skewed distributions (e.g., the distribution of lottery payouts), a much larger sample will be required before the distribution of sample means is approximately normal. This may require billions of observations. Simulation can be used to determine if a particular sample size is sufficient. For this course, if the sample size is at least 30, we will conclude that the sampling distribution of \\(\\bar x\\) will be approximately normal.\n\n\n\nAnswer the following question:\n\n\n\nThere are two ways that \\(\\bar x\\) can be (approximately) normally distributed. What are they?\n\n\n\nShow/Hide Solution\n\n\n\\(\\bar x\\) will be normally distributed if the data were drawn from a normal population.\n\\(\\bar x\\) will be approximately normally distributed if the sample size is sufficiently large."
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Mean and Standard Deviation of the Distribution of Sample Means",
    "text": "Mean and Standard Deviation of the Distribution of Sample Means\nThe following facts are always true. They do not depend on the Central Limit Theorem. They do not depend on the sample size. These facts hold for the sample mean \\(\\bar x\\) of any simple random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\nThe mean of the sample means is \\(\\mu\\)\nThe standard deviation of the sample means is \\(\\sigma / \\sqrt{n}\\)\n\nRemember, these facts are always true. They do not depend on normality or the sample size. So, even though these facts are often used in conjunction with the Central Limit Theorem, they do not depend on it.\nThink about the simulations you have observed. The mean of the sample means was always aligned with \\(\\mu\\). Also, if you took samples larger than \\(n = 1\\), the standard deviation of sample means was always smaller than \\(\\sigma\\).\n\nAnswer the following questions:\n\n\n\nThe amount of time passengers spend waiting for a bus on a particular urban route follows a distribution that has a mean of 8.7 minutes with a standard deviation of 2.2 minutes. Transportation officials observed the waiting times for a random sample of \\(n=121\\) individual passengers and recorded the sample mean, \\(\\bar x\\). We can think of this sample mean as one value observed out of all the possible sample means that could have been observed. What is the mean of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\mu = 8.7~\\text{minutes}\n\\]\n\n\nUse the information in the previous problem to answer this question: What is the standard deviation of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{2.2}{\\sqrt{121}} = 0.2~\\text{minutes}\n\\]"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#the-law-of-large-numbers",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#the-law-of-large-numbers",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\n\nThe Law of Large Numbers states that the sample mean, \\(\\bar{x}\\), will become closer to \\(\\mu\\) as the sample size \\(n\\) becomes larger.\n\n\nAnswer the following questions:\n\n\n\nWe just learned that the standard deviation of sample means is \\(\\displaystyle{ \\sigma \\over \\sqrt{n} }\\). What happens to the standard deviation of sample means when the sample size is increased?\n\n\n\nShow/Hide Solution\n\n\nIf the sample size, \\(n\\), is increased, then the standard deviation of the sample mean will decrease. The fraction will get smaller.\n\n\n\nIf the standard deviation of the sample mean gets smaller, what happens to the values of \\(\\bar x\\)?\n\n\n\nShow/Hide Solution\n\n\nThe values that will be observed will be very close to each other and therefore close to \\(\\mu\\), if the sample size is large.\n\n\n\nThe result you have discovered in the previous two questions is called the Law of Large Numbers. The Law of Large Numbers states that if the sample size is large, then the sample mean will typically be close to the population mean, \\(\\mu\\). This happens because the standard deviation \\(\\sigma / \\sqrt{n}\\) will get smaller as the sample size \\(n\\) increases.\nNotice that this is very different from the Central Limit Theorem. The Central Limit Theorem is a statement about the DISTRIBUTION of sample means, that if the sample size is large that \\(\\bar x\\) will be approximately normal. The Law of Large numbers states that the sample mean \\(\\bar x\\) will be close to \\(\\mu\\).\nTake a moment to study the difference between the Central Limit Theorem and the Law of Large Numbers. They are very different, but it is easy to mix them up when you are first learning about them."
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#review-of-key-concepts",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#review-of-key-concepts",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\nIn order to “do statistics” we need to compute probabilities. This requires we know the distribution of the population of interest. The most important distribution in this course is the normal distribution. In this lesson, we are interested in another very important distribution: the distribution of all the sample means, \\(\\bar{x}\\). In order to do statistics with \\(\\bar{x}\\) we need the sampling distribution of \\(\\bar{x}\\) to be normal. There are two situations when the distribution of \\(\\bar{x}\\) is guaranteed to be normal (or at least very close to normal). They are:\n\nIf the parent population is normal, the distribution of the sample means \\(\\bar{x}\\) will be normal, for every sample size \\(n\\).\nEven if the parent population is not normal, the Central Limit Theorem guarantees that the distribution of the sample mean \\(\\bar{x}\\) will be approximately normal if the sample size \\(n\\) is large enough. For this course, if \\(n \\geq 30\\), we will say the distribution of the sample means will be approximately normal (even if the parent population is not normal).\n\nThe mean and standard deviation of \\(\\bar{x}\\) are:\n\nThe mean \\(\\mu_{\\bar{x}}\\) of the sample means is the population mean \\(\\mu\\).\nThe standard deviation \\(\\sigma_{\\bar{x}}\\) of the sample means is the population standard deviation \\(\\sigma\\) divided by the square root of \\(n\\), \\(\\sigma / \\sqrt{n}\\)"
  },
  {
    "objectID": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#summary",
    "href": "3-Stat_Fundamentals/05-The_Central_Limit_Theorem.html#summary",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe distribution of sample means is a distribution of all possible sample means (\\(\\bar x\\)) for a particular sample size.\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large. In this class, \\(n\\ge 30\\) is considered to be sufficiently large.\nThe mean of the distribution of sample means is the mean \\(\\mu\\) of the population: \\(\\mu_{\\bar{x}} = \\mu\\).\nThe standard deviation of the distribution of sample means is the standard deviation \\(\\sigma\\) of the population divided by the square root of \\(n\\): \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\).\nThe distribution of sample means is normal in either of two situations: (1) when the data is normally distributed or (2) when, thanks to the Central Limit Theorem (CLT), our sample size (\\(n\\)) is large.\nThe Law of Large Numbers states that as the sample size (\\(n\\)) gets larger, the sample mean (\\(\\bar x\\)) will get closer to the population mean (\\(\\mu\\)). This can be seen in the equation for \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Notice as \\(n\\) increases, then \\(\\sigma_\\bar{x}\\) will get smaller."
  },
  {
    "objectID": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html",
    "href": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html",
    "title": "Probability Calculations for Means (Reading)",
    "section": "",
    "text": "When is the sample mean normally distributed? This happens when either of the two conditions are satisfied:\n\nThe population is normally distributed, so the sample mean is automatically normally distributed.\nThe sample size is large, and the Central Limit Theorem implies that the sample mean is normally distributed.\n\nFor the mean of draws from a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the following are true:\n\nThe mean of the random variable \\(\\bar X\\) is \\(\\mu\\).\nThe standard deviation of the random variable \\(\\bar X\\) is \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\).\n\nPreviously, you learned how to use the normal probability applet and R to convert a \\(z\\)-score to a corresponding area under the curve. These skills will be applied again in this activity. The following questions will help you brush up on how to calculate probabilities for a normal distribution.\n\n\n\n\n\nAnswer the following questions:\n\n\n\nLook at the output from the applet, given above. What is the probability that a standard normal random variable will be below 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(0.8944\\)\n\n\n1a. Compare this to the R output using pnorm()\n\n\nShow/Hide Solution\n\n\npnorm(1.25)\n\n[1] 0.8943502\n\n\n\n\nLook at the output from the applet given above. What is the probability that a standard normal random variable will be above 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(1 - 0.8944 = 0.1056\\)\n\n\n\nChallenge problem: Use R to determine the probability that a standard normal random variable will be between -1.3 and 1.25.\n\n\n\nShow/Hide Solution\n\n\npnorm(1.25) - pnorm(-1.3)\n\n[1] 0.7975497\n\n\n\n\nUse R to find the probability that a standard normal random variable will be greater than -0.75.\n\n\n\nShow/Hide Solution\n\n\n1-pnorm(-.75)\n\n[1] 0.7733726\n\n# Equivalantly \npnorm(-.75, lower.tail = FALSE)\n\n[1] 0.7733726\n\n\n\n\nIf the mean of a normally distributed random variable is -23 and the standard deviation is 7, what is the probability that the random variable will have a value that is less than -25?\n\n\n\nShow/Hide Solution\n\n\nThe \\(z\\)-score is: \n\n\\(\\displaystyle{z = \\frac{-25 - (-23)}{7} = -0.2857}\\)\n\n Using R:\n\n\nx &lt;- -25\nmu &lt;- -23\nsigma &lt;- 7\n\nz &lt;- (x-mu)/sigma\nz\n\n[1] -0.2857143\n\npnorm(z)\n\n[1] 0.3875485\n\n\n*We find that the area to the left of \\(z\\) = -0.2857 is \\(0.3876\\)."
  },
  {
    "objectID": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#review-of-sampling-distributions",
    "href": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#review-of-sampling-distributions",
    "title": "Probability Calculations for Means (Reading)",
    "section": "",
    "text": "When is the sample mean normally distributed? This happens when either of the two conditions are satisfied:\n\nThe population is normally distributed, so the sample mean is automatically normally distributed.\nThe sample size is large, and the Central Limit Theorem implies that the sample mean is normally distributed.\n\nFor the mean of draws from a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the following are true:\n\nThe mean of the random variable \\(\\bar X\\) is \\(\\mu\\).\nThe standard deviation of the random variable \\(\\bar X\\) is \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\).\n\nPreviously, you learned how to use the normal probability applet and R to convert a \\(z\\)-score to a corresponding area under the curve. These skills will be applied again in this activity. The following questions will help you brush up on how to calculate probabilities for a normal distribution.\n\n\n\n\n\nAnswer the following questions:\n\n\n\nLook at the output from the applet, given above. What is the probability that a standard normal random variable will be below 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(0.8944\\)\n\n\n1a. Compare this to the R output using pnorm()\n\n\nShow/Hide Solution\n\n\npnorm(1.25)\n\n[1] 0.8943502\n\n\n\n\nLook at the output from the applet given above. What is the probability that a standard normal random variable will be above 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(1 - 0.8944 = 0.1056\\)\n\n\n\nChallenge problem: Use R to determine the probability that a standard normal random variable will be between -1.3 and 1.25.\n\n\n\nShow/Hide Solution\n\n\npnorm(1.25) - pnorm(-1.3)\n\n[1] 0.7975497\n\n\n\n\nUse R to find the probability that a standard normal random variable will be greater than -0.75.\n\n\n\nShow/Hide Solution\n\n\n1-pnorm(-.75)\n\n[1] 0.7733726\n\n# Equivalantly \npnorm(-.75, lower.tail = FALSE)\n\n[1] 0.7733726\n\n\n\n\nIf the mean of a normally distributed random variable is -23 and the standard deviation is 7, what is the probability that the random variable will have a value that is less than -25?\n\n\n\nShow/Hide Solution\n\n\nThe \\(z\\)-score is: \n\n\\(\\displaystyle{z = \\frac{-25 - (-23)}{7} = -0.2857}\\)\n\n Using R:\n\n\nx &lt;- -25\nmu &lt;- -23\nsigma &lt;- 7\n\nz &lt;- (x-mu)/sigma\nz\n\n[1] -0.2857143\n\npnorm(z)\n\n[1] 0.3875485\n\n\n*We find that the area to the left of \\(z\\) = -0.2857 is \\(0.3876\\)."
  },
  {
    "objectID": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#probabilities-involving-a-mean",
    "href": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#probabilities-involving-a-mean",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Probabilities Involving a Mean",
    "text": "Probabilities Involving a Mean\nIf the sample mean is normally distributed, then we can consider the sample mean, \\(\\bar x\\), as one observation from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\). With this in mind, we can compute the \\(z\\)-score for any value of this random variable:\n\\[z = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} }\\]\nNotice that we just replaced the “value” with the normal random variable, the “mean” with its mean and “standard deviation” with its standard deviation.\nThe \\(z\\)-score follows a standard normal distribution. That is, it has a mean of 0 and a standard deviation of 1. Now we can simply input the \\(z\\)-score into pnorm() to get the probability that a randomly selected mean will be above or below a given value of \\(\\bar x\\).\nWorked Example: Finding the Area under a Normal Curve (Based on a Sample Mean)\nAfter finding the \\(z\\)-score, we can use pnorm() to find the area under the curve (i.e. the probability.) Suppose that a sample of size \\(n = 4\\) has been drawn from a normal population with mean \\(\\mu = 7\\) and standard deviation \\(\\sigma = 3\\). Since the parent population is normal, we know the sampling distribution of the sample mean \\(\\bar x\\) will be normal with mean \\(\\mu = 7\\) and standard deviation \\(\\displaystyle{ \\frac{\\sigma}{\\sqrt{n}} = \\frac{3}{\\sqrt{4}} = 1.5}\\). We can use this information to find the probability that \\(\\bar x\\) will be greater than 10.\nThe \\(z\\)-score is\n\\[z=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{10 - 7}{3 / \\sqrt{4} }=2.0\\]\nThe probability that \\(z\\) will be greater than \\(2.00\\) is the area under the standard normal distribution to the right of \\(2.00\\). We find this area using the normal probability using R:\n\n\nxbar &lt;- 10\nmu &lt;- 7\nsigma &lt;- 3\nn &lt;- 4\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (xbar-mu)/sigma_xbar\nz\n\n[1] 2\n\n#Area to the left:\npnorm(z)\n\n[1] 0.9772499\n\n#Area to the right:\n1-pnorm(z)\n\n[1] 0.02275013\n\n\nThe area to the right of \\(z = 2.00\\) is \\(0.02275\\). This is the probability that the random sample of \\(n = 4\\) items will have a mean that is greater than \\(10\\).\nIn this example, the sample mean was automatically normally distributed because the parent population was normally distributed. This will always be true, no matter what size sample is drawn.\nWorked Example: Finding the Area under a Normal Curve (Based on a Sample Mean)\nWhat do we do if the parent population is not normally distributed? If the sample size is large, then the Central Limit Theorem guarantees that the sample mean will be approximately normally distributed. Based on this, we can still do normal probability calculations for the mean of a random sample.\nThe distribution of the weekly costs incurred by Global Solutions Unlimited is right skewed. The population mean of the costs is $26,400 and the standard deviation is $23,200. A random sample of \\(n = 40\\) weeks is to be selected, what is the probability that the mean weekly costs will be less than $20,000?\nSince the number of observations is large, the Central Limit Theorem assures that the sample mean will follow a normal distribution. The \\(z\\)-score for \\(\\bar x =\\) $20,000 is:\n\\[z = \\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{20,000-26,400}{23,200 / \\sqrt{40}} = -1.745 \\]\nUsing R, we can find the area to the left of this \\(z\\)-score:\n\nxbar &lt;- 20000\nmu &lt;- 26400\nsigma &lt;- 23200\nn &lt;- 40\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (xbar-mu) / sigma_xbar\nz\n\n[1] -1.744705\n\n#Area to the left:\npnorm(z)\n\n[1] 0.04051812\n\n#Area to the right:\n1-pnorm(z)\n\n[1] 0.9594819\n\n\n\nNOTE: Recall the the Normal Probabilty Applet rounds the \\(z\\)-scores first, then calculates the probabilities so they differ slightly from R.\nThe area to the left of \\(z= -1.745\\) is \\(0.04052\\). This is the probability that the mean costs for the \\(n = 40\\) weeks will be less than $20,000.\n\n\nExample: Environmental Clean Up\n\n\nWe will now consider a complete example that shows how these probabilities are used in practice.\nThe United States Government decided to open some land near a uranium enrichment facility to public use. After a few years of the public hiking, biking, and sometimes even hunting on this land, workers from the facility noticed that there were several unnatural-looking mounds in the earth near the area. Because this land was once used by the facility and nobody knew the origin of these piles, the government closed public access to the land until they could assess if the mounds were safe.\n\n\nStep 1: Design the study.\nMeasurements were taken from the mounds to assess one of the contaminants, lead. The tests involved are very expensive. Each sample costs about $600 to process. The Environmental Protection Agency (EPA) has set a “No Action Level” (NAL) for lead. If the mean concentration in the soil of the contaminant is less than the NAL, then the area can be declared safe for public use. If the concentration of the contaminant reaches or exceeds the NAL, the site must be cleaned additionally before it is declared safe. The NAL for lead is 50 milligrams of lead per kilogram of soil (mg/kg).\nThe hypothesis for this test are:\n\\(H_0:~~\\mu = 50 \\frac{\\text{mg}}{\\text{kg}}\\)\n\\(H_a:~~\\mu &lt; 50 \\frac{\\text{mg}}{\\text{kg}}\\)\nIn environmental testing, we always assume the site is dirty. That is, our null hypothesis is that the mean level of contamination is at the NAL. We gather data to determine if there is sufficient evidence to support rejecting the null hypothesis.\n\n\nStep 2: Collect Data\nScientists collected \\(n = 61\\) measurements of the lead concentration in the soil, measured in mg/kg. The data are given in the file Uranium Plant Data-Lead.\n\n\nStep 3: Describe the Data\n\nAnswer the following question:\n\n\n\nCreate a histogram of the lead contamination data.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following object is masked from 'package:rio':\n\n    factorize\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nuranium &lt;- import('https://github.com/byuistats/BYUI_M221_Book/raw/master/Data/UraniumPlantData-Lead.xlsx')\n\nhistogram(uranium$`Lead (mg/kg)`, xlab=\"Lead (mg/kg)\")\n\n\n\n\n\n\n\n\n\n\nWhat is the shape of the distribution of the data?\n\n\n\nShow/Hide Solution\n\n\nThe lead concentration data are right skewed.\n\n\n\nAre any of the measured values above the NAL (50 mg/kg)? What might this suggest?\n\n\n\nShow/Hide Solution\n\n\nYes, there are several observations that are above the NAL. This may suggest that the lead concentrations are very high in some isolated soil samples. It may also be an indication of variability in the test results.\n\n\n\nVisually, does it look like the mean lead concentration is less than 50 mg/kg?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary.\n\n\n\nWhat is the value of the sample mean?\n\n\n\nShow/Hide Solution\n\n\nknitr::kable(favstats(uranium$`Lead (mg/kg)`))\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n9.8\n14.1\n19.8\n33.1\n115\n29.13279\n23.26021\n61\n0\n\n\n\n\n\n\nThe sample mean is 29.13 mg/kg.\n\n\n\n\n\nStep 4: Make Inferences\nWe assume the null hypothesis is true and we gather evidence against this requirement. We will find the probability that the mean lead concentration is less than \\(\\bar x\\) = 29.13 mg/kg, assuming that the true mean lead concentration is \\(\\mu\\) = 50 mg/kg. This probability is called the \\(P\\)-value.\nSince the sample size is large (\\(n = 61\\)), we can conclude that the sample mean is normally distributed. So, we can use R to find the probability that the sample mean will be less than 29.13. Historical analyses show that the population standard deviation is \\(\\sigma\\) = 24 mg/kg.\nFirst, we compute the \\(z\\)-score.\n\\[z=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{29.13 - 50}{24 / \\sqrt{61} }=-6.79\\]\n\nx &lt;- 29.13\nmu &lt;- 50\nsigma &lt;- 24\nn &lt;- 61\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (x-mu)/sigma_xbar\nz\n\n[1] -6.791663\n\npnorm(z)\n\n[1] 5.542414e-12\n\n\nUsing the pnorm() function, we find the area to the left of \\(z = -6.79\\). This is the \\(P\\)-value. It is not clear from the image of the applet, but the area to the left was shaded before the \\(z\\)-score was entered.\n\nThe \\(P\\)-value for this test is so small we have to resort to scientific notation. \\(5.54 \\times 10^{-12} = 0.000~000~000~005~6\\). This is a very small probability. Assuming the null hypothesis is true-that is, the site is unacceptably contaminated-it is very unlikely that we would find such a low mean contamination level among the \\(n = 61\\) randomly selected soil samples.\nSince the \\(P\\)-value is low, we reject the null hypothesis.\n\n\nStep 5: Take Action\nThere is sufficient evidence to suggest that the mean lead level is less than 50 mg/kg. We conclude that the lead concentration in the soil is low enough that it is not a danger to the public. Based on the results of this and other similar test results, the government has reopened public access to this area."
  },
  {
    "objectID": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#review-of-key-concepts",
    "href": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#review-of-key-concepts",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\n\nTo compute probabilities involving the sample mean \\(\\bar{x}\\) we must first determine if \\(\\bar{x}\\) is normally distributed. If it is not, we cannot compute probabilities. There are two cases when \\(\\bar{x}\\) is guaranteed to be normally distributed. They are:\n\n\nThe parent population is Normally distributed.\nThe Central Limit Theorem guarantees the distribution of \\(\\bar{x}\\) is Normal if the sample size \\(n\\) is large enough. For this course, we require \\(n \\geq 30\\).\n\n\nThe collection of all possible sample means \\(\\bar{x}\\) is also a population. The mean of the distribution of sample means is the population mean \\(\\mu\\). The standard deviation of the distribution of sample means is \\(\\frac{\\sigma}{\\sqrt{n}}\\), where \\(\\sigma\\) is the parent population standard deviation and \\(n\\) is the sample size.\nOnce we have determined that the sample mean is Normally distributed, we can compute probabilities with \\(\\bar{x}\\). We first compute \\(z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) and then use R to compute probabilities about \\(z\\)."
  },
  {
    "objectID": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#summary",
    "href": "3-Stat_Fundamentals/07-Prob_Calculations_for_Means.html#summary",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA z-score for a sample mean is calculated as: \\(\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{\\bar x-\\mu}{\\sigma/\\sqrt{n}}}\\)\nWhen the distribution of sample means is normally distributed, we can use a z-score R to calculate the probability that a sample mean is above, below or between some given value (or values)."
  },
  {
    "objectID": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html",
    "href": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html",
    "title": "Probability Calculations for Means (Practice)",
    "section": "",
    "text": "Answer the following questions, render the document and submit the .html report."
  },
  {
    "objectID": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#gpa",
    "href": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#gpa",
    "title": "Probability Calculations for Means (Practice)",
    "section": "GPA",
    "text": "GPA\nSuppose the mean GPA of BYU-Idaho students is 3.5 and the standard deviation is 0.7. It is well known that this distribution is left-skewed. A random sample of n = 81 students will be drawn.\nUse the following R code to answer the questions below:\n\nxbar &lt;- \nmu &lt;- \nsigma &lt;- \nn &lt;- \nsigma_xbar &lt;- \nsigma_xbar\n\nError in eval(expr, envir, enclos): object 'sigma_xbar' not found\n\nz &lt;- \nz\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n# Area to the left:\n  \n# Area to the right:\n\nQuestion: What is the mean of the distribution of the sample means (\\(\\mu_{\\bar{x}}\\)) for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer:\nQuestion: What is the standard deviation of the distribution of the sample means (\\(\\sigma_{\\bar{x}}\\)) for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer:\nQuestion: What is the shape of the distribution of the sample means for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer:\nQuestion: What is the probability that the mean GPA for 81 randomly selected BYU-Idaho students will be less than 3.3?\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#gre",
    "href": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#gre",
    "title": "Probability Calculations for Means (Practice)",
    "section": "GRE",
    "text": "GRE\nScores on the quantitative portion of the GRE are approximately normally distributed with mean, \\(\\mu = 150.8\\) and standard deviation, \\(\\sigma = 8.8\\).\nUse the following R code to answer the questions below:\n\nxbar &lt;- \nmu &lt;- \nsigma &lt;- \nn &lt;- \nsigma_xbar &lt;- \nsigma_xbar\n\nError in eval(expr, envir, enclos): object 'sigma_xbar' not found\n\nz &lt;- \nz\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n# Area to the left:\n  \n# Area to the right:\n\n# Percentile (qnorm())\n\nQuestion: Dianne earned a score of 160 on the quantitative portion of the GRE. What is the z-score corresponding to Dianne’s score?\nAnswer:\nQuestion: What is the probability that a randomly selected student will score above 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What GRE score (n=1) corresponds to the 95th percentile?\nAnswer:\nQuestion: What is the probability that the average GRE score of 5 randomly selected students will be above 160?\nAnswer:"
  },
  {
    "objectID": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#coast-guard",
    "href": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#coast-guard",
    "title": "Probability Calculations for Means (Practice)",
    "section": "Coast Guard",
    "text": "Coast Guard\nThe United States Coast Guard assumes the mean weight of passengers in commercial boats is 185 pounds. The previous value was lower, but was raised after a tragic boating accident. The standard deviation of passenger weights is 26.7 pounds. The weights of a random sample of 48 commercial boat passengers were recorded. The sample mean was determined to be 177.6 pounds.\nUse the following R code to answer the questions below:\n\nxbar &lt;- \nmu &lt;- \nsigma &lt;- \nn &lt;- \nsigma_xbar &lt;- \nsigma_xbar\n\nError in eval(expr, envir, enclos): object 'sigma_xbar' not found\n\nz &lt;- \nz\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n# Area to the left:\n  \n# Area to the right:\n\nQuestion: Find the probability that a random sample of passengers will have a mean weight that is as extreme or more extreme (either above or below the mean) than was observed in this sample.\nAnswer:\nHINT: To get a value “as extreme” means it could be higher or lower. Because the normal distribution is symmetric, if the \\(z&lt;0\\), we can take the area to the left of \\(z\\) and multiply by 2. Conversely, if \\(z&gt;0\\) we can take the area to the right and multiply by 2."
  },
  {
    "objectID": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#tankers",
    "href": "3-Stat_Fundamentals/09-Prob_Calculations_for_Means_Practice.html#tankers",
    "title": "Probability Calculations for Means (Practice)",
    "section": "Tankers",
    "text": "Tankers\nTanker trucks are designed to carry huge quantities of gasoline from refineries to filling stations. A factory that manufactures the tank of the trucks claims to manufacture tanks with a capacity of 8550 gallons of gasoline. The actual capacity of the tanks is normally distributed with mean, \\(\\mu = 8544\\) gallons, and standard deviation, \\(\\sigma=12\\) gallons.\nUse the following R code to answer the questions below:\n\nxbar &lt;- \nmu &lt;- \nsigma &lt;- \nn &lt;- \nsigma_xbar &lt;- sigma / sqrt(n)\n\nError in eval(expr, envir, enclos): object 'n' not found\n\nsigma_xbar\n\nError in eval(expr, envir, enclos): object 'sigma_xbar' not found\n\nz &lt;- (xbar - mu) / sigma_xbar\n\nError in eval(expr, envir, enclos): object 'xbar' not found\n\nz\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n# Area to the left:\npnorm(z)\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n#Area to the right\n1-pnorm(z)\n\nError in eval(expr, envir, enclos): object 'z' not found\n\n## Area between A and B\nA &lt;- \nB &lt;- \n\npnorm(B, mu, sigma_xbar) - pnorm(A, mu, sigma_xbar)\n\nError in eval(expr, envir, enclos): object 'B' not found\n\n\nQuestion: Find the z-score corresponding to a single tank (\\(n=1\\)) with a capacity of 8550 gallons. Round your answer to one decimal place.\nAnswer:\nQuestion: What is the probability that a randomly selected tank will have a capacity of less than 8550 gallons?\nAnswer:\nQuestion: A simple random sample of \\(n = 20\\) tanks was selected. Find the z-score corresponding to a sample mean capacity for 20 tanks of 8550.\nAnswer:\nQuestion: What is the probability that the sample mean of \\(n=20\\) randomly selected tanks will be between 8541 and 8547?\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "title": "Inference for a Mean",
    "section": "",
    "text": "When we know what the population standard deviation, \\(\\sigma\\), for individuals, we can calculate Z-scores and use the Standard Normal Distribution to calculate probabilities. Z has a standard normal distribution, meaning it has a mean of 0 and a standard deviation of 1.\n\\[ z= \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nThough rare, there are situations where we might know the population standard deviation from published research or census data. For example, Standardized test organizations publish population-level summaries which would allow us to test how our sample compares to the general population using the Z formula.\n\n\nWhen testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. The \\(z\\)-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:\n\n\n\nWe can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "title": "Inference for a Mean",
    "section": "",
    "text": "When testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. The \\(z\\)-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "title": "Inference for a Mean",
    "section": "",
    "text": "We can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "title": "Inference for a Mean",
    "section": "Calculating P-values by hand with the T Distribution (One out of ten. Would NOT RECOMMENDED)",
    "text": "Calculating P-values by hand with the T Distribution (One out of ten. Would NOT RECOMMENDED)\nSuppose we have 25 test scores defined as “data” in the code chunk below. We can calculate the \\(t\\)-statistic just like we did with the \\(z\\)-score.\nSuppose we believe that the mean score of these 25 students is significantly higher than 50. Our null and alternative hypothesis are as follows:\n\\[ H_0:  \\mu = 50 \\] \\[ H_a: \\mu &gt; 50 \\]\n\n# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom\n\ndata &lt;- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)\n\n# Hypothesized Mean:\nmu_0 &lt;- 50\n\n# Sample size, sample Mean and sample SD\n#  The length function tells us how many data points are in the list.  \nn &lt;- length(data)\nxbar &lt;- mean(data)\ns &lt;- sd(data)\n\ns_xbar = s / sqrt(n)\n\nt &lt;- (xbar - mu_0) / s_xbar\n\n# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true\n1-pt(t, n-1)\n\n[1] 0.00151866\n\n\nThis means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (P-value).\nThe good news is that the more complicated the math becomes, the less of it we have to do! Instead of using R like a calculator to calculate \\(z\\) or \\(t\\)-scores and calculating probabilities “by hand” (using pnorm() or pt()), we can use R functions with the data directly and get much more useful output."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "title": "Inference for a Mean",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Read in data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "title": "Inference for a Mean",
    "section": "Step 2: Review the Data",
    "text": "Step 2: Review the Data\nIn this step, we are looking to see if the data are as expected. Are the columns we’re interested in numeric? Categorical? and do these match expectations. We can also start to look for strange data and outliers. Visualizations can help.\nOther common issues to look for include: negative numbers that should only be positive, date values that shouldn’t exist, missing values, character variables inside what should be a number.\nIn the real world, data are messy. Reviewing the data is a critical part of an analysis.\nTake a look through the personality dataset and see if there are any anomalies that might need to be addressed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "title": "Inference for a Mean",
    "section": "Step 3: Visulize the data",
    "text": "Step 3: Visulize the data\nSo far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc. When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.\nCreate a histogram of Extroversion. Describe some features of the data. Is it symmetric? Skewed? Are there outliers?\n\n# Basic Graph\nhistogram(big5$Extroversion)\n\n\n\n\n\n\n\n# Improved graph\nhistogram(big5$Extroversion, main = \"Extroversion Scores\", xlab = \"Extroversion\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "title": "Inference for a Mean",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\nIn this example, we will be testing the hypothesis that Brother Cannon’s students are similar to the general population. We suspect that these youthful BYU-I students are, on average, more extroverted.\n\nHypothesis Test\nWrite out the Null and alternative hypotheses.\n\\[ H_0: \\mu_{extroversion} = 50\\]\n\\[ H_A: \\mu_{extroversion}  &gt; 50\\]\n\\[\\alpha = 0.05\\]\nPerform the one-sample t-test using the “t.test()” function in R. If you ever get stuck remembering how to use a function in R, you can run ?t.test to see documentation. The question mark will open up the help files for any given function in R.\nThe t.test() function takes as input, the data, the hypothesized mean, mu, and the direction of the alternative hypothesis.\nThe default parameters for the t.test() function are: t.test(data, mu = 0, alternative = \"two.sided\").\n\n#?t.test\n\n# One-sided Hypothesis Test\nt.test(big5$Extroversion, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: How do we explain our conclusion in context of our research question?\nAnswer:\nTechnical Conclusion:\nContextual Conclusion:\n\n\nConfidence Intervals\nWe can also use the t.test() function to create confidence intervals. Recall that confidence intervals are always 2-tailed. Confidence intervals are typically written in the form: (lower limit, upper limit).\n\n# Confidence Intervals are by definition 2-tailed\n# We can also change the confidence level\n\nt.test(big5$Extroversion, mu = 50, alternative = \"two.sided\", conf.level = .99)\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 9.394e-11\nalternative hypothesis: true mean is not equal to 50\n99 percent confidence interval:\n 54.26631 59.69903\nsample estimates:\nmean of x \n 56.98267 \n\n\nTo get only the output for the confidence interval to be shown, we can use a $ to select specific output. Much like when pulling a specific column from a dataset, the $ can pull specific output from an analysis.\nBecause the option for the alternative = in the t.test function is “two.sided”, we don’t actually need to include it when getting confidence intervals.\nAlso, confidence intervals do not require an assumed mu value. So a more efficient way to get a confidence interval for a given set of data is:\n\nt.test(big5$Extroversion, conf.level = .99)$conf.int\n\n[1] 54.26631 59.69903\nattr(,\"conf.level\")\n[1] 0.99\n\n\nQuestion: Describe in words the interpretation of the confidence interval in context of Extroversion.\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "title": "Inference for a Mean",
    "section": "Body Temperature Data",
    "text": "Body Temperature Data\nThe dataset below contains information about body temperatures of healthy adults.\n\nLoad the data:\n\n# These lines load the data into the data frame body_temp:\n\nbody_temp &lt;- import(\"https://byuistats.github.io/M221R/Data/body_temp.xlsx\")\n\n\n\nReview the Data\nCreate a table of summary statistics:\n\n\nVisualize the Data\nCreate a histogram to visualize the body temperature data.\nQuestion: Describe the general shape of the distribution.\nAnswer:\n\n\nAnalyze the Data\nIt’s widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.\nSuppose we suspect that the average temperature is different than 98.6\nUse a significance level of \\(\\alpha = 0.01\\) to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.\nQuestion: What is the P-value?\nAnswer:\nExplain our conclusion.\n\nConfidence Interval\nCreate a 99% confidence interval for the true population average temperature of healthy adults."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "A matched pairs design is used in statistics to compare 2 treatments or conditions measured on subjects that are logically connected in a meaningful way. In the simplest case, measurements are collected on the same subject as in a before-and-after evaluation.\nMatched pairs designs are often called “dependent samples” because knowing who or what is in the first treatment group determines who will be in the second. In the case of a before-and-after situation, if you are selected to be in the “pre” group, then you will also be in the “post” group. But pairs are not always the same subjects.\n\n\n\nYou want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item.\n\n\n\n\nAs with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality.\n\n\n\nWhen we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "You want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "As with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "When we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nweight_loss &lt;- import(\"https://byuistats.github.io/M221R/Data/weight_loss.xlsx\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms summary statistics for the pre and post weight measurements:\n\nView(weight_loss)\n\n# Pre-weight histogram\nhistogram(weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$pre)\n\n  min    Q1 median   Q3  max mean       sd  n missing\n 60.7 72.75     77 80.7 99.6 76.9 10.20671 27       0\n\n# Post-weight histogram\nhistogram(weight_loss$post)\n\n\n\n\n\n\n\nfavstats(weight_loss$post)\n\n  min    Q1 median   Q3 max    mean       sd  n missing\n 53.3 63.45   69.9 75.2  97 70.0963 10.63273 27       0"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you’re going to define the difference (\\(post - pre\\) or \\(pre - post\\)).\nQuestion: What does a negative number mean based on your definition?\nAnswer:\n\n# Decide which column to subtract from the other\ndiff &lt;- weight_loss$post - weight_loss$pre\n\nCreate a histogram and a qqPlot of the differences to determine if you will be able to trust the statsitical analyses:\n\n# histogram of the differences\nhistogram(diff)\n\n\n\n\n\n\n\nfavstats(diff)\n\n   min   Q1 median   Q3  max      mean       sd  n missing\n -13.6 -8.9   -6.7 -4.2 -1.6 -6.803704 3.172051 27       0\n\n# Check if the differences are normally distributed:\nqqPlot(diff)\n\n\n\n\n\n\n\n\n[1] 14 18"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\n\\[ H_0: \\mu_{differences} = 0\\]\n\\[H_a:  \\mu_{differences} &lt; 0\\]\n\\[ \\alpha = 0.01\\]\nQuestions to consider:\n\nHow did you define your difference?\nBased on your decision for the difference, what does a negative number mean? a positive number?\nAre you expecting the difference to be greater than, less than, or not equal to 0?\n\nNOTE: One way to verify that you have used the correct alternative is to look at the difference as defined (weight_loss$post - weight_loss$pre) and decide which one you think is supposed to be bigger. Swap out the subtraction sign for the inequality that makes sense (weight_loss$post &lt; weight_loss$pre). Therefore, your alternative should be “less” because \\(&lt;\\) corresponds to “less than”.\nPerform a t-test of the differences:\n\n# Hypothesis t.test()\nt.test(diff, mu = 0, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean of x \n-6.803704 \n\n\nState your conclusion in context of the research question:\nBecause P-value &lt; 0.01 we reject the null hypothesis. We have SUFFICIENT evidence to suggest that participation in the weight loss program led to weight loss.\n\n\nConfidence Interval\n\n# Confidence interval using t.test()\nt.test(diff, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\n\nI am 99% confident that the true average weight loss during the program was between -8.5 and -5.11 kilograms."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Check Data",
    "text": "Step 2: Check Data\nReview the data to see if the variables look correct. Calculate summary statistics and histograms for chol_day2 and chol_day4."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nDecide how to define your difference. What does a negative number indicate? a positive number?\nCreate a histogram and summary statistics of the differences:\nBecause we \\(n &lt; 30\\) we need to check the qqPlot() to assess the normality of the differences."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nWhat is your confidence level, ($1-= $)?\nPerform a matched pairs t-test for the difference in cholesterol at day 2 and day 4:\nState your conclusion:\n\n\nConfidence Interval\nCalculate a confidence interval for the average difference.\nState your conclusions and interpret the confidence interval in context of the research question."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "title": "Independent Two-sample T-test",
    "section": "",
    "text": "In many situations we would like to compare averages from different populations. In these situations, we take 2 random samples from each population and perform statistical tests to determine if the population means are significantly different. Because these two groups of individuals are sampled independently, we call this analysis Independent 2-Sample t-test.\nAlternatively, in many experimental designs, participants are randomly assigned into a treatment and a control group. The randomization process ensures that there is no association between participants in either group. They are independent.\nWhen 2 random samples are taken from 2 separate populations, or when a group of people are randomly assigned into treatment groups, the samples are independent.\nThis differs from the dependent t-test. Recall that samples are dependent when knowing who or what is in one group determines who or what is in the second group.\nSome examples include:\n\nComparing salaries of men and women (randomly sample men and women separately)\nTesting a new medication compared to a placebo (participants randomly assigned to treatment groups)\nComparing average GPA of Math majors and Economics majors (randomly select from each population)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "title": "Independent Two-sample T-test",
    "section": "Independent t-test in R",
    "text": "Independent t-test in R\nA 2-sample independent t-test in R requires a slight modification to the 1-sample and dependent t-tests already performed. The syntax should look familiar.\nRecall that when we created a boxplot() or did favstats() for one set of data it looked like:\n\nboxplot(data$response_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nfavstats(data$response_variable)\n\nError in favstats(data$response_variable): could not find function \"favstats\"\n\n\nwith data$response_variable corresponding to our quantitative variable of interest.\nWhen we wanted to break the analysis down by a grouping factor we used the ~ notation to add a group variable:\n\nboxplot(data$response_variable ~ data$grouping_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nfavstats(data$response_variable ~ data$grouping_variable)\n\nError in favstats(data$response_variable ~ data$grouping_variable): could not find function \"favstats\"\n\n\nWe use the exact same modification for a t-test with 2 groups:\n\nt.test(data$response_variable ~ data$grouping_variable, alterntive = \"greater\")\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\n\nRecall that the t-test() function uses mu=0 as a default, we do not need to specify it in the function because the null value when comparing 2 groups is 0.\nNOTE: In R, group 1 and 2 are determined alphabetically according to the labels in the dataset."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "title": "Independent Two-sample T-test",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that confidence intervals are necessarily two-sided. So the code for a 99% confidence interval looks like:\n\nt.test(data$response_variable ~ data$grouping_variable, conf.level = .99)$conf.int\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\n\nWe interpret a confidence interval for the difference of means as follows:\n\nI am 99% confident that the true difference of the means is between [lower limit] and [upper limit].\n\nWe can usually do better within the context of a research question:\n\nClass 1 did, on average, between 3.21 and 5.67 percent better than class 2 on the last exam.\n\nStore A outperforms Store B by between $27,022 and $36,977 on average"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\n\nfifa_heart_attacks &lt;- import(\"https://byuistats.github.io/M221R/Data/fifa_heart_attacks.xlsx\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review Data",
    "text": "Step 2: Review Data\nLook at the data.\nCreate summary statistics tables of the number of heart attacks for each group.\nCreate a side-by-side boxplot for the during the World Cup and the Control.\nDo you notice any outliers or data that may need to be omitted for analysis?\nCheck to see if the means from both groups are normally distributed:\n\nIs n &gt; 30 for both groups?\nCreate a qqPlot()\n\n\nqqPlot(fifa_heart_attacks$heart_attacks, groups = fifa_heart_attacks$time_period)\n\n\n\n\n\n\n\n\nCan we trust that the central limit theorem applies?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThese data look ready for analysis."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nAre the individuals in each group dependent or independent of each other?\nWrite out your null and alternative hypotheses.\nHo: Ha:\nWhich group is considered group 1 and which is group 2 in R?\nCheck the alphabetical order:\n\nunique(fifa_heart_attacks$time_period)\n\n[1] \"Control\"   \"World Cup\"\n\n\nPerform the appropriate t-test.\nWhat is your test statistic?\nWhat is your p-value?\nState your conclusion:\n\n\n97% Confidence interval\nCalculate the 97% confidence interval for the difference of the means.\nIn context of the research question, interpret the confidence interval."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "title": "Independent Two-sample T-test",
    "section": "New Zealand Rugby",
    "text": "New Zealand Rugby\nRugby is a popular sport in the United Kingdom, France, Australia, New Zealand and South Africa. It is gaining popularity in the US, Canada, Japan and parts of Europe. Some of the rules of the game have recently been changed to make play more exciting. In a study to examine the effects of the rule changes, Hollings and Triggs (1993) collected data on some recent games.\nTypically, a game consists of bursts of activity that terminate when points are scored, if the ball is moved out of the field of play or if a violation of the rules occurs. In 1992, the investigators gathered data on ten international matches which involved the New Zealand national team, the All Blacks. The first five games were the last international games played under the old rules, and the second set of five were the first internationals played under the new rules.\nFor each of the ten games, the data give the successive times (in seconds) of each passage of play in that game.\nYou will investigate whether the mean duration of the passages has dropped under the new rules.\nUse a level of significance of 0.01.\n\nLoad the Data\n\nrugby &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/nz_rugby.csv\")\n\n\n\nExplore the Data\nCreate a side-by-side boxplot for the amount of reported passage of play before and after the rule changes.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics of play time for before and after the rule change. (favstats()):\n\n\nPerform the Appropriate Analysis\n\nHypothesis Test\nState your null and alternative hypotheses:\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = NewRules\nCompare the the time per play under the new and old rules:\n\nqqPlot(rugby$time, groups = rugby$period)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nWhat is the value of the test statistic?\nHow many degrees of freedom for this test?\nWhat is the p-value?\nWhat do you conclude?\n\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\ncopd &lt;- import(\"https://byuistats.github.io/M221R/Data/copd_rehab.xlsx\") %&gt;% pivot_longer(cols=c(\"community\", \"hospital\"), names_to = \"Treatment\", values_to = \"Steps\") %&gt;% select(Treatment, Steps) %&gt;% arrange(Treatment)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nCreate side-by-side boxplots and summary statistics for the community and hospital groups:\nCheck to see if the means are expected to be normally distributed.\nCan trust the CLT for our test statistic and P-value?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThe data cleansing has been performed for you. You’re welcome."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nHo:\nHa:\nWhich group is considered group 1 in this data?\nRun the appropriate t-test.\n\n#t.test()\n\nState your conclusion about the hypothesis test.\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference between the means\nInterpret the 95% confidence interval for the mean difference between the community-based and hospital-based groups."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "When we want to compare 3 or more groups, the math get’s more complicated. Analysis of Variance (ANOVA) compares how spread out the means are relative to the average within group variation. The formula for the new test statistic, \\(F\\), is messy, but we can get a sense for what it’s doing visually.\n\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more significant the result.\n\n\nRecall that the shape of the \\(t\\)-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the \\(t\\)-distribution, the \\(F\\)-distribution is not centered around zero and can never be negative. \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests.\n\n\n\nThe null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that something is different.\n\n\n\nJust as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value.\n\n\n\n\nMuch of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\nError in favstats(data$response_variable ~ data$categorical_variable): could not find function \"favstats\"\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\nError in eval(expr, envir, enclos): object 'sds' not found\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nqqPlot(output$residuals)\n\nError in qqPlot(output$residuals): could not find function \"qqPlot\"\n\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Recall that the shape of the \\(t\\)-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the \\(t\\)-distribution, the \\(F\\)-distribution is not centered around zero and can never be negative. \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#hypothesis-test",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#hypothesis-test",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that something is different."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Just as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Much of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\nError in favstats(data$response_variable ~ data$categorical_variable): could not find function \"favstats\"\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\nError in eval(expr, envir, enclos): object 'sds' not found\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nqqPlot(output$residuals)\n\nError in qqPlot(output$residuals): could not find function \"qqPlot\"\n\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4))"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\n\nQuestion: What is the F-statistics?\nAnswer:\nQuestion: What are the between-groups degrees of freedom?\nAnswer:\nQuestion: What are the within-groups degrees of freedom?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: What is your conclusion?\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html",
    "title": "Comparing Means",
    "section": "",
    "text": "In this activity, you will use everything we’ve covered up to this point including:\n\nData manipulation using tidyverse functions\nHypothesis tests\n\n1-Sample t-test\n2-sample dependent t-test\n2-sample independent t-test\nANOVA\n\nConfidence Intervals where applicable\n\nWe will be using data collected about students in 2 Portuguese schools including their final grade. The goal is to answer research questions using statistical methods to see what factors significantly impact final grades.\n\n\nIn class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#getting-to-know-a-new-dataset",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#getting-to-know-a-new-dataset",
    "title": "Comparing Means",
    "section": "",
    "text": "In class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#categories-labeled-as-numbers",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#categories-labeled-as-numbers",
    "title": "Comparing Means",
    "section": "Categories Labeled as Numbers",
    "text": "Categories Labeled as Numbers\nSometimes even correct data can have issues that must be addressed. For example, categories are often labeled as numbers. Software can’t guess when numbers are supposed to be categories, so we have to tell R when a number should be treated as a category.\nTo force a variable to be a category, we use the factor() function in R. We can change the variable type in the data itself or change it in the analysis. We demonstrate both methods below.\n\nChanging a Column Type in a dataset\nFather’s education, Fedu, shows up as a number in R. The website suggests that the numbers represent categories (0 = none, 1 = primary education (4th grade), 2 = 5th to 9th grade, 3 = secondary education or 4 = higher education).\nTo change the data type in the data itself, we can use a mutate statement in the following manner:\n\n# Create a new dataset called fedu_data that begins with the clean data and adds a column that we called Fedu_factor, which is the factorized column, Fedu:\n\nnew_data &lt;- student %&gt;%\n  mutate(Fedu_factor = factor(Fedu))\n\n# Check the column names of the new dataset.  Notice the new column\nnames(new_data)\n\n# glimpse() shows us data types.  Notice after Fedu_factor, the &lt;fct&gt;, which shows us that this is in fact, a factor variable type.  &lt;dbl&gt; stands for \"double\" and is a numeric variable type\n\nglimpse(new_data)\n\n\n\nChanging the Variable Type “on the fly”\nYou may not want to bother changing all the variable types for each potential analysis. Fortunately, you can create a factor “on the fly” within the analysis function itself.\nBecause there are more than 2 levels of Father’s Education, I will demonstrate how this is done in an ANOVA:\n\n# Force Fedu to be treated like a category in ANOVA:\nfedu_anova &lt;- aov(student$G3 ~ factor(student$Fedu))\n\nsummary(fedu_anova)\n\n                      Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(student$Fedu)   4    238   59.53   2.891 0.0222 *\nResiduals            390   8032   20.59                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis works with most analysis functions including t.test() and aov().\nNOTE: You only have to do this for variables in a dataset that are categories labeled as numbers. If the categories are text, t.test() and aov() automatically recognizes the variable as categorical. However, it does no harm to put a column with text into a factor() statement."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#cleaning-the-data",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#cleaning-the-data",
    "title": "Comparing Means",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nWhile exploring the data, you may have noticed a few students ended up with a final grade of zero. While it may be interesting to explore what factors lead to an incomplete grade, we want to make conclusions about students who completed the course.\nCreate a clean dataset called, clean, that excludes zeros for G3. This will be used for the following analyses."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-schools",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-schools",
    "title": "Comparing Means",
    "section": "Comparing Schools",
    "text": "Comparing Schools\nSuppose the Gabriel Pereira school (GP) has more stringent admissions requirements. We suspect this would lead to higher grades, on average.\nCreate a side-by-side boxplot of the final grades for each school. Change the y-axis label to read “Final Grade out of 20”, the x-axis label to read “School”, and add a title.\nQuestion: What do you notice?\nAnswer:\nCreate a table of summary statistics of final grade for each school:\n\nHypothesis Test\nCreate a qqPlot to look at the normality of both groups:\nQuestion: Do the grades look normally distributed for both groups? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypotheses and significance level.\nNOTE: Recall that R uses alphabetical order to determine which group is the reference group. It is useful to put this group on the left side of the null hypothesis and set your alternative hypothesis accordingly.\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate statistical test:\nQuestion: What is the P-value?\nAnswer:\nQuestion: What is your conclusion in context of the research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval and explain it in context of the research question.\nExplanation:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-second-period-grade-with-final-grade",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-second-period-grade-with-final-grade",
    "title": "Comparing Means",
    "section": "Comparing Second Period Grade with Final Grade",
    "text": "Comparing Second Period Grade with Final Grade\nWe suspect there is a difference between the second period and the final grade, though we do not know if they go up or down. Carry out a hypothesis test to evaluate this suspicion.\n\nHypothesis Test\nChoose how you will define the difference between final grade and second period grade, and create a new object called diff:\n\ndiff &lt;- \n\nError: &lt;text&gt;:2:0: unexpected end of input\n1: diff &lt;- \n   ^\n\n\nQuestion: What does a negative number mean?\nAnswer:\nCreate a qqPlot() of diff and check for normality:\nQuestion: Do the grade differences look normally distributed? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypothesis and choose a significance level:\n\\[H_o: \\]\n\\[Ha:  \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis.\nQuestion: What is the P-value?\nAnswer:\nQuestion: What conclusion do you make in context of this research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for the differences and explain it in context of the research question.\nExplanation:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#absenteeism-in-portugal",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#absenteeism-in-portugal",
    "title": "Comparing Means",
    "section": "Absenteeism in Portugal",
    "text": "Absenteeism in Portugal\nIn 2021, Portugal reported having 0% absenteeism for 15-year-olds. We suspect that the actual absenteeism is higher than the reported value (zero).\n\nHypothesis Test\nCreate a qqPlot() for absences.\nQuestion: Do absences look normally distributed? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypotheses and choose a significance level:\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis.\nQuestion: What is the P-value?\nAnswer:\nQuestion: What conclusion do you make in context of this research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for average absences and interpret it in context of the problem.\nExplanation:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#the-impact-of-mothers-education-level",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#the-impact-of-mothers-education-level",
    "title": "Comparing Means",
    "section": "The Impact of Mother’s Education Level",
    "text": "The Impact of Mother’s Education Level\nThe level of education of the mother in the home is thought to have a significant impact on student success.\nCreate a side-by-side boxplot of final grades for each level of mother’s education.\nCreate a table of summary statistics of final grades for each level of mother’s education.\nQuestion: How many respondents have a mother with no formal education (level 0)?\nAnswer:\nCreate a new dataset, clean_medu, that does not include mother’s education level 0.\n\n\nclean_medu &lt;- clean %&gt;% \n\nError: &lt;text&gt;:4:0: unexpected end of input\n2: clean_medu &lt;- clean %&gt;% \n3: \n  ^\n\n\nCreate another boxplot with the new dataset that excludes level 0.\nCreate a summary table of final grades for each level of a mother’s education with the new dataset.\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:\nQuestion: Verify that the maximum is less than twice as large as the minimum to check the “equality of standard deviations”.\nAnswer:\n\nHypothesis Test\nState your null and alternative hypotheses and pick alpha:\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate statistical test.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nCheck the normality of the residuals.\nQuestion: Do the residuals appear roughly normally distributed?\nAnswer:\nQuestion: Can we trust the P-value.\nAnswer:\nState your conclusion."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#choose-your-own-adventure",
    "href": "4-Statistical_Tests_Part1/09-AA_Unit4_Review.html#choose-your-own-adventure",
    "title": "Comparing Means",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nPick another variable that was not analyzed above.\nCreate a side-by-side boxplot. Be sure to properly label the graph and add sufficient information so readers can know what they are looking at without having to search through the report or code.\nPerform the appropriate analysis. Be sure to include a concise conclusion in the context of the research question, including a hypothesis test (and confidence interval if applicable.)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Consider the relationship between Score on a math exam and a student’s self-reported Confidence Rating.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\n\nQuestion: What is the explanatory (aka independent) variable, \\(x\\)?\nAnswer:\nQuestion: What is the response (aka the dependent) variable, \\(y\\)?\nAnswer:\nPlot the relationship:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Does the relationship appear linear?\nAnswer:\nQuestion: What is the direction of the relationship? Answer:\nQuestion: What do you think is the strength of the relationship? (Strong/Moderate/Weak) Answer:\nQuestion: What is the correlation coefficient, r? Answer:\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\n\nBase R\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function in base R, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\nabline(math_lm)\n\n\n\n\n\n\n\n\nJust as with the other plotting functions we’ve used, we can change the color, type and width of the line:\n\nplot(Score ~ ConfidenceRatingMean, data = math, pch = 16, main = \"Title\")\nabline(math_lm, col = \"purple\", lwd = 3, lty = 3)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "title": "Simple Linear Regression",
    "section": "GGPlot",
    "text": "GGPlot\nUsing ggplot(), we can simply add a geom_smooth() geometry and specify the method of “smoothing” as a linear model:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\nBy default, this gives us a confidence interval for the slope of the regression line. We can turn that off by forcing the “standard error” to be FALSE:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most often only interested in the slope\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese are the same for all simple linear regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\nTechnically correct: 95% Confident that the true population slope is within (10.674297, 14.71535)\nContextual Explanation: For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements (Is our P-value sus?)",
    "text": "Regression Requirements (Is our P-value sus?)\nThere are certain requirements for all statistical tests to be valid. For means, we needed to make sure that the Central Limit Theorem applied. This meant that we had a large enough sample size (N&gt;30) or that the population itself was normally distributed.\nFor ANOVA, we had to check that the residuals were normally distributed and that the population standard deviations were the same.\nRegression analysis has 5 requirements to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot() exactly as with the aov() output.\nRecall that with ANOVA, we had to check that the variation in each group was roughly the same (largest standard deviation was less than twice as large as the smallest standard deviation). For a quantitative explanatory variable, we can’t calculate the standard deviation for a specified level of \\(x\\) because \\(x\\) can be any number.\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "title": "Sampling Distribution of P_hat",
    "section": "",
    "text": "Categorical data is often summarized as a percent. If we randomly select 500 students and find 276 have brown hair, we can estimate the population proportion using \\(\\hat{p} = \\frac{X}{N}\\) where \\(X\\) is the number with brown hair and N is the number in our sample. We often interchange percents and proportions, but strictly speaking, a proportion is a number between 0 and 1. This can be interpreted as a probability as well.\nIf another researcher were to collect a different sample of 500 from the same population, they would almost certainly get a different number of people with brown hair than the first study. We can imagine taking many samples of 500 students and imagine the theoretical distribution of all possible \\(\\hat{p}\\). If we are taking good samples, most of these \\(\\hat{p}\\)’s should be near the population proportion, \\(p\\).\nIn fact, under certain conditions we can know the distribution of \\(\\hat{p}\\). As you probably guessed, the distribution of \\(\\hat{p}\\) is approximately normal with:\n\\[\\mu_{\\hat{p}} = p\\] \\[\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{N}}\\]\nThat is to say, the mean of all sample proportions is the population proportion.\nJust as it was with a sample mean, \\(\\bar{x}\\), we have to check certain conditions to assume the distribution is approximately normal. For \\(\\bar{x}\\), we needed the population to be normally distributed or to have a sample size greater than 30. The principle of having a large enough sample applies, but it’s different for a proportion.\nWe can assume the distribution is approximately normal if:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nIn plain English, this means our sample size has to be big enough to have at least 10 “successes” and 10 “failures”. For example, if we’re estimating the proportion of left handed people, we would need a sample size large enough to have at least 10 left handed people and 10 right handed people.\nIf the distribution of sample means is approximately normal according to the conditions above, we can calculate a z-score as we did in Unit 1 and 2:\n\\[z = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{N}}}\\]\nthen use pnorm() as before."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "title": "Sampling Distribution of P_hat",
    "section": "Example",
    "text": "Example\nSuppose we have a population where the true proportion of success is \\(p = 0.6\\). We take a random sample of size \\(n = 100\\) from this population. We want to find the probability that the sample proportion \\(\\hat{p}\\) is less than \\(0.55\\).\n\n# Given data\nx &lt;- 55\nn &lt;- 100\np_hat &lt;- x/n\n\n# population proportion\np &lt;- 0.6\n\n# Calculate standard deviation\nsigma_phat &lt;- sqrt(p * (1 - p) / n)\n\n# Calculate z-score\nz &lt;- (p_hat - p) / sigma_phat\n\n# Left Tail (lower than p)\npnorm(z)\n\n[1] 0.1537171\n\n# Right Tail (greater than p)\n1-pnorm(z)\n\n[1] 0.8462829"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "title": "Sampling Distribution of P_hat",
    "section": "Your Turn",
    "text": "Your Turn\nThe nationwide, fully-vaccinated rate if November 2021 was 58%. Based on survey responses of 150, 81% of all BYU-I students on campus during Fall 2021 semester had received at least one vaccination dose against COVID-19 with 74% being fully vaccinated.\nWhat is the probability that we get a random sample of 150 individuals with a \\(\\hat{p}\\) higher than the fully vaccinated rate that we observed?\n\n# given data\n\n# Population Proportion\n\n# Calculate Z\n\n# P-value"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "title": "One-sample Proportion Tests",
    "section": "",
    "text": "In statistics, one-sample proportion tests are used to compare proportions or percentages to a hypothesized value. These tests are useful when dealing with categorical data. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should look very familiar:\n\\[H_0: p = p_0\\] \\[H_a: p\\; (&lt;,&gt;,\\neq) \\: p_0\\] where \\(p_0\\) is some hypothesized value for the population proportion.\nSo far, we have been using Greek letters to represent population parameters. We deviate from that now due to the fact that the Greek letter for p, \\(\\pi\\), already has a long-established meaning in mathematics. In the hypothesis definition above, \\(p\\) represents the population proportion."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "title": "One-sample Proportion Tests",
    "section": "Example 1: One Sample Proportion Test",
    "text": "Example 1: One Sample Proportion Test\nSuppose we want to test whether the proportion of students who passed an exam is significantly less than 0.75. We have a sample of 100 students, of which 72 passed.\n\\[\\hat{p} = \\frac{X}{N} = \\frac{72}{100} = .72\\]\nIf we want to test if this is significantly less than 75%, we can use the prop.test() which is very similar to t.test(). Instead of putting in a sample mean, \\(\\bar{x}\\), with a hypothesized \\(\\mu\\), we put in \\(X\\) and \\(N\\) and a hypothesized \\(p\\). Setting the alternative and confidence level operates the same as t.test().\nConfidence intervals for proportions for proportions can also be obtained just as with t.test().\n\n# One Sample Proportion Test Example\n# Hypothesized proportion: 0.75\n# Sample size: 100\n# Number of successes: 72\n\nprop.test(x = 72, n = 100, p = 0.75, alternative = \"less\", conf.level = .9)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  72 out of 100, null probability 0.75\nX-squared = 0.33333, df = 1, p-value = 0.2819\nalternative hypothesis: true p is less than 0.75\n90 percent confidence interval:\n 0.0000000 0.7782396\nsample estimates:\n   p \n0.72 \n\nprop.test(x = 72, n = 100, conf.level = .9)$conf.int\n\n[1] 0.6358512 0.7917861\nattr(,\"conf.level\")\n[1] 0.9\n\n\n\nThe distribution of \\(\\hat{p}\\)\nRecall that the sampling distribution for \\(\\hat{p}\\) is approximately normally distributed when our sample has more than 10 expeted “successes” and more than 10 expected “failures”. We test this by looking at\nHypothesis Test Requirements:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nWe use p, not \\(\\hat{p}\\) for hypothesis testing because hypothesis testing always assumes the null hypothesis is true. Confidence intervals, on the other hand, make no such assumption.\nTo see if the calculated confidence interval is appropriate, we use \\(\\hat{p}\\).\nConfidence Interval Requirements: \\[n\\hat{p} \\geq 10\\] \\[n(1-\\hat{p}) \\geq 10\\] Can we trust the p-value and confidence interval?\nYou can use a calculator for this, or simply use R as a calculator:\n\nx &lt;- 72\nn &lt;- 100\np_hat &lt;- x/n\np &lt;- .75\n\n# For Hypothesis Testing:\nn*p &gt;= 10\n\n[1] TRUE\n\nn*(1-p) &gt;= 10\n\n[1] TRUE\n\n# For Confidence Intervals:\nn*p_hat &gt;= 10\n\n[1] TRUE\n\nn*(1-p_hat) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "title": "One-sample Proportion Tests",
    "section": "Example 2: Handedness",
    "text": "Example 2: Handedness\nSuppose the United States national average percent of left-handed people is 11%. A researcher wants to know if visual arts majors are significantly more likely to be left handed. She samples 250 visual arts majors and finds that 36 are left handed.\nPerform a one-sample proportion to see if visual arts majors are significantly more left-handed than the general population.\nState the null and alternative hypotheses and your significance level.\n\\[H_0: p = \\] \\[H_a: p \\] \\[\\alpha = \\]\nQuestion: What is the value of the test statistics for this test?\nAnswer:\nQuestion: What is the P-Value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:\nMake a \\((1-\\alpha)\\) level confidence interval for the true population proportion.\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:\nQuestion: Are the test requirements for the normality of \\(\\hat{p}\\) satisfied?\nAnswer:\nQuestion: Are the requirements for a confidence interval for \\(p\\) satisfied?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html",
    "href": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html",
    "title": "Two-sample Proportion Tests",
    "section": "",
    "text": "In statistics, two sample proportion tests are used to compare proportions or percentages between two independent groups. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should not be surprising:\n\\[H_0: p_1 = p_2\\] \\[H_a: p_1\\; (&lt;,&gt;,\\neq) \\: p_2\\] where \\(p_1\\) represents the unknown population proportion for group 1 and \\(p_2\\) represents the unknown population proportion for group 2."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "href": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "title": "Two-sample Proportion Tests",
    "section": "Example 1: Voting Behaviour by Gender",
    "text": "Example 1: Voting Behaviour by Gender\nSuppose we want to test if women are more likely to identify as Democrat than men. We sample 250 men and 250 women and measure their political affiliation. We find that 80 men identify as Democrat and 102 females identify as Democrat.\nJust as with the two-sample t-test for means, we must define a reference group. In this example, we will use females as the reference group so that our alternative will be relative to that group.\n\\[H_0: p_{femaleDem} = p_{maleDem}\\] \\[H_a: p_{femaleDem} &gt; p_{maleDem}\\] We will use \\(\\alpha = 0.05\\)\n\nprop.test(x = c(102, 80), n = c(250, 250), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(102, 80) out of c(250, 250)\nX-squared = 3.8099, df = 1, p-value = 0.02548\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01350993 1.00000000\nsample estimates:\nprop 1 prop 2 \n 0.408  0.320 \n\n\nWe can also create a confidence interval for the difference:\n\nprop.test(x = c(102, 80), n = c(250, 250))$conf.int\n\n[1] 5.904086e-06 1.759941e-01\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals for differences can be positive and negative. In this example, a negative number would indicate that Females are less likely to be Democrat and a positive number means they are more likely to be Democrat.\nOur confidence interval is just above zero on the lower end. We are 95% confident that females are between 0.000% and 17.6% more likely to be Democrat than men.\n\nTest Requirments\nJust as with 1-sample proportion tests, we must validate that we have a large enough sample size to ensure that \\(\\hat{p}\\) is approximately normally distributed. When we have 2 samples, however, we must check both \\(\\hat{p}\\)’s. For both hypothesis testing and confidence intervals we check:\nRequirements for Hypothesis Testing and Confidence Intervals\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nAn easy R calculator to check this is:\n\n# All must be true:\n\nx1 &lt;- 102\nn1 &lt;- 250\nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] TRUE\n\nn1*(1-phat1) &gt;=10\n\n[1] TRUE\n\nx2 &lt;- 80\nn2 &lt;- 250\nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] TRUE\n\nn2*(1-phat2) &gt;=10\n\n[1] TRUE\n\n\nIf all conditions are greater than or equal to 10, we can trust our p-values and confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "href": "5-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "title": "Two-sample Proportion Tests",
    "section": "Example 2: Favorite Sports",
    "text": "Example 2: Favorite Sports\nSoccer is becoming much more popular in the United States. We would like to test if this is being driven by demographic shifts in the population where the younger generation is more likely to favor soccer.\nA researcher samples 524 individuals under 40 and 655 individuals older than 40 and asks what their preferred sport is. Of the 524 respondents under 40, 44 identified soccer as their favorite sport. Of the 655 respondents over 40, 27 identified soccer as their favorite sport.\nPerform a 2-sample proportion test to determine if significantly more younger people identify soccer as their favorite sport.\nCreate and interpret the confidence interval for the difference in the proportions.\nAre the requirements for the hypothesis test and confidence interval satisfied?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "When working through statistical inference for means, we progressed from learning 1-sample t-tests to 2-sample t-tests. When we wanted to compare a quantitative variable between multiple groups, we introduced ANOVA. The hypothesis test changed and we introduced the F-statistic.\nRecall that the F-statistics was based on a ratio of squared quantities and was therefore always positive and skewed right.\nSimilarly, when we want to compare a categorical variable across multiple groups, we must modify the hypothesis test from the 2-sample proportion test and introduce a new test statistic: \\(\\chi^2\\). The Greek letter, \\(\\chi\\), is pronounced like “ki” in “kite”, not like “chi” in “tai chi”.\nAs can be seen from its name, \\(\\chi^2\\) is a squared value and is thus always positive and right skewed like the F-statistic.\n\n\nThe \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table.\n\n\n\nThe null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Test Requirements",
    "text": "Test Requirements\nRecall that 2-sample tests for proportions needed an at least 10 expected successes and at least 10 expected failures (\\(np \\ge 10\\) and \\(n(1-p)\\ge10\\)) for the test statistic to be valid.\nFor a \\(\\chi^2\\) test we need to check the expected counts for all the different combinations.\nWe don’t need to fret about the math behind the expected count calculation. Intuitively, if there was no relationship between the two variables you would expect all the row totals to be proportionally distributed across the column groups.\nWe only need to check that all expected counts are greater than 5.\n\nchisq.test(chiro_table)$expected &gt;= 5\n\n               \n                At Risk Prevention Self Care Sick Role Wellness\n  Australia        TRUE       TRUE      TRUE      TRUE     TRUE\n  Europe           TRUE       TRUE      TRUE      TRUE     TRUE\n  United States    TRUE       TRUE      TRUE      TRUE     TRUE"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "title": "Chi-square Test of Independence",
    "section": "Visualization",
    "text": "Visualization\nWe can use ggplot() to create nice bar charts to help interpret the results.\n\nggplot(chiropractic, aes(x = location, fill = motivation)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "title": "Chi-square Test of Independence",
    "section": "Homework: Grades and Classroom Type",
    "text": "Homework: Grades and Classroom Type\nThis is the raw data for the Homework Quiz. Use it to answer the homework questions, but also create a Bar Chart using ggplot().\n\ncourse &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/course_type_by_grade.csv')\n\nCreate a Bar Chart using ggplot()\nPerform the \\(\\chi^2\\) test of independence.\nQuestion: Are the requirements for a \\(\\chi^2\\) test satisfied?\nAnswer:\nQuestion: What are the null and alternative hypotheses?\nQuestion: What is the test statistic, \\(\\chi^2\\)?\nQuestion: How many degrees of freedom does this test have?\nQuestion: What is the P-value?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "title": "Chi-square Test of Independence",
    "section": "Heart Disease among Australian Women and Men",
    "text": "Heart Disease among Australian Women and Men\nIn 1982 in Western Australia, 1317 males and 854 females died of ischemic heart disease, 1119 males and 828 females died of cancer, 371 males and 460 females died of cerebral vascular disease, and 346 males and 147 females died of accidents. A medical researcher wanted to see if gender and cause of death are independent using a level of significance of 0.05.\nThe data read in below are a summary table of counts. One way to create a bar chart to compare heart disease deaths between men and women is to take this data and make it “longer”. This stacks the columns with count data in them and makes 2 new columns, one for the counts and one for the category of cardiovascular death.\nI will comment the code below to walk through each step.\nThe %&gt;% “pipe” below comes from the tidyverse library. You can think of this like making a series of steps where everything before the %&gt;% is pushed to the next step. For example, we assign aussie_death the original data table then move the table into the pivot_long() function which is the function that stacks the data. The output of that function becomes a new data shape, aussie_death.\n\n# Import the table data\naussie_death_table &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/aussie_death.csv\") \naussie_death_table\n\n      V1 heart_disease cancer vascular_disease accident\n1 female           854    828              460      147\n2   male          1371   1119              371      346\n\n# Pipe the original table into the pivot_longer() function\n# Pivot_longer needs to know which columns to \"stack\" which is input by the 'cols = ' argument.\n# We also need to give a name to the new column containing the count information.  The 'values_to=' argument names the column that will have the values, in this case we use \"count\".  \n# The 'names_to = ' argument names the column that will contain the labels of each category\n\n# Run the code and see if you can follow what happened\n\naussie_death &lt;- aussie_death_table %&gt;% \n  pivot_longer(cols = c('heart_disease', 'cancer', 'vascular_disease', 'accident'), values_to = 'count', names_to = \"reason\")\n\naussie_death\n\n# A tibble: 8 × 3\n  V1     reason           count\n  &lt;chr&gt;  &lt;chr&gt;            &lt;int&gt;\n1 female heart_disease      854\n2 female cancer             828\n3 female vascular_disease   460\n4 female accident           147\n5 male   heart_disease     1371\n6 male   cancer            1119\n7 male   vascular_disease   371\n8 male   accident           346\n\n# V1 was the default label and really represents \"Gender\" in this study.  We could change the name or leave it as is and fix it in the graphs\n\n\n# When using raw data, 'geom_bar()' creates the counts automatically from the categorical columns in a raw dataset.  The data in aussie_death is still a summary.  If we have the counts already, then we use the `geom_col()` function and have to specify a y variable to define how high to make the bars (in our case \"count\")\n\nggplot(aussie_death, aes(x = reason, y = count, fill = V1)) +\n  geom_col(position=\"dodge\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html",
    "title": "Who Shot First?",
    "section": "",
    "text": "Use the Star Wars dataset to answer the following questions:\n\nDo less than 20% of respondents feel Very Favorably towards Emperor Palpatine? (1-sample Z test for proportion)\nWhat is the difference in proportions of females and males who are Very Favorable towards Jar-Jar Binks? (2-sample Proportion)\nCome up with one other 2-sample proportion test using anything from the Star Wars dataset.\nTest to see if income and response to “Which Character Shot First?” are Independent (Chi-square)\n\nFor the proportion tests:\n\nDefine the null and alternative hypotheses\nInclude an explanation and conclusion for hypothesis tests\nInclude Confidence intervals and a sentence explaining each\nCheck the requirements for the hypothesis test and the confidence intervals\n\nFor the Chi-square test:\n\nDefine the null and alternative hypotheses\nInclude an explanation of the conclusion for\nBe sure to check the hypothesis requirements for a test of independence"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "title": "Who Shot First?",
    "section": "Explore the data",
    "text": "Explore the data\n\nnames(sw)\n\n [1] \"Are You a Fan of SW?\"               \"Favorability_Han Solo\"             \n [3] \"Favorability_Luke Skywalker\"        \"Favorability_Princess Leia Organa\" \n [5] \"Favorability_Anakin Skywalker\"      \"Favorability_Obi Wan Kenobi\"       \n [7] \"Favorability_Emperor Palpatine\"     \"Favorability_Darth Vader\"          \n [9] \"Favorability_Lando Calrissian\"      \"Favorability_Boba Fett\"            \n[11] \"Favorability_C-3P0\"                 \"Favorability_R2 D2\"                \n[13] \"Favorability_Jar Jar Binks\"         \"Favorability_Padme Amidala\"        \n[15] \"Favorability_Yoda\"                  \"who_shot_first\"                    \n[17] \"Familiar_with_expanded_universe\"    \"are_you_a_fan_of_expanded_universe\"\n[19] \"fan_of_star_trek\"                   \"Gender\"                            \n[21] \"Age\"                                \"Household.Income\"                  \n[23] \"Education\"                          \"Location\"                          \n\ntable(sw$`Favorability_Han Solo`)\n\n\n                                            \n                                          5 \nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\naddmargins(table(sw$`Favorability_Han Solo`, sw$Gender))\n\n                                             \n                                                  Female Male Sum\n                                                0      2    3   5\n  Neither favorably nor unfavorably (neutral)   1     22   21  44\n  Somewhat favorably                            4     71   76 151\n  Somewhat unfavorably                          1      3    4   8\n  Unfamiliar (N/A)                              0      9    6  15\n  Very favorably                               10    289  311 610\n  Very unfavorably                              0      0    1   1\n  Sum                                          16    396  422 834\n\naddmargins(table(sw$`Favorability_Emperor Palpatine`))\n\n\n                                            \n                                         20 \nNeither favorably nor unfavorably (neutral) \n                                        213 \n                         Somewhat favorably \n                                        143 \n                       Somewhat unfavorably \n                                         68 \n                           Unfamiliar (N/A) \n                                        156 \n                             Very favorably \n                                        110 \n                           Very unfavorably \n                                        124 \n                                        Sum \n                                        834"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "One-sample Proportion Test",
    "text": "One-sample Proportion Test\nWhat proportion of respondents are very favorable towards Emperor Palpatine?\nIs this significantly less than 20%?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "Two-sample Proportion Test",
    "text": "Two-sample Proportion Test\nWhat percent of female respondents are favorable towards Jar-Jar Binks?\nWhat percent of male respondents are favorable towards Jar-Jar Binks?\nAre they significantly different?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "title": "Who Shot First?",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nCompare 2 proportions of your choosing and perform a prop.test()."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "href": "5-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "title": "Who Shot First?",
    "section": "Chi-square Test for Independence",
    "text": "Chi-square Test for Independence\nTest to see if how you responded to the question “Who Shot First” is independent of income category.\nState your conclusion:"
  },
  {
    "objectID": "6-Semester_Project/Importing_Data.html",
    "href": "6-Semester_Project/Importing_Data.html",
    "title": "Downloading Data",
    "section": "",
    "text": "So far in class, we have encountered only fairly clean data examples carefully managed and stored in an easy to access location. We have been able to use the import() function from the rio library with a link to a well-contained data resource.\nWhen we encounter data in the wild, it can be much more complicated to extract and clean up. However, good research should be transparent, with data sources cited and, where possible, published. We can often find links to raw data for graphs.\nIn these notes we will see how we can use tools we’ve already used to import data directly from the internet (or a saved file on your computer) and learn some new skills about how to clean up the data for specific needs."
  },
  {
    "objectID": "6-Semester_Project/Importing_Data.html#mac-instructions",
    "href": "6-Semester_Project/Importing_Data.html#mac-instructions",
    "title": "Downloading Data",
    "section": "Mac Instructions:",
    "text": "Mac Instructions:\n\nOpen Finder, which is the smiley face icon located on the bottom of your screen.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it (or hold down the Control key while clicking), and select “Get Info”.\nIn the window that pops up, you’ll see a field called “Where:”, which shows you the file path. It looks something like /Users/YourUsername/Documents/YourFile.csv. You can copy this path by selecting it and pressing Command + C."
  },
  {
    "objectID": "6-Semester_Project/Importing_Data.html#pc-instructions",
    "href": "6-Semester_Project/Importing_Data.html#pc-instructions",
    "title": "Downloading Data",
    "section": "PC Instructions:",
    "text": "PC Instructions:\n\nOpen File Explorer, which is usually the folder icon located on your taskbar or in the Start menu.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it and select “Properties”.\nIn the Properties window, you’ll see a field called “Location” or “Location:” which shows you the file path. It looks something like C:\\Users\\YourUsername\\Documents\\YourFile.csv. You can copy this path by selecting it and pressing Ctrl + C.\n\nIn both cases, the file path tells the computer where to find your file, similar to how a street address tells someone where to find a physical location. You can use this file path to access your file from anywhere on your computer.\nUnfortunately, R doesn’t like single backslashes, \\. If you’re on a PC you can either add a second \\ or switch them to /\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nwar &lt;- import('C:\\\\Users\\\\paulccannon\\\\OneDrive - BYU-Idaho\\\\Math221inR\\\\countries-in-conflict-data.csv')\nwar &lt;- import('C:/Users/paulccannon/OneDrive - BYU-Idaho/Math221inR/countries-in-conflict-data.csv')\n\nThe rio::import() function can handle many different types data types (.xlsx, .xls, .csv, .txt, and many others). If you run into a data file type that rio can’t import, there are other libraries that have similar functions that can do the same thing. But we limit the scope for this class to sources that rio can handle."
  },
  {
    "objectID": "6-Semester_Project/Semester_Project_Template.html",
    "href": "6-Semester_Project/Semester_Project_Template.html",
    "title": "Semester Project",
    "section": "",
    "text": "Introduction\n\n\nDesign the Study\n\n\nCollect the Data\n\n\nDescribe/Summarize the Data\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\n# Be sure to make this using ggplot()\n\n\n\nMake Inference\n\n# Perform the appropriate test for the data selected\n\n\n\nConclusion (Take Action)"
  }
]